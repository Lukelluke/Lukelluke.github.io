<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Clion中的EOF</title>
    <url>/2020/06/21/Clion%E4%B8%AD%E7%9A%84EOF/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="1-问题："><a href="#1-问题：" class="headerlink" title="1.问题："></a>1.问题：</h1><p>在Mac &amp; Clion 中尝试使用 </p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(<span class="built_in">cin</span> &gt;&gt; varient)&#123;&#125;</span><br></pre></td></tr></table></figure>

<p>来作为输入结束判断时，由于判断的是“流”结束与否（具体不钻牛角尖），即在win环境下，一般是使用 <strong>CTRL+C</strong>结束，而在Clion中这个方法不行。</p>
<h1 id="2-解决："><a href="#2-解决：" class="headerlink" title="2.解决："></a>2.解决：</h1><ul>
<li><p>有很多博客说使用 <strong>CTRL+D</strong> /<strong>Command + D</strong>，均失败</p>
</li>
<li><p>有效方法：以 <strong>Debug模式</strong> 运行程序，正确输入数据之后—&gt;回车—&gt; command +D</p>
</li>
</ul>
<hr>
<p>以上，谨此纪念C++修习结束。</p>
<p>很多人说Python 比Cpp好学，我觉得不然。这句话可能只能仅限于 使用单纯语言自身特性上；若是要再加上语言的各种外载功能包，还指不定孰优孰劣呢。</p>
<p>在学习TF、Torch的路上，被各种乱七八糟的工具包整的落花流水。</p>
<p>实习实习找的不顺，想来也是自己基础实在没有打扎实，虽说自己是反抗成为上班族的，但是在发觉自己在想衡量自己的市场价值的时候竟然这么不值钱，就心里不爽万分。</p>
<p>再把合成相关的东西学学吧，没啥学不会的。</p>
<hr>
<p>06_21_2020 Sunday</p>
<p>@PT</p>
]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Diffusion Model</title>
    <url>/2023/07/08/Diffusion%20Model/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>Diffussion Model——扩散概率模型</p>
<p>——适用于所有的生成类任务：TTS （☑️）、VC（❓）</p>
<p>——其实和VAE有点像：多层 VAE</p>
<hr>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605112436.png" alt="image-20220605112436197"></p>
<ul>
<li><p>最早见刊时间 1995-2004</p>
</li>
<li><p>论文：2015-ICML    &amp;&amp;    ==2020-NIPS==</p>
<ul>
<li>[1] Ho, Jonathan, Ajay Jain, and Pieter Abbeel. “Denoising diffusion probabilistic models.” <em>Advances in Neural Information Processing Systems</em> 33 (<strong>2020</strong>): 6840-6851.</li>
<li>[2] Sohl-Dickstein, Jascha, et al. “Deep unsupervised learning using nonequilibrium thermodynamics.” <em>International Conference on Machine Learning</em>. PMLR, <strong>2015.</strong></li>
</ul>
</li>
<li><p><a href="https://github.com/hojonathanho/diffusion" target="_blank" rel="noopener">github代码</a></p>
</li>
<li><p><a href="https://www.bilibili.com/video/BV1b541197HX" target="_blank" rel="noopener">视频解读Bilibili</a></p>
</li>
</ul>
<ul>
<li><p>可参考解读阅读材料：</p>
<ul>
<li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" target="_blank" rel="noopener">What are Diffusion Models?</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/267696723" target="_blank" rel="noopener">基于扩散概率模型 (Diffusion Probabilistic Model ) 的音频生成模型</a></li>
<li><a href="https://www.birentech.com/news/114.html" target="_blank" rel="noopener"><strong>深度生成网络新思路：扩散概率模型</strong></a></li>
<li><a href="https://www.zhaoyabo.com/?p=7675" target="_blank" rel="noopener">概述|Diffusion Models扩散模型|学习笔记</a></li>
<li><a href="http://nooverfit.com/wp/%E5%9C%A8%E5%99%AA%E5%A3%B0%E4%B8%AD%E7%94%9F%E9%95%BF%EF%BC%9A%E6%89%A9%E6%95%A3%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8Bdiffusion-models%EF%BC%8Cscore-based-models%EF%BC%8C%E5%9F%BA/" target="_blank" rel="noopener">在噪声中“生长”：扩散生成模型(Diffusion Models)，score-based models，基于评分的生成模型</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/500532271" target="_blank" rel="noopener">详解diffusion model-知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/366004028" target="_blank" rel="noopener">另辟蹊径—Denoising Diffusion Probabilistic 一种从噪音中剥离出图像/音频的模型</a></li>
</ul>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/448575579" target="_blank" rel="noopener">简述马尔可夫链【通俗易懂】</a></p>
</li>
</ul>
<a id="more"></a>



<hr>
<ul>
<li>前置的数学知识</li>
</ul>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605143554.png" alt="image-20220605143554259"></p>
<ul>
<li>贝叶斯公式</li>
<li>VAE：KL散度</li>
</ul>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605144221.png" alt="image-20220605144221925">)<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605151903.png" alt="image-20220605151903434"></p>
<ul>
<li>训练：x-&gt;z</li>
<li>推理：z-&gt;x</li>
<li>【联合概率分布】</li>
<li>训练目标：最大化“对数似然” log p(x)</li>
<li>分子分母同时乘上一个“后验分布” q</li>
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605151544.png" alt="image-20220605151544252"></li>
</ul>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605151340.png" alt="image-20220605151340796">)<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605151645.png" alt="image-20220605151645670"></p>
<ul>
<li><p>基于的是 Markvo 假设</p>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605152702.png" alt="image-20220605152702039"></p>
</li>
<li></li>
</ul>
<p>❤️思考：Diffusion &amp; Multi-VAE 的区别？</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605152857.png" alt="image-20220605152857214"></p>
<ul>
<li><ol>
<li>X0-&gt;Xt 扩散过程：熵增</li>
<li>Xt-&gt;X0 逆扩散：去噪，还原</li>
<li>：对应的“条件概率分布”</li>
<li></li>
</ol>
</li>
</ul>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605153221.png" alt="image-20220605153221865"></p>
<ul>
<li>==正向过程不含参数，所有 均值、方差 都是确定的==，是一个 markvo链的关系。类比lr，固定不变</li>
<li>正向加噪的过程，是一个条件概率分布，而且是一个 高斯正太分布，均值为： “$<br>\sqrt{1-\beta_{t}} \mathbf{x}<em>{t-1}<br>$”，方差为 ：“$\beta</em>{t}$”</li>
<li>各向独立：“各向同性”</li>
</ul>
<p>🌟怎么算 X 呢？：</p>
<ul>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605154232.png" alt="image-20220605154232033"></p>
</li>
<li><p>然后可以套 Markvo 迭代，不断得到 Xt+1 … 更多采样值</p>
</li>
</ul>
<ul>
<li>问题： <ul>
<li>T 理论是 ∞ 无穷大，怎么设置？</li>
<li>参数化分布的 “$\beta_{t}$” （方差）应该怎么设置？</li>
</ul>
</li>
<li>$<br>\left{\beta_{t} \in(0,1)\right}_{t=1}^{t}<br>$</li>
<li>随着时间推移，$\beta_{t}$ 越来越大</li>
</ul>
<p>关于 T：参数重整化 算出来</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605154836.png" alt="image-20220605154836448"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605180042.png" alt="image-20220605180041989"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220606000627.png" alt="image-20220606000627561"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605160908.png" alt="image-20220605160908387"></p>
<p>所以：</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605161022.png" alt="image-20220605161022505"></p>
<ul>
<li><p>所以，时间取决于：最后的 均值、方差，接近于 “==各向同性==”的正态分布了：即==（0，1）==时，那么就可以说明 Xt “正向扩散过程”已经完全ok了，可以开始“逆扩散过程”了。</p>
</li>
<li><p>这其中，均值方差 alpha、beta （参数化正态分布时设置的）都是我们自己设定的，是个固定值，所以完全可控</p>
</li>
<li><p>总之：==正向扩散，想要得到“标准正态分布”==</p>
</li>
<li><p>一般原则：分布接近噪声时，beta 可以变大，刚开始时，beta 不要太大</p>
</li>
</ul>
<p>🌟逆扩散过程</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605161617.png" alt="image-20220605161617915">)<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605161825.png" alt="image-20220605161824972"></p>
<ul>
<li>逆扩散，仍然是一个 Markvo chain 过程</li>
<li>需要构建一个“==参数分布==”：不然直接从 Xt 到 X0 链式求取非常麻烦，所以需要构建一个网络</li>
<li>也是假设为一个 “高斯分布”</li>
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605164735.png" alt="image-20220605164735409"></li>
<li>把问题转化为 （Xt，t）两个变量的关系网络，均值方差都是和（Xt，t）这两个相关</li>
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605164854.png" alt="image-20220605164854035"></li>
</ul>
<ul>
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605165014.png" alt="image-20220605165014152">：</li>
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605165127.png" alt="image-20220605165127198"></li>
</ul>
<p>🌟说说怎么算：</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605165947.png" alt="image-20220605165947867"></p>
<p>==》基于贝叶斯公式：</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605165908.png" alt="image-20220605165908200"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605161903.png" alt="image-20220605161903503"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605161940.png" alt="image-20220605161940198">)<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605162006.png" alt="image-20220605162006080"></p>
<hr>
<ul>
<li>图 1. 不同类型的生成模型概述。</li>
</ul>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605125755.png" alt="image-20220605125755791"></p>
<ol>
<li><h6 id="发展历史"><a href="#发展历史" class="headerlink" title="发展历史"></a>发展历史</h6><ul>
<li><em>扩散概率模型</em>( <a href="https://arxiv.org/abs/1503.03585" target="_blank" rel="noopener">Sohl-Dickstein et al., 2015</a> )</li>
<li><em>噪声条件评分网络</em>( <strong>NCSN</strong> ; <a href="https://arxiv.org/abs/1907.05600" target="_blank" rel="noopener">Yang &amp; Ermon, 2019</a> )</li>
<li><em>去噪扩散概率</em>模型 (<strong>DDPM</strong>; <a href="https://arxiv.org/abs/2006.11239" target="_blank" rel="noopener">Ho et al. 2020</a>).</li>
</ul>
</li>
</ol>
<ol start="2">
<li><h6 id="快速总结"><a href="#快速总结" class="headerlink" title="快速总结"></a>快速总结</h6><ul>
<li><strong>优点</strong>：易处理性和灵活性是生成建模中两个相互冲突的目标。易于处理的模型可以进行分析评估并廉价地拟合数据（例如通过高斯或拉普拉斯），但它们不能轻易地描述丰富数据集中的结构。灵活的模型可以拟合数据中的任意结构，但从这些模型中评估、训练或采样通常很昂贵。<strong>扩散模型在分析上易于处理和灵活</strong></li>
<li><strong>缺点</strong>：扩散模型依赖于<strong>长马尔可夫扩散步骤链</strong>来生成样本，因此在<strong>时间和计算方面可能非常昂贵</strong>。已经提出了新的方法来使该过程更快，但<strong>采样仍然比 GAN 慢</strong>。</li>
</ul>
</li>
</ol>
<ol start="3">
<li><p>谈谈马尔科夫：</p>
<ul>
<li>下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。这种特定类型的“<strong>无记忆性</strong> ”称作马尔可夫性质。</li>
<li><strong>转移概率矩阵</strong>：</li>
</ul>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605132903.jpg" alt="img"></p>
<p>上图中有 A 和 B 两个状态，A 到 A 的概率是 0.3，A 到 B 的概率是 0.7；B 到 B 的概率是 0.1，B 到 A 的概率是 0.9。</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605132940.jpg" alt="img"></p>
</li>
</ol>
<ul>
<li><h6 id="状态转移矩阵-的稳定性"><a href="#状态转移矩阵-的稳定性" class="headerlink" title="==状态转移矩阵==的稳定性"></a>==<strong>状态转移矩阵</strong>==的稳定性</h6><p>状态转移矩阵有一个非常重要的特性，经过一定有限次数序列的转换，最终一定可以得到一个<strong>稳定的概率分布</strong> ，且与初始状态概率分布无关。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 状态转移矩阵</span></span><br><span class="line">matrix = np.matrix([[<span class="number">0.9</span>, <span class="number">0.075</span>, <span class="number">0.025</span>],</span><br><span class="line">                    [<span class="number">0.15</span>, <span class="number">0.8</span>, <span class="number">0.05</span>],</span><br><span class="line">                    [<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.5</span>]], dtype=float)</span><br><span class="line">vector1 = np.matrix([[<span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.3</span>]], dtype=float)</span><br><span class="line"><span class="comment">## 牛 熊 横盘</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    vector1 = vector1 * matrix</span><br><span class="line">    print(<span class="string">'Courrent round: &#123;&#125;'</span>.format(i+<span class="number">1</span>))</span><br><span class="line">    print(vector1)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Current round: 1</span><br><span class="line">[[ 0.405   0.4175  0.1775]]</span><br><span class="line">Current round: 2</span><br><span class="line">[[ 0.4715   0.40875  0.11975]]</span><br><span class="line">Current round: 3</span><br><span class="line">[[ 0.5156  0.3923  0.0921]]</span><br><span class="line">Current round: 4</span><br><span class="line">[[ 0.54591   0.375535  0.078555]]</span><br><span class="line">。。。。。。</span><br><span class="line">Current round: 58</span><br><span class="line">[[ 0.62499999  0.31250001  0.0625    ]]</span><br><span class="line">Current round: 59</span><br><span class="line">[[ 0.62499999  0.3125      0.0625    ]]</span><br><span class="line">Current round: 60</span><br><span class="line">[[ 0.625   0.3125  0.0625]]</span><br><span class="line">。。。。。。</span><br><span class="line">Current round: 99</span><br><span class="line">[[ 0.625   0.3125  0.0625]]</span><br><span class="line">Current round: 100</span><br><span class="line">[[ 0.625   0.3125  0.0625]]</span><br></pre></td></tr></table></figure>



<ul>
<li>n-gram 语音识别</li>
</ul>
<p>语言模型：N-Gram 是一种简单有效的语言模型，基于独立输入假设：<strong>第 n 个词的出现只与前面 N-1 个词相关，而与其它任何词都不相关</strong> 。整句出现的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计 N 个词同时出现的次数得到。</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605134140.jpg" alt="img"></p>
<p>声学模型：利用 HMM 建模（隐马尔可夫模型），HMM 是指这一马尔可夫模型的内部状态外界不可见，外界只能看到各个时刻的输出值。对语音识别系统，输出值通常就是从各个帧计算而得的声学特征。</p>
<hr>
<p>Diffusion </p>
<ul>
<li>前向扩散（逐渐加噪声）</li>
<li>后向扩散（学会从带噪声数据中恢复内容信息）</li>
</ul>
<ol>
<li>高斯噪声：符合正态分布的噪声<ul>
<li>起伏噪声、宇宙噪声、热噪声和散粒噪声等等</li>
<li>N（μ，σ^2）</li>
<li>高斯分布函数<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605134753.png" alt="image-20220605134753320" style="zoom:50%;"></li>
</ul>
</li>
</ol>
<ol start="2">
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605135130.png" alt="image-20220605135130047"></li>
</ol>
<ol start="3">
<li><p>高斯分布</p>
<p>高斯分布可以写成以下形式：</p>
<pre><code>![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal+N%28x%7C%5Cmu%2C%5Csigma%5E2%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7Dexp%5Cleft%28+-%5Cfrac%7B%28x-%5Cmu%29%5E2%7D%7B2%5Csigma%5E2%7D+%5Cright%29)</code></pre><p><img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="[公式]"> 是均只期望， <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2" alt="[公式]"> 是方差，以上形式是基于只有一个变化维度的连续随机变量，因此以上又称为<strong>一元高斯分布</strong>。</p>
</li>
</ol>
<p>   当 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal+N%28x%7C0%2C1%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%7Dexp%5Cleft%28+-%5Cfrac%7Bx%5E2%7D%7B2%7D+%5Cright%29" alt="[公式]">时，称为<strong>标准高斯分布</strong>（标准正态分布）：</p>
<p>   <img src="https://www.zhihu.com/equation?tex=%5Cmathcal+N%28x%7C0%2C1%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%7Dexp%5Cleft%28+-%5Cfrac%7Bx%5E2%7D%7B2%7D+%5Cright%29" alt="[公式]"></p>
<ol start="4">
<li><p>当我们要对概率密度函数求值时，我们需要对σ平方并且取倒数。当我们需要经常对不同参数下的概率密度函数求值时，一种更高效的==<strong>参数化分布</strong>==的方式是==使用参数β∈(0,∞)==，来控制分布的精度(precision)(或方差的倒数)：</p>
<p>​                                                  $$\frac{1}{\sigma}=\sqrt{\beta} $$    </p>
</li>
</ol>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605140547.png" alt="img"></p>
<p>​        正态分布可以推广到Rn空间，这种情况下被称为多维正态分布(multivariate     normal distribution)。它的参数是一个正定对称矩阵∑：</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605140834.png" alt="img"></p>
<p>参数μ仍然表示分布的均值，只不过现在是向量值。参数∑给出了分布的 ==<strong>协方差矩阵</strong>== 。和单变量的情况类似，当我们希望对很多不同参数下的概率密度函数多次求值时，协方差矩阵并不是一个很高效的参数化分布的方式，因为对概率密度函数求值时需要对∑求逆。我们可以使用一个<strong>精度矩阵</strong>(precision matrix)β进行替代：</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220605140921.png" alt="img"></p>
<p>我们常常把协方差矩阵固定成一个对角阵。一个更简单的版本是==各向同性(isotropic)高斯分布==，它的协方差矩阵是一个标量乘以单位阵。</p>
<ol start="5">
<li></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>《Voice Conversion with transformer network-samsung》论文总结</title>
    <url>/2020/08/15/NVAE%EF%BC%8C%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%B7%B1%E6%8C%96/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="NVAE"><a href="#NVAE" class="headerlink" title="NVAE"></a>NVAE</h1><ol>
<li><p>先大致搞清楚 VAE<img src="https://spaces.ac.cn/usr/uploads/2018/03/4168876662.png" alt="为了使模型具有生成能力，vae要求每个p(https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200810181426.png)都向正态分布看齐"></p>
<a id="more"></a>
</li>
<li><p>有两个 <strong>Encoder</strong>，一个求 $\mu$， 一个求 $\sigma$<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200810171034.png" alt="img"></p>
</li>
<li><p>说一下 VAE 中的「正态分布拟合」以及「从拟合的正态分布中采样」（Box-Muller 等等方法）<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200810171405.png" alt="image-20200810171404665"></p>
</li>
<li><p>VAE的名字中“变分”，是因为它的推导过程用到了<strong>KL散度及其性质</strong></p>
</li>
<li><p>说一说 <strong>VAE中的噪声</strong>（方差）</p>
<ol>
<li>增加重构难度，所以想减小它，让生成的数据更清晰</li>
<li>但是正是这个噪声，才是VAE精髓，增加了随机性</li>
</ol>
</li>
<li><p>噪声的好处（增加随机性） 和 坏处（导致采样结果成为确定性结果——均值 u ）</p>
</li>
<li><p>对噪声的处理：</p>
<ol>
<li>不直接让 方差 变为0（导致退化成 <strong>AE</strong> ）</li>
<li>而是配合着，让每段语音数据的 <strong>后验分布</strong> 朝着正态分布区靠近</li>
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200810162409.png" alt="image-20200810162407100"></li>
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200810162445.png" alt="image-20200810162443056"></li>
</ol>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200810170403.png" alt="image-20200810170401384"></p>
</li>
<li><p><strong>关于 后验分布 &amp;&amp; 先验分布 的理解</strong>（公式（2））</p>
<ol>
<li>后：单独的一段语音的高斯分布（标准正态）</li>
<li>先：某个说话人，所有语音的高斯分布集合（也是符合标准正态）</li>
</ol>
</li>
<li><p>VAE 中的 KL-loss <img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200810171818.png" alt="image-20200810171816918"></p>
</li>
<li><p>CVAE中的KL-loss（一类方法：实现标签的加入，从 <strong>无监督</strong> 转为 <strong>有监督</strong>）<img src="/2020/08/15/NVAE%EF%BC%8C%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%B7%B1%E6%8C%96/huangshengjie/Documents/2020/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99hexo%E6%B5%8B%E8%AF%95/Typora%E5%8D%9A%E5%AE%A2%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9/20200810172007.png" alt="image-20200810172006118"><strong>我们可以希望同一个类的样本都有一个专属的均值 $μ^Y$， （方差不变，还是单位方差），这个$μ^Y$让模型自己训练出来</strong></p>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200810172258.png" alt="img">【CVAE结构图】</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python preprocess.py --resample_rate 16000 \</span><br><span class="line">                     --origin_wavpath .&#x2F;data&#x2F;VCTK-Data&#x2F;VCTK-Corpus&#x2F;wav48 \</span><br><span class="line">                     --target_wavpath .&#x2F;data&#x2F;VCTK-Data&#x2F;VCTK-Corpus&#x2F;wav16 \</span><br><span class="line">                     --mc_dir_train .&#x2F;data&#x2F;VCTK-Data&#x2F;mc&#x2F;train \</span><br><span class="line">                     --mc_dir_test .&#x2F;data&#x2F;VCTK-Data&#x2F;mc&#x2F;test \</span><br><span class="line">                     --speaker_dirs p262 p272 p229 p232 p292 p293 p360 p361 p248 p251</span><br></pre></td></tr></table></figure>





<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python main.py --train_data_dir .&#x2F;data&#x2F;VCTK-Data&#x2F;mc&#x2F;train \</span><br><span class="line">               --test_data_dir .&#x2F;data&#x2F;VCTK-Data&#x2F;mc&#x2F;test \</span><br><span class="line">               --use_tensorboard False \</span><br><span class="line">               --wav_dir .&#x2F;data&#x2F;VCTK-Data&#x2F;VCTK-Corpus&#x2F;wav16 \</span><br><span class="line">               --model_save_dir .&#x2F;data&#x2F;aca16sjb&#x2F;VCTK-Data&#x2F;models \</span><br><span class="line">               --sample_dir .&#x2F;data&#x2F;VCTK-Data&#x2F;samples \</span><br><span class="line">               --num_iters 200000 \</span><br><span class="line">               --batch_size 8 \</span><br><span class="line">               --speakers p262 p272 p229 p232 \</span><br><span class="line">               --num_speakers 4</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python convert.py --resume_model 120000 \</span><br><span class="line">                  --num_speakers 4 \</span><br><span class="line">                  --speakers p262 p272 p229 p232 \</span><br><span class="line">                  --train_data_dir .&#x2F;data&#x2F;VCTK-Data&#x2F;mc&#x2F;train&#x2F; \</span><br><span class="line">                  --test_data_dir .&#x2F;data&#x2F;VCTK-Data&#x2F;mc&#x2F;test&#x2F; \</span><br><span class="line">                  --wav_dir .&#x2F;data&#x2F;VCTK-Data&#x2F;VCTK-Corpus&#x2F;wav16 \</span><br><span class="line">                  --model_save_dir .&#x2F;data&#x2F;aca16sjb&#x2F;VCTK-Data&#x2F;models \</span><br><span class="line">                  --convert_dir .&#x2F;data&#x2F;VCTK-Data&#x2F;converted</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>VC</tag>
        <tag>论文阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>1103——WaveNet &amp; 机器学习 考点小结</title>
    <url>/2020/11/03/WaveNet%E5%B0%8F%E7%BB%93/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="WaveNet小结"><a href="#WaveNet小结" class="headerlink" title="WaveNet小结"></a>WaveNet小结</h2><a id="more"></a>

<ol>
<li><h4 id="WaveNet-一种语音合成的模型-https-www-pianshen-com-article-43431455160-image-20201016164508908-https-blog-1301959139-cos-ap-beijing-myqcloud-com-picGo-20201016164510-png"><a href="#WaveNet-一种语音合成的模型-https-www-pianshen-com-article-43431455160-image-20201016164508908-https-blog-1301959139-cos-ap-beijing-myqcloud-com-picGo-20201016164510-png" class="headerlink" title="WaveNet:一种语音合成的模型 https://www.pianshen.com/article/43431455160/![image-20201016164508908](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016164510.png)"></a>WaveNet:一种语音合成的模型 <a href="https://www.pianshen.com/article/43431455160/![image-20201016164508908](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016164510.png)" target="_blank" rel="noopener">https://www.pianshen.com/article/43431455160/![image-20201016164508908](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016164510.png)</a></h4></li>
<li><h3 id="WaveNet，一种端到端的语音合成模型https-zhuanlan-zhihu-com-p-51359150"><a href="#WaveNet，一种端到端的语音合成模型https-zhuanlan-zhihu-com-p-51359150" class="headerlink" title="WaveNet，一种端到端的语音合成模型https://zhuanlan.zhihu.com/p/51359150"></a>WaveNet，一种端到端的语音合成模型<img src="/2020/11/03/WaveNet%E5%B0%8F%E7%BB%93/huangshengjie/Documents/2020/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99hexo%E6%B5%8B%E8%AF%95/Typora%E5%8D%9A%E5%AE%A2%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9/20201016164427.png" alt="image-20201016164426034"><a href="https://zhuanlan.zhihu.com/p/51359150" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/51359150</a></h3></li>
</ol>
<hr>
<h3 id="预测："><a href="#预测：" class="headerlink" title="预测："></a>预测：</h3><ol>
<li><p>机器学习各种 Loss 总结：</p>
<ol>
<li><a href="https://blog.csdn.net/perfect1t/article/details/88199179" target="_blank" rel="noopener">https://blog.csdn.net/perfect1t/article/details/88199179</a></li>
<li><a href="https://www.cnblogs.com/guoyaohua/p/9217206.html" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/9217206.html</a></li>
<li><a href="https://www.cnblogs.com/lliuye/p/9549881.html" target="_blank" rel="noopener">https://www.cnblogs.com/lliuye/p/9549881.html</a></li>
</ol>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016181353.png" alt="image-20201016181352142"></p>
</li>
<li><p><img src="/2020/11/03/WaveNet%E5%B0%8F%E7%BB%93/huangshengjie/Documents/2020/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99hexo%E6%B5%8B%E8%AF%95/Typora%E5%8D%9A%E5%AE%A2%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9/image-20201019005049252.png" alt="image-20201019005049252"><a href="https://zhuanlan.zhihu.com/p/74874291" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/74874291</a></p>
</li>
<li><p>常见优化算法及其优缺点：</p>
<ol>
<li><p><a href="https://blog.csdn.net/qq_19446965/article/details/81591521" target="_blank" rel="noopener">https://blog.csdn.net/qq_19446965/article/details/81591521</a> </p>
</li>
<li><p><a href="http://www.julyedu.com/question/big/kp_id/23/ques_id/1524![image-20201016181800596](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016181802.png)![image-20201016181557363](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016181558.png)" target="_blank" rel="noopener">http://www.julyedu.com/question/big/kp_id/23/ques_id/1524![image-20201016181800596](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016181802.png)![image-20201016181557363](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016181558.png)</a></p>
</li>
<li><h3 id="深度学习中优化方法——momentum、Nesterov-Momentum、AdaGrad、Adadelta、RMSprop、Adam"><a href="#深度学习中优化方法——momentum、Nesterov-Momentum、AdaGrad、Adadelta、RMSprop、Adam" class="headerlink" title="深度学习中优化方法——momentum、Nesterov Momentum、AdaGrad、Adadelta、RMSprop、Adam"></a>深度学习中优化方法——momentum、Nesterov Momentum、AdaGrad、Adadelta、RMSprop、Adam</h3><ol>
<li><h3 id="https-blog-csdn-net-u012328159-article-details-80311892-image-20201019162755198-Users-huangshengjie-Documents-2020-个人网站hexo测试-Typora博客图片文件夹-20201019162757-png"><a href="#https-blog-csdn-net-u012328159-article-details-80311892-image-20201019162755198-Users-huangshengjie-Documents-2020-个人网站hexo测试-Typora博客图片文件夹-20201019162757-png" class="headerlink" title="https://blog.csdn.net/u012328159/article/details/80311892![image-20201019162755198](/Users/huangshengjie/Documents/2020/个人网站hexo测试/Typora博客图片文件夹/20201019162757.png)"></a><a href="https://blog.csdn.net/u012328159/article/details/80311892![image-20201019162755198](/Users/huangshengjie/Documents/2020/个人网站hexo测试/Typora博客图片文件夹/20201019162757.png)" target="_blank" rel="noopener">https://blog.csdn.net/u012328159/article/details/80311892![image-20201019162755198](/Users/huangshengjie/Documents/2020/个人网站hexo测试/Typora博客图片文件夹/20201019162757.png)</a></h3></li>
<li><p><a href="https://blog.csdn.net/u012328159/article/details/80252012![image-20201019162915626](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201019162916.png)" target="_blank" rel="noopener">https://blog.csdn.net/u012328159/article/details/80252012![image-20201019162915626](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201019162916.png)</a></p>
</li>
<li><p><a href="https://www.cnblogs.com/yangmang/p/7477802.html![image-20201019165031144](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201019165036.png)" target="_blank" rel="noopener">https://www.cnblogs.com/yangmang/p/7477802.html![image-20201019165031144](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201019165036.png)</a></p>
</li>
</ol>
</li>
</ol>
</li>
<li><p>常用激活函数：<a href="https://zhuanlan.zhihu.com/p/32610035![image-20201016224701337](/Users/huangshengjie/Documents/2020/个人网站hexo测试/Typora博客图片文件夹/20201016224703.png)" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32610035![image-20201016224701337](/Users/huangshengjie/Documents/2020/个人网站hexo测试/Typora博客图片文件夹/20201016224703.png)</a></p>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016224719.jpg" alt="img"></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/73214810![image-20201016224906446](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016224907.png)" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/73214810![image-20201016224906446](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016224907.png)</a></p>
</li>
<li><p><a href="https://www.cnblogs.com/itmorn/p/11132494.html![image-20201019170355380](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201019170356.png)" target="_blank" rel="noopener">https://www.cnblogs.com/itmorn/p/11132494.html![image-20201019170355380](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201019170356.png)</a></p>
</li>
<li><p><a href="https://www.cnblogs.com/itmorn/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98/default.html?page=2![image-20201019170708536](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201019170710.png)" target="_blank" rel="noopener">https://www.cnblogs.com/itmorn/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98/default.html?page=2![image-20201019170708536](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201019170710.png)</a></p>
</li>
</ol>
<hr>
<ol>
<li><h6 id="A率13折线PCM编码的C语言实现-https-blog-csdn-net-u010480899-article-details-51172633-image-20201017001812322-Users-huangshengjie-Documents-2020-个人网站hexo测试-Typora博客图片文件夹-20201017001813-png"><a href="#A率13折线PCM编码的C语言实现-https-blog-csdn-net-u010480899-article-details-51172633-image-20201017001812322-Users-huangshengjie-Documents-2020-个人网站hexo测试-Typora博客图片文件夹-20201017001813-png" class="headerlink" title="A率13折线PCM编码的C语言实现 https://blog.csdn.net/u010480899/article/details/51172633![image-20201017001812322](/Users/huangshengjie/Documents/2020/个人网站hexo测试/Typora博客图片文件夹/20201017001813.png)"></a>A率13折线PCM编码的C语言实现 <a href="https://blog.csdn.net/u010480899/article/details/51172633![image-20201017001812322](/Users/huangshengjie/Documents/2020/个人网站hexo测试/Typora博客图片文件夹/20201017001813.png)" target="_blank" rel="noopener">https://blog.csdn.net/u010480899/article/details/51172633![image-20201017001812322](/Users/huangshengjie/Documents/2020/个人网站hexo测试/Typora博客图片文件夹/20201017001813.png)</a></h6></li>
<li><p><a href="https://blog.csdn.net/u012323667/article/details/79214336?utm_medium=distribute.pc_relevant_download.none-task-blog-blogcommendfrombaidu-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant_download.none-task-blog-blogcommendfrombaidu-1.nonecas![image-20201017003241394](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201017003242.png)" target="_blank" rel="noopener">https://blog.csdn.net/u012323667/article/details/79214336?utm_medium=distribute.pc_relevant_download.none-task-blog-blogcommendfrombaidu-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant_download.none-task-blog-blogcommendfrombaidu-1.nonecas![image-20201017003241394](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201017003242.png)</a></p>
</li>
<li><p>!                              <a href="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201017003854.png" target="_blank" rel="noopener">image-20201017003852920</a><a href="https://wenku.baidu.com/view/725f75bf1a37f111f1855b3c.html#![image-20201017003821332](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201017003822.png)" target="_blank" rel="noopener">https://wenku.baidu.com/view/725f75bf1a37f111f1855b3c.html#![image-20201017003821332](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201017003822.png)</a></p>
</li>
</ol>
<p><strong>1</strong></p>
]]></content>
      <categories>
        <category>语音</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>语音合成</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo一些基础指令备忘</title>
    <url>/2020/06/16/hexo%E4%B8%80%E4%BA%9B%E5%9F%BA%E7%A1%80%E6%8C%87%E4%BB%A4%E5%A4%87%E5%BF%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="1-创建文章"><a href="#1-创建文章" class="headerlink" title="1. 创建文章"></a>1. 创建文章</h2><ul>
<li><h4 id="在hexo下创建一个新的文章"><a href="#在hexo下创建一个新的文章" class="headerlink" title="在hexo下创建一个新的文章"></a>在hexo下创建一个新的文章</h4></li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new &quot;文章名称&quot;</span><br></pre></td></tr></table></figure>

<h2 id="2-创建标签"><a href="#2-创建标签" class="headerlink" title="2. 创建标签"></a>2. 创建标签</h2><ul>
<li>创建分类页面</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new page categories</span><br></pre></td></tr></table></figure>

<ul>
<li>基本设置</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">title: tags</span><br><span class="line">date: </span><br><span class="line">type: &quot;tags&quot;</span><br></pre></td></tr></table></figure>

<h2 id="3-创建分类"><a href="#3-创建分类" class="headerlink" title="3.创建分类"></a>3.创建分类</h2><ul>
<li>创建分类页面</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new page categories</span><br></pre></td></tr></table></figure>

<ul>
<li>基本设置</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">title: categories</span><br><span class="line">date: </span><br><span class="line">type: &quot;categories&quot;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>test</title>
    <url>/2020/06/07/test/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>ceshi</p>
<p>插入html链接</p>
<p><a href="https://blog-1301959139.cos.ap-beijing.myqcloud.com/2020/%E5%8D%9A%E5%AE%A2/061421180327/061421180327.html" target="_blank" rel="noopener">https://blog-1301959139.cos.ap-beijing.myqcloud.com/2020/%E5%8D%9A%E5%AE%A2/061421180327/061421180327.html</a></p>
<p>用iframe插入html链接</p>
<iframe width="86%" height="460" scrolling="auto" frameborder="0" src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/2020/%E5%8D%9A%E5%AE%A2/061421180327/061421180327.html"></iframe>



<p>插入mp3:</p>
<p>1.aplayer</p>

			<script>
				console.error("Error: [hexo-tag-aplayer] Unrecognized tag argument(2): autoplay=false");
			</script>

<p>2.文件直接拖拽，本地存储</p>
<p> <a href="./30003.wav">30003.wav</a> </p>
<p>3.aplayer不稳定，还是应该用iframe标签</p>

			<script>
				console.error("Error: [hexo-tag-aplayer] Unrecognized tag argument(2): autoplay=false");
			</script>

<hr>
<p>4.aplayer meeting，产生歌单，用网易云的连接id</p>

    <div id="aplayer-ZExpIhLu" class="aplayer aplayer-tag-marker meting-tag-marker" data-id="523845661" data-server="netease" data-type="playlist" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#FF4081"></div>



<hr>
<p>5.用iframe插入</p>
<iframe frameborder="yes" border="1" marginwidth="100" marginheight="100" width="600" height="100" src="http://qbjun0qc6.bkt.clouddn.com/30001.wav">
</iframe>

<p>插入pdf</p>
<a id="more"></a>

<p>插入腾讯云pdf测试：</p>
<div class="pdfobject-container" data-target="https://blog-1301959139.cos.ap-beijing.myqcloud.com/2020/%E5%8D%9A%E5%AE%A2/VC%E8%AE%BA%E6%96%87202005.pdf" data-height="500px"></div>



<p>用iframe插入：</p>
<iframe width="86%" height="460" scrolling="auto" frameborder="0" src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/2020/%E5%8D%9A%E5%AE%A2/VC%E8%AE%BA%E6%96%87202005.pdf"></iframe>



<p>本地路径插入：pdf ./vc.pdf</p>
<div class="pdfobject-container" data-target="./vc.pdf" data-height="500px"></div>





<p>谷歌网址外链插入</p>
<div class="pdfobject-container" data-target="https://drive.google.com/file/d/0B6qSwdwPxPRdTEliX0dhQ2JfUEU/preview" data-height="500px"></div>



<p><img src="/2020/06/07/test/1.png" alt="1"></p>
<iframe frameborder="yes" border="1" marginwidth="1" marginheight="1" width="330" height="100" src="//music.163.com/outchain/player?type=2&id=444267925 & auto=1 & height=60 "></iframe>

<p><img src="https://s1.ax1x.com/2020/06/07/t2bJ56.png" alt="t2bJ56.png"></p>
<p><img src="/2020/06/07/test/stargan/StarGAN-VC2_files/network.png" alt="stargan.png"></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">iframe</span> <span class="attr">frameborder</span>=<span class="string">"no"</span> <span class="attr">border</span>=<span class="string">"0"</span> <span class="attr">marginwidth</span>=<span class="string">"0"</span> <span class="attr">marginheight</span>=<span class="string">"0"</span> <span class="attr">width</span>=<span class="string">330</span> <span class="attr">height</span>=<span class="string">86</span> <span class="attr">src</span>=<span class="string">"//music.163.com/outchain/player?type=3&amp;id=2066166810&amp;auto=1&amp;height=66"</span>&gt;</span><span class="tag">&lt;/<span class="name">iframe</span>&gt;</span></span><br></pre></td></tr></table></figure>


			<script>
				console.error("Error: [hexo-tag-aplayer] Specified asset file not found (picture.jpg)");
			</script>


			<script>
				console.error("Error: [hexo-tag-aplayer] Specified asset file not found ([picture_url,)");
			</script>

]]></content>
      <tags>
        <tag>测试</tag>
      </tags>
  </entry>
  <entry>
    <title>关于 WORLD 中 code_spectral_envelope 和 MFCC 关系的理解</title>
    <url>/2023/07/08/%E5%85%B3%E4%BA%8E%20WORLD%20%E4%B8%AD%20code_spectral_envelope%20%E5%92%8C%20MFCC%20%E5%85%B3%E7%B3%BB%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h1><ol>
<li><a href="https://github.com/mmorise/World/issues/90" target="_blank" rel="noopener">https://github.com/mmorise/World/issues/90</a> （有人提问 WORLD 提取得到的 mel spectrum 梅尔谱和传统概念上 经过一系列stft之后还要经过“三角滤波器组”的过程区别？）</li>
<li><a href="https://github.com/mmorise/World/issues/33" target="_blank" rel="noopener">https://github.com/mmorise/World/issues/33</a> （r9y9 在<a href="https://github.com/mmorise" target="_blank" rel="noopener">mmorise</a>/<strong><a href="https://github.com/mmorise/World" target="_blank" rel="noopener">World</a></strong> 下提问关于 编码/解码后音色变化问题；代码bug已解决；学习一下画图和使用特点）</li>
</ol>
<hr>
<a id="more"></a>

<h1 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h1><ol>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200723190241.png" alt="image-20200723190240435"></p>
</li>
<li><p>简言之，传统论文  <a href="https://www.sp.nitech.ac.jp/~tokuda/selected_pub/pdf/conference/tokuda_icslp1994.pdf" target="_blank" rel="noopener">Mel-cepstral analysis</a> 提到的方法，也就是正常思路的经过 FFT 后再经过三角滤波器组 得到的 mel 谱，之所以需要三角滤波，可以理解为是，因为MFCC是在 <strong>频谱图</strong> 上进行的操作，所以是未经过 <strong>平滑</strong> 操作的，所以需要滤波器；</p>
</li>
<li><p>而WORLD，是在频谱包络上进行的操作，本身已经是顺滑过的，所以得到的 sp 特征，看似流程上没有三角滤波，但是它在使用的时候，效果和SPTK、librosa、merlin之类工具得到的 MFCC 来处理的音频效果是差不多的。</p>
</li>
<li><p>所以，就可以理解，很多论文的实现上，作者们在遇到：MFCC 这个特征需要时，若非论文着重强调，是可以用 <strong>code_spectral_envelope</strong> ，并取维度参数为 36 等，来表示36维度（bin）的MFCC特征的。</p>
</li>
</ol>
<hr>
<p>以上，解决了一直没人能帮我说清楚的问题疑惑。</p>
<p>还是要多看看源码和 issue，和大佬们交流才进步的多。</p>
<hr>
<p>这行里，可能大佬很多，但是能真正带领小白入门的系统专家真的少。sigh。我可能适合做老师，喜欢把大家难懂的东西，娓娓道来，教会孩子们。😁</p>
]]></content>
      <categories>
        <category>-[语音]</category>
      </categories>
      <tags>
        <tag>语音</tag>
      </tags>
  </entry>
  <entry>
    <title>关于pyworld.load 读取音频和soundfile.read 差别</title>
    <url>/2023/07/08/%E5%85%B3%E4%BA%8Epyworld.load%20%E8%AF%BB%E5%8F%96%E9%9F%B3%E9%A2%91%E5%92%8Csoundfile.read%20%E5%B7%AE%E5%88%AB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h1><p>在pyworld使用前，一般需要读取音频文件：</p>
<ul>
<li><p>librosa.load() 默认得到的是float32类型的数据，所以一般会再跟上 x.astype(np.float64)</p>
<ul>
<li>而恰恰是这么一个Numpy类型转换，会导致得到的 ap 特征中会含有 Nan 数据，这会导致最终的计算出现不必要的偏差；</li>
</ul>
</li>
</ul>
<a id="more"></a>

<!--more-->

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">f"ap <span class="subst">&#123;np.isnan(ap).any()&#125;</span>"</span>)  <span class="comment"># 返回True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#统计nan个数</span></span><br><span class="line">t = ap</span><br><span class="line">t = t[np.isnan(t)]  <span class="comment"># 用切片法 + 条件限制，来得到 nan 值的切片</span></span><br><span class="line"><span class="comment"># t = t[np.where(np.isnan(t))]</span></span><br><span class="line">print(t.shape)  <span class="comment"># （4k+, ）</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p>（这个问题和怎么计算得到ap无关：尝试了 world.wav2worl（）和 pyworld.harvest + cheaptrick + d4c路径，结果都一样）</p>
</li>
<li><p>Soundfile.read()  # 默认的数据返回值是 float64，所以可以直接得到所要求的数据格式</p>
</li>
<li><p>其中，对应的函数参数调整：sr 变为 samplerate ， mono 的单通道 改用 channels=1</p>
</li>
</ul>
<h2 id="Ps-附上代码和-issue网址"><a href="#Ps-附上代码和-issue网址" class="headerlink" title="Ps.附上代码和 issue网址"></a>Ps.附上代码和 issue网址</h2><p>[issue][<a href="https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder/issues/50]" target="_blank" rel="noopener">https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder/issues/50]</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pyworld <span class="keyword">as</span> world</span><br><span class="line"><span class="keyword">import</span> pyworld</span><br><span class="line"><span class="keyword">import</span> librosa</span><br><span class="line"><span class="keyword">import</span> librosa.display</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> soundfile</span><br><span class="line"></span><br><span class="line"><span class="comment"># wav,fs = librosa.load(os.getcwd()+"/bed (537).wav")</span></span><br><span class="line">wav, fs = soundfile.read(os.getcwd()+<span class="string">"/bed (537).wav"</span>)</span><br><span class="line"><span class="comment"># wav = wav.astype(np.float64)</span></span><br><span class="line"><span class="comment"># print(wav[0].type)  # 'numpy.float64' object has no attribute 'type'</span></span><br><span class="line">frame_period = <span class="number">5.0</span></span><br><span class="line">hop_length = int(fs * frame_period * <span class="number">0.001</span>)</span><br><span class="line">fftlen = world.get_cheaptrick_fft_size(fs)</span><br><span class="line"></span><br><span class="line">f0, timeaxis = pyworld.harvest(wav, fs, frame_period=frame_period, f0_floor=<span class="number">71.0</span>, f0_ceil=<span class="number">800.0</span>)</span><br><span class="line">sp = pyworld.cheaptrick(wav, f0, timeaxis, fs)</span><br><span class="line">ap = pyworld.d4c(wav, f0, timeaxis, fs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># f0, sp, ap = world.wav2world(x,sr,fftlen,frame_period)</span></span><br><span class="line">print(ap.shape)</span><br><span class="line">print(<span class="string">f"ap <span class="subst">&#123;np.isnan(ap).any()&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">wav = pyworld.synthesize(f0, sp, ap, fs, frame_period)</span><br><span class="line"><span class="comment"># wav = wav.astype(np.float32)</span></span><br><span class="line">soundfile.write(<span class="string">'test.wav'</span>, wav, fs)</span><br><span class="line"></span><br><span class="line">x, sr = soundfile.read(os.getcwd()+<span class="string">"/bed (537).wav"</span>)</span><br><span class="line"><span class="comment"># print(x[0].type)  # 'numpy.float64' object has no attribute 'type'</span></span><br><span class="line">f01, timeaxis1 = pyworld.harvest(x, sr, frame_period=frame_period, f0_floor=<span class="number">71.0</span>, f0_ceil=<span class="number">800.0</span>)</span><br><span class="line">sp1 = pyworld.cheaptrick(x, f01, timeaxis1, fs)</span><br><span class="line">ap1 = pyworld.d4c(x, f01, timeaxis1, fs)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">以上这样，直接用soundfile 读取成float64 数据，</span></span><br><span class="line"><span class="string">然后再直接用 soundfile.write 保存float64的文件 或者是 先转化成float32 再保存成文件， </span></span><br><span class="line"><span class="string">保存出来的文件再次用soundfile 读取出来，</span></span><br><span class="line"><span class="string">再次用测试ap是否有 nan值，都没有问题。</span></span><br><span class="line"><span class="string">综上，能用soundfile就避免用librosa，读取和写入文件都是这个道理；ßå</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'************'</span>)</span><br><span class="line">print(ap1.shape)</span><br><span class="line">print(<span class="string">f"ap1 <span class="subst">&#123;np.isnan(ap1).any()&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"ap <span class="subst">&#123;np.isnan(ap).any()&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # print(np.isnan(ap))</span></span><br><span class="line"><span class="comment"># t = ap</span></span><br><span class="line"><span class="comment"># t = t[np.isnan(t)]</span></span><br><span class="line"><span class="comment"># # t = t[np.where(np.isnan(t))]</span></span><br><span class="line"><span class="comment"># print(t.shape)</span></span><br><span class="line"></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ap:</span><br><span class="line">    <span class="comment"># if np.isnan(x):</span></span><br><span class="line">    count = count+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(count)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> np.isnan(ap).any():</span><br><span class="line"></span><br><span class="line">    f0, sp, ap = world.wav2world(np.absolute(x),fs,fftlen,frame_period)</span><br><span class="line">    print(<span class="string">f"ap abs <span class="subst">&#123;np.isnan(ap).any()&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">exit()</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>-[语音]</category>
      </categories>
      <tags>
        <tag>语音</tag>
      </tags>
  </entry>
  <entry>
    <title>关于ssh本地查看Tensorboard记录</title>
    <url>/2020/06/21/%E5%85%B3%E4%BA%8Essh%E6%9C%AC%E5%9C%B0%E6%9F%A5%E7%9C%8BTensorboard%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="1-需求：在本地查看Tensorboard"><a href="#1-需求：在本地查看Tensorboard" class="headerlink" title="1.需求：在本地查看Tensorboard"></a>1.需求：在本地查看Tensorboard</h1><ul>
<li><pre><code class="ssh">ssh -L 16006:127.0.0.1:6006 hsj@student.is99kdf.xyz -p 15203
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  * 作用：将远程的服务器**6006**端口转发到本地的 **16006**端口</span><br><span class="line"></span><br><span class="line">*</span><br></pre></td></tr></table></figure>
cd /home/sdb3/home/hsj/taco1_tf/tacotron
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  * 作用：进入所要运行的 logs 文件夹所在路径</span><br><span class="line"></span><br><span class="line">*</span><br></pre></td></tr></table></figure>
source activate py36
tensorboard --logdir ./logs-tacotron
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  * 开启有 Tensorboard 的环境</span><br><span class="line">  * 运行 ，打开logs文件</span><br><span class="line"></span><br><span class="line">*</span><br></pre></td></tr></table></figure>
本地浏览器打开 127.0.0.1:16006</code></pre>
<ul>
<li>成功实现本地查看 Tensorboard</li>
<li>再也不用像之前那样傻傻滴每次要手动下载 log 文件到本地之后才执行</li>
</ul>
</li>
</ul>
<hr>
<h1 id="2-待解决："><a href="#2-待解决：" class="headerlink" title="2.待解决："></a>2.待解决：</h1><ul>
<li>再进一步了解一下 Tensorboard 上方的各种功能：<ul>
<li>Distributions</li>
<li>Histograms</li>
<li>Projector ：这个很有意思啊，貌似是吧训练过程中数据点的变化，以动图的形式表现出来；</li>
</ul>
</li>
</ul>
<hr>
<h1 id="备注："><a href="#备注：" class="headerlink" title="备注："></a>备注：</h1><p>tensorflow 和 Numpy 对应关系：</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200626183013.png" alt="image-20200626175304169"></p>
]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorboard，Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2023/07/08/hello-world/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h3 id><a href="#" class="headerlink" title></a></h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo c &amp;&amp; hexo g &amp;&amp; hexo s</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo d</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<!--more-->

<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>散文诗</title>
    <url>/2020/06/16/%E6%95%A3%E6%96%87%E8%AF%97/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script>
        <div id="aplayer-FEsjOqEN" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;">
            <pre class="aplayer-lrc-content"></pre>
        </div>
        <script>
          var ap = new APlayer({
            element: document.getElementById("aplayer-FEsjOqEN"),
            narrow: false,
            autoplay: false,
            showlrc: false,
            music: {
              title: "父亲写的散文诗",
              author: "许飞",
              url: "https://blog-1301959139.cos.ap-beijing.myqcloud.com/2020/%E5%8D%9A%E5%AE%A2/%E8%AE%B8%E9%A3%9E%20-%20%E7%88%B6%E4%BA%B2%E5%86%99%E7%9A%84%E6%95%A3%E6%96%87%E8%AF%97.flac",
              pic: "http://qbjun0qc6.bkt.clouddn.com/Stay%20with%20me%E4%BA%94%E7%BA%BF%E8%B0%B1",
              lrc: ""
            }
          });
          window.aplayers || (window.aplayers = []);
          window.aplayers.push(ap);
        </script>



<hr>
<p>不知怎么的，每次听这首歌都会泪目</p>
<a id="more"></a>

<p>\当下的社会环境很浊。我没觉得很乱，就是纯粹的很浊。只不过因为我个人的眼界原因，我有点看不清了。</p>
<p>28岁的ByteDance-郭宇，实现了财务自由，选择退休成为温泉旅行作家。</p>
<p>第一时间看到这个信息，我和所有人一样，着实感到震撼，并为之颤抖。想想自己，27才能硕士毕业，估计还要为 进入大厂 / 找份舒适工作 而发愁，缺有个同龄人完成了你的所有梦想。</p>
<p>和泽奇说的一样，当下要实现财务自由，只能想方设法拿到股权，通过分红的方式来获取财富。只单纯工作、打工的形式，是永远得不到想要的境界。</p>
<p>而普通人，能获得股份期权的途径，可能就是读博，技术入股。</p>
<hr>
<p>其实很多时候，发现自己的想法也并没有比别人有很多高明之处，往往我能想到的，大家都会明白，或早或晚。</p>
<p>最年少时，希冀能改变一点点世界。</p>
<p>后来，本科时期，觉得自己太太太普通，想做个普通人，安安稳稳，上了研，开始有意识地探索“铁饭碗”。</p>
<p>但是经过一段时间，尝试完全不投入学习，以自己能想象的最随意的方式生活，发觉有点讨厌。生活的细碎繁琐，还是会让我耳朵起茧子，心里起皱褶。尽管和同龄人相比，肯定是和父母相处的很融洽的了，但长时间相处，还是会发觉自己不逃习惯过度的关心。或者应该说，自己受宠若惊而有点烦，不喜欢被过度关怀，嘴上得花很多唾沫来拒绝别人的安排和说辞。</p>
<hr>
<p>类似这篇文章这样，比较随意地袒露自己心里想法的文字，我应该永远不会在所谓的“朋友圈”来表达，更不会通过公众号来写文章。</p>
<p>没必要。</p>
<p>所以我会觉得在自己的博客抒发情感会很舒适自然。</p>
<p>相比之前，wordpress繁琐、丑陋的后台文章写作环境，我太喜欢本地md编写，然后保存即可轻松推流的方式。</p>
<p>像之后尝试写写随笔杂文，以后有机会整理成册，发些书玩玩。</p>
<p>晚安</p>
<p>06/17/2020 凌晨</p>
<p>@PT</p>
<hr>
]]></content>
      <categories>
        <category>作家计划</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>关于深度学习中模型矢量图的绘制、剪裁、保存 小结</title>
    <url>/2020/06/14/%E5%85%B3%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%A8%A1%E5%9E%8B%E7%9F%A2%E9%87%8F%E5%9B%BE%E7%9A%84%E7%BB%98%E5%88%B6%E3%80%81%E5%89%AA%E8%A3%81%E3%80%81%E4%BF%9D%E5%AD%98-%E5%B0%8F%E7%BB%93/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="几个概念"><a href="#几个概念" class="headerlink" title="几个概念"></a>几个概念</h1><ol>
<li>矢量图（.EPS）格式，不会失真</li>
<li>参考文章<a href="https://www.zhihu.com/question/58540942" target="_blank" rel="noopener">知乎</a></li>
<li>需要的工具：<a href="https://www.macwk.com/soft/adobe-acrobat-pro-dc" target="_blank" rel="noopener">Adobe Acrobat Pro DC</a> &amp;&amp; <a href="https://www.macwk.com/article/adobe-zii" target="_blank" rel="noopener">Adobe Zii 激活工具</a></li>
</ol>
<hr>
<a id="more"></a>

<h2 id="操作步骤"><a href="#操作步骤" class="headerlink" title="操作步骤"></a>操作步骤</h2><ol>
<li>将画好的 PPT 模板导出成 pdf 文件格式，用 DC 打开</li>
<li>搜索 DC 功能 <strong>裁剪</strong>，选定图像区域（去掉白边）</li>
<li>然后打开 DC 侧边栏，选取要到处的图片所在界面，<strong>右击</strong>，<strong>提取页面</strong>，<strong>输入要导出的界面</strong> <em>（这是为了在很庞大的所有ppt页面中，单独拎出需要的页面，方便下一步的导出为 .eps 格式而服务的）</em></li>
<li>保存这个由新选取的页面们所组成的新 PDF，然后点击：<strong>文件</strong>—&gt;<strong>另存为</strong>—&gt;<strong>内嵌式PostScript</strong> 格式</li>
<li>Duang！就这样完成了，尽情滴放大她，也不会失真啦！</li>
<li>Ps.如果是在 LaTex 写作中，插入图片时，可以直接选择使用 PDF 格式哦！就省去了转为 EPS 格式的过程（如果没有 DC 软件的话）</li>
</ol>
<hr>
<p>最后，本博客开始正式运营，相比于之前使用庞杂、臃肿的 WorldPress，我并不需要多么复杂的后台管理功能，能让我安心地在本地快快乐乐写文字输出就ok，还没有了租服务器的额外开销（汇率提升，我的钱包实在扛不住了，太贵了）</p>
<hr>
<ul>
<li>发现的一个小问题就是，插入pdf，在手机端没法查看，所以之后可以尝试用 html 形式专门做成一个界面栏目，像 VC 比赛 demo 界面那样。clone一下大神们的个人主页</li>
<li>或者再找找看 .md 格式怎么写一个漂亮的简历</li>
</ul>
<hr>
<h1 id="找到了！！！"><a href="#找到了！！！" class="headerlink" title="找到了！！！"></a>找到了！！！</h1><ul>
<li><a href="http://www.pdfdo.com/pdf-to-html.aspx" target="_blank" rel="noopener">pdf 转 html 文件格式</a></li>
<li>使用方法：导入 pdf 之后，转换，点击下载（千万别直接右键保存，那样不完整！）</li>
<li>然后本地 WebStorm 打开，在 body 标签体后面加一对 center 标签，就能全体文字 &amp; 图片居中啦！</li>
</ul>
<hr>
<ul>
<li>另一个<a href="https://www.aconvert.com/cn/pdf/" target="_blank" rel="noopener"><strong>大全能格式转换工具网站</strong></a></li>
<li>有一个致命缺点：转出来的 html 文件在 body 中间加入 center 之后，文字和图片会歪，很不理想，不知什么原因；</li>
</ul>
<ul>
<li><del><strong>原因找到了：</strong></del></li>
<li><del>这个网站没有直接提供下载功能，先点击压缩，然后选择下载压缩文件</del></li>
<li><del>千万别直接打开htm之后，直接右键保存，注意到直接保存的是 .htm 格式，不是 .html （这个在之前的侧边栏环节也遇到这个问题），查查什么区别。</del></li>
<li><del>这个网站下载的很慢，还是不如上面那个网站。</del></li>
<li>这个网站还是垃圾，下载了之后内部语法乱的一笔，只有单一的一个html文件，图片也不知道给👴整哪里去了，center 居中之后又是乱糟糟，别玩了，就用上面那个吧，太漂亮了！</li>
</ul>
<p>真的 OK 了，本文结束。</p>
<p>真的结束了#2</p>
<p>以上</p>
<p>June / 14 / 2020</p>
<p>@PT</p>
]]></content>
      <categories>
        <category>写作</category>
      </categories>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title>python 切片的一些混淆点（备忘）</title>
    <url>/2023/07/08/python%20%E5%88%87%E7%89%87%E7%9A%84%E4%B8%80%E4%BA%9B%E6%B7%B7%E6%B7%86%E7%82%B9%EF%BC%88%E5%A4%87%E5%BF%98%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="1-经常混淆的py切片细节"><a href="#1-经常混淆的py切片细节" class="headerlink" title="1.经常混淆的py切片细节"></a>1.经常混淆的py切片细节</h2><a id="more"></a>

<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200716200314.png" alt="image-20200716200313100"></p>
<h1 id="python-中的-1-和-1"><a href="#python-中的-1-和-1" class="headerlink" title="python 中的 [:-1] 和 [::-1]"></a>python 中的 [:-1] 和 [::-1]</h1><ul>
<li><p><a href="https://www.runoob.com/note/51257" target="_blank" rel="noopener">https://www.runoob.com/note/51257</a></p>
</li>
<li><pre><code class="python">a=<span class="string">'python'</span>
b=a[::<span class="number">-1</span>]
print(b) <span class="comment">#nohtyp</span>
c=a[::<span class="number">-2</span>]
print(c) <span class="comment">#nhy</span>
<span class="comment">#从后往前数的话，最后一个位置为-1</span>
d=a[:<span class="number">-1</span>]  <span class="comment">#从位置0到位置-1之前的数</span>
print(d)  <span class="comment">#pytho</span>
e=a[:<span class="number">-2</span>]  <span class="comment">#从位置0到位置-2之前的数</span>
print(e)  <span class="comment">#pyth</span>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">* &#96;&#96;&#96;python</span><br><span class="line">  b &#x3D; a[i:j]   # 表示复制a[i]到a[j-1]，以生成新的list对象</span><br><span class="line">  </span><br><span class="line">  a &#x3D; [0,1,2,3,4,5,6,7,8,9]</span><br><span class="line">  b &#x3D; a[1:3]   # [1,2]</span><br><span class="line">  </span><br><span class="line">  # 当i缺省时，默认为0，即 a[:3]相当于 a[0:3]</span><br><span class="line">  # 当j缺省时，默认为len(alist), 即a[1:]相当于a[1:10]</span><br><span class="line">  # 当i,j都缺省时，a[:]就相当于完整复制一份a</span><br><span class="line">  </span><br><span class="line">  b &#x3D; a[i:j:s]    # 表示：i,j与上面的一样，但s表示步进，缺省为1.</span><br><span class="line">  # 所以a[i:j:1]相当于a[i:j]</span><br><span class="line">  </span><br><span class="line">  # 当s&lt;0时，i缺省时，默认为-1. j缺省时，默认为-len(a)-1</span><br><span class="line">  # 所以a[::-1]相当于 a[-1:-len(a)-1:-1]，也就是从最后一个元素到第一个元素复制一遍，即倒序。</span><br></pre></td></tr></table></figure>


</code></pre>
</li>
</ul>
<hr>
<h2 id="2-关于-torch-的pad：full模式（相对于tf中的same模式）"><a href="#2-关于-torch-的pad：full模式（相对于tf中的same模式）" class="headerlink" title="2.关于 torch 的pad：full模式（相对于tf中的same模式）"></a>2.关于 torch 的pad：full模式（相对于tf中的same模式）</h2><p>$$<br>              H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]<br>                        \times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor</p>
<pre><code>W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1]
          \times (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor</code></pre><p>$$</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200801174740.png" alt="image-20200801174738560"></p>
<h2 id="相比tensorflow，PyTorch需要用户清楚的知道的自己的卷积核选取对结果的影响。"><a href="#相比tensorflow，PyTorch需要用户清楚的知道的自己的卷积核选取对结果的影响。" class="headerlink" title="相比tensorflow，PyTorch需要用户清楚的知道的自己的卷积核选取对结果的影响。"></a>相比tensorflow，PyTorch需要用户清楚的知道的自己的卷积核选取对结果的影响。</h2><ul>
<li><strong>简单一点</strong>：先只看 t = kernel【0】// 2；</li>
<li>Kernel【0】为奇数，那么padding就等于 t;</li>
<li>否则，kernel【0】为偶数，那么padding就等于 【t - 1】</li>
</ul>
<hr>
<h2 id="3-反卷积"><a href="#3-反卷积" class="headerlink" title="3.反卷积"></a>3.反卷积</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200801184750.png" alt="image-20200801184748828"></p>
<p><a href="https://blog.csdn.net/g11d111/article/details/82665265" target="_blank" rel="noopener">https://blog.csdn.net/g11d111/article/details/82665265</a></p>
<ul>
<li>反卷积这部分用的少，公式其实就是正卷积中，in 和 out 对调；</li>
<li>只不过 反卷积没有了 正卷积 中 的 <strong>向下取整</strong> 的操作</li>
<li>所以在反卷积中， 需要按规矩公式，简单计算一下padding 尺寸参数；</li>
<li>最简单就是用局部代码输出看一下结果，看是否符合输入输出尺寸要求</li>
</ul>
<hr>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200801193707.png" alt="image-20200801193706275"></p>
<p><a href="https://blog.csdn.net/m0_37586991/article/details/87855342" target="_blank" rel="noopener">https://blog.csdn.net/m0_37586991/article/details/87855342</a></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200801194743.png" alt="image-20200801194739307"></p>
<hr>
]]></content>
      <categories>
        <category>-[python]</category>
      </categories>
      <tags>
        <tag>-[python] -[编程]</tag>
      </tags>
  </entry>
  <entry>
    <title>1213组会汇报</title>
    <url>/2023/07/08/1213%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="1213组会汇报"><a href="#1213组会汇报" class="headerlink" title="1213组会汇报"></a>1213组会汇报</h2><hr>
<h2 id="Done："><a href="#Done：" class="headerlink" title="Done："></a>Done：</h2><ol>
<li>PGAN 训起来了，用的 AiShell所有 train 训练集，测试集没混进去</li>
<li>Deep Voice 的 train1 提取 ppgs 的网络也训起来了，用的Timit，自带 Phonetic 信息</li>
<li>VCTK-Corpus-0.92.zip 还在043下载，速度比较慢</li>
</ol>
<hr>
<h2 id="Did-amp-Doing："><a href="#Did-amp-Doing：" class="headerlink" title="Did &amp; Doing："></a>Did &amp; Doing：</h2><ol>
<li>StarGAN-VC <strong>2</strong> 的<strong>复现环节</strong>，</li>
<li>把 G 和 D 网络结构写好了，还差 一个 dataloader 写一下，另外 Solver 部分模仿着写一下就差不多了</li>
<li>特征用的是 PGAN 处理的方式（整体训练集做 均值&amp;方差 归一化处理），为了和后端配合上，直接用vocoder的训练数据</li>
<li>🌟<strong>困难点</strong>：<ol>
<li>跟进了 <strong>CIN</strong> 在图像领域的具体实现，照猫画虎写出来（IN的改进，加上 特征偏置）</li>
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201213171057.png" alt="image-20201213171056728"></li>
<li>论文中提到的 GSP ，查了别人不同版本代码，挑了一个合适的方式移植进来</li>
<li>论文的尺寸结构 是【36维，128帧】，和咱们 mel 80 不对等，进行尺寸匹配计算</li>
</ol>
</li>
</ol>
<hr>
<h2 id="论文方面："><a href="#论文方面：" class="headerlink" title="论文方面："></a>论文方面：</h2><ol>
<li><p>近期看到很多围绕</p>
<ol>
<li><p>“<strong>Speaker embedding 循环损失</strong>” </p>
<ul>
<li>「OPTIMIZING VOICE CONVERSION NETWORK WITH CYCLE CONSISTENCY LOSS OF SPEAKER IDENTITY」</li>
</ul>
</li>
<li><p>❤️ “<strong>stargan-vc2 进一步改造</strong>”、——（投到了 <strong>2021 icassp</strong>）</p>
<ul>
<li><p>WEIGHT ADAPTIVE INSTANCE NORMALIZATION（<strong>WAStarGAN-VC</strong>）</p>
</li>
<li><p>「TOWARDS LOW-RESOURCE STARGAN VOICE CONVERSION USING WEIGHT ADAPTIVE INSTANCE NORMALIZATION」</p>
</li>
</ul>
</li>
<li><p>“<strong>PPG实现多对多转换</strong>” 「Phonetic Posteriorgrams based Many-to-Many Singing Voice Conversion via Adversarial Training」<strong>的论文</strong></p>
</li>
</ol>
</li>
</ol>
<ol start="2">
<li><p>主要的创新点都不多，其中 <strong>stargan-vc2改进</strong>，基于的是 其中提到的 <strong>CIN</strong> 结构，再把 <strong>one-hot</strong> 类型改进成 同步训练的 <strong>Speaker Encoder 出来的 Speaker Embedding</strong> 来改；但是声码器还是用的 WORLD</p>
<ol>
<li><strong>demo</strong>网址 ： <a href="https://minidemo.dcs.shef.ac.uk/wastarganvc/" target="_blank" rel="noopener">https://minidemo.dcs.shef.ac.uk/wastarganvc/</a></li>
</ol>
</li>
</ol>
<ol start="3">
<li>咱们的想法算是<strong>综合上面两篇</strong></li>
</ol>
<p>（论文具体细节内容还没全部看完，接下来看一下）</p>
<hr>
<h2 id="下周安排"><a href="#下周安排" class="headerlink" title="下周安排"></a>下周安排</h2><ol>
<li>把 <strong>dataloader</strong> 和 <strong>solver</strong> 尽快写完，让代码先跑通</li>
<li>先试 one-hot 类型的，再试 AiShell3 的 Speaker encoder 方式提取的【256】维 embedding<ul>
<li>嵌入 embedding 两个可能的实现方式路线：<ol>
<li>同样用 CIN；但是感觉不太符合 CIN 本意是针对 one-hot 而设计的初衷</li>
<li>不用CIN，直接在 通道层 方向上拼接：像一代那样，但是 2 代 这里已经 reshape 成一维卷积层了</li>
<li>可以分别做实验看效果，或者直接选第二个就好</li>
</ol>
</li>
</ul>
</li>
<li>再跟进一下 看近期有没有新的针对 「隐变量」路线的VC论文的代码</li>
</ol>
<hr>
]]></content>
  </entry>
  <entry>
    <title>1203组会汇报进度</title>
    <url>/2023/07/08/1203%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5%E8%BF%9B%E5%BA%A6/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="1206工作进度："><a href="#1206工作进度：" class="headerlink" title="1206工作进度："></a>1206工作进度：</h2><ol>
<li>很多论文 ——看了</li>
<li>很多代码 ——看了、改了、写了</li>
<li>很多想法 ——可行与否缺乏交流</li>
<li>🌟没有机器 ——用单位机器被 <strong>Kill</strong> 被<strong>批评</strong></li>
</ol>
<hr>
<ol>
<li>初步想法——在<strong>stargan-vc2</strong>基础上改进，侧重点还是 <strong>少数据量</strong> &amp;&amp; <strong>快速</strong> </li>
<li>用 <strong>PGAN</strong> + Mel 处理，Embedding 部分调用 AiShell （<strong>还是得咱们自己重训</strong>，特征处理方式和PGAN不一样）（<strong>Mel 归一化方式不同</strong>）</li>
<li>另外引入 “ <strong>Speaker Embedding</strong>”<strong>循环损失</strong>，提升说话人<strong>相似度</strong></li>
<li>最后再引入 洪丰 的“相对损失”来替代<strong>MSE</strong>，看效果提升度</li>
<li><ul>
<li>Speaker Embedding 尺寸 <strong>256</strong></li>
<li>PGAN 尺寸 <strong>80 mel</strong></li>
<li>StarGAN 网络结构也得跟着改动了，为了迎合 两者输入特征尺寸</li>
</ul>
</li>
</ol>
<ol start="6">
<li><p>PPG 路线：没有机器训，就在 Deep_voice_conversion 基础上改</p>
<ul>
<li></li>
<li><p>但是不知是论文读的少还是什么原因：</p>
</li>
<li><p>所有 PPG –&gt; Mel 环节，都是用的自身转自身（<strong>就是只能单一目标人转换</strong>）</p>
</li>
<li><p>🌟如果加 speaker embedding 的思路来做<strong>多目标人</strong>，那是不是就只能用 <strong>GAN</strong> 来做？</p>
</li>
<li></li>
<li></li>
<li><p>（这条路线真的好复杂，想训 Kaldi-librispeech 来学做 ppg 提取，完全训不起来，cpu占用太高了）</p>
</li>
<li><p>（有 fac-via-ppg 项目，做的口音转换，用的pykaldi，但是没法引入 VC）</p>
</li>
</ul>
</li>
</ol>
<hr>
<ol>
<li><h4 id="guanlongzhao-fac-via-ppg"><a href="#guanlongzhao-fac-via-ppg" class="headerlink" title="guanlongzhao/fac-via-ppg"></a><a href="https://github.com/guanlongzhao" target="_blank" rel="noopener">guanlongzhao</a>/<strong><a href="https://github.com/guanlongzhao/fac-via-ppg" target="_blank" rel="noopener">fac-via-ppg</a></strong></h4><ol>
<li>这个任务，实现了用PyKaldi提取PPGs信息（可套来用）、PPGs 转 Mel（咱们做的是<strong>口音转换，看怎么通过改 Loss 成语音转换</strong>）、WavaGlow后端（有预训练的模型，256维度？）</li>
<li>这个的训练需要自己手动整理 file list，等我整理一下，自己训一下看成不成功，应该问题好解决</li>
<li>准备用aishell3 数据集试试中文提取ppg效果</li>
</ol>
</li>
</ol>
<ol start="2">
<li><h3 id="sos1sos2Sixteen-aishell-3-baseline-fc"><a href="#sos1sos2Sixteen-aishell-3-baseline-fc" class="headerlink" title="sos1sos2Sixteen/aishell-3-baseline-fc"></a><a href="https://github.com/sos1sos2Sixteen" target="_blank" rel="noopener">sos1sos2Sixteen</a>/<strong><a href="https://github.com/sos1sos2Sixteen/aishell-3-baseline-fc" target="_blank" rel="noopener">aishell-3-baseline-fc</a></strong></h3><ol>
<li><p>从❤️ <a href="https://github.com/caizexin/tf_multispeakerTTS_fc" target="_blank" rel="noopener">这里</a> 抄的，原本是做的英文，改成中文</p>
</li>
<li><p>这个AiShell3 的多说话人合成任务，实质是 Fork 自另一份<a href="https://github.com/caizexin" target="_blank" rel="noopener">caizexin</a>/<strong><a href="https://github.com/caizexin/tf_multispeakerTTS_fc" target="_blank" rel="noopener">tf_multispeakerTTS_fc</a></strong></p>
</li>
<li><p>不得不说，文档真的很差，看不清，折腾好几天起不来</p>
</li>
<li><p>🌟问题：</p>
<ol>
<li>数据的放置位置，处理顺序全都没讲</li>
<li>给的<strong>预训练模型</strong>都是另一份<strong>英文</strong>的，并没有任何体现<strong>中文aishell3</strong>的存在感的地方</li>
<li>尝试直接跑 预训练模型，起不来，代码有问题！真的是代码有问题，传参时缺参数，一时半会不知怎么加上去，改了N多代码还是不行</li>
<li>不得不说，<strong>Tensorflow 问题真的比Torch多多了</strong>！</li>
</ol>
</li>
<li><p>🌟最新进展：demo .ipynb 跑通了！：</p>
<ol>
<li><p>将 wavernn 代码 vocoder/models/fatchord_version.py 中所有 .cuda() 注释掉</p>
</li>
<li><p>将三个 ckpt 文件放入对应文件夹：</p>
<ol>
<li>pertained/tacotron_ckpt  </li>
<li>tf_multispeaker/vocoder/saved_models/<strong>aishell/aishell_ssb.pt</strong></li>
<li>❤️这里的<strong>aishell/aishell_ssb.pt</strong>手动改名就行</li>
</ol>
</li>
<li><h6 id="注释掉【51】："><a href="#注释掉【51】：" class="headerlink" title="注释掉【51】："></a>注释掉【51】：</h6></li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># def mel2audio(mel) : </span><br><span class="line">#     return np.random.randn(14000)[None]</span><br></pre></td></tr></table></figure>

<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201204002225.png" alt="image-20201204002224675"></p>
</li>
<li><p><strong>display 函数</strong>是<strong>jupter notebook</strong>环境下的一个函数接口：</p>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201204002343.png" alt="image-20201204002342062"></p>
</li>
<li><p><img src="/2023/07/08/1203%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5%E8%BF%9B%E5%BA%A6/huangshengjie/Documents/2020/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99hexo%E6%B5%8B%E8%AF%95/Typora%E5%8D%9A%E5%AE%A2%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9/image-20201204002439910.png" alt="image-20201204002439910"></p>
<ol start="8">
<li><p>文本送进去的是音素，所以生成语音需要的是音素信息，这个转换不用管，【可以用声韵母训练，用pypinyin做标注，或者找对应的音素字典就好】<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201204002637.png" alt="image-20201204002635896"></p>
</li>
<li><p>说话人 embedding 这里aishell3 采用的是 ，先用预训练的 speaker encoder 提前提取所有语音embeddinmg，然后碎每个人的embedding做一个均值处理；而且这里demo提供的 embedding提取，是已经准备好的；</p>
</li>
<li><p>在原版的代码中，这里是需要手工提取的，这个回头是我钻研的重点，看怎么准备数据和文本，然后训起来再说；</p>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201204002900.png" alt="image-20201204002857320"></p>
</li>
</ol>
</li>
</ol>
</li>
<li><h3 id="fatchord-WaveRNN"><a href="#fatchord-WaveRNN" class="headerlink" title="fatchord/WaveRNN"></a><a href="https://github.com/fatchord" target="_blank" rel="noopener">fatchord</a>/<strong><a href="https://github.com/fatchord/WaveRNN" target="_blank" rel="noopener">WaveRNN</a></strong></h3><ol>
<li>这个非常清楚，代码写的很工整</li>
<li>主要特色就是他的 WaveRNN声码器 + tacotron 前端（更改了 attention 部分机制）</li>
<li>目前在训练 taco前段部分，用的是 LjSpeech </li>
</ol>
</li>
</ol>
<ol start="4">
<li><h5 id="CorentinJ-Real-Time-Voice-Cloning"><a href="#CorentinJ-Real-Time-Voice-Cloning" class="headerlink" title="CorentinJ/Real-Time-Voice-Cloning"></a><a href="https://github.com/CorentinJ" target="_blank" rel="noopener">CorentinJ</a>/<strong><a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning" target="_blank" rel="noopener">Real-Time-Voice-Cloning</a></strong></h5><ol>
<li>这份是比较经典的多说话人合成任务，里面有 Speaker Encoder，还没跑，有参考价值</li>
</ol>
</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>《240921-硕士论文致谢补档》</title>
    <url>/2024/09/21/240921-%E8%87%B4%E8%B0%A2%E8%A1%A5%E6%A1%A3-101/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>2024.09.21</p>
<ul>
<li>度过了一个阶段性的锻炼，难得有时间停下脚步，收拾收拾心情，好再出发。</li>
<li>考古自己过往的成果文件，发现之前在知网选择了隐藏致谢。便站定，再次翻看起了自己曾经真情流露的文字。再次阅读，还是能让我感到欣慰，并感慨万千。虽然三年的硕士生涯，在前两年把我折磨得天昏地暗，但依然很感谢那段日子里，努力向上攀爬的自己。这不像是军训后普遍存在的那种“斯德哥尔摩综合征”式的回味，而是对命运就此被改变的那种“事后烟”的舒畅，开心、疲惫、后怕、敬畏多种情愫交织杂糅在一起，构成了那段时间的高度美好概括。</li>
</ul>
<a id="more"></a>

<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/image/%E7%A1%95%E5%A3%AB%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87%E8%87%B4%E8%B0%A2_%E9%A1%B5%E9%9D%A2_1.png" alt="硕士毕业论文致谢_页面_1"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/image/a9de23c6f44da0d883122e491a7e93a.png" alt="1"></p>
<ul>
<li>致谢里没写的，还有在研一时，枕头旁掉落的一堆头发、在长期熬夜久坐后的腰肌劳损，在研二时克服疫情天天外出实习的波折困难。</li>
</ul>
<p>算了，还有好多可以唠嗑的，今天很晚了，要睡了，早睡真好~</p>
<p>祝福明天的自己更美好、更优秀、更幸运。也希望能遇到双向奔赴的爱情。</p>
]]></content>
      <categories>
        <category>作家计划</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>《END-TO-END ACCENT CONVERSION WITHOUT USING NATIVE UTTERANCES》论文总结</title>
    <url>/2020/08/17/91-%E3%80%8AEND-TO-END%20ACCENT%20CONVERSION%20WITHOUT%20USING%20NATIVE%20UTTERANCES%E3%80%8B%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200816133725.png" alt="image-20200816133724321"></p>
<p>展示demo：<a href="https://liusongxiang.github.io/end2endAC/" target="_blank" rel="noopener">https://liusongxiang.github.io/end2endAC/</a></p>
<a id="more"></a>

<ul>
<li>是在 VC 之上，再做的进一步 口音修正：</li>
<li>source 语音在带有口音（e.g. 咖喱味的英语）的前提下，做VC，但是 我们除了想要 target 的身份，还要 <strong>目标说话人口音</strong>（e.g. 纯正的母语英语）</li>
<li>所提到的整个网络模型，综合了 TTS 、VC、ASR 的方法，也有一种采众家之长的意思，但是麻烦的是，四个模型部分，需要分别训练</li>
</ul>
<hr>
<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><ol>
<li>传统的口音转换：<ol>
<li>内容（content）、发音（pronounciation） 不变</li>
<li>把 本地source说话人 的口音（accent），转换成 非本地target说话人 的口音</li>
</ol>
</li>
<li>本文尝试source非母语：北印度人说的英语口音—&gt;北美正统英语口音</li>
<li>传统的局限：做转换时，需要具备的条件：<ol>
<li>需要有 印度人说的英语 作为source</li>
<li>需要有美国人说的英语作为 target</li>
<li>这导致了，在实际上也用途时，很臃肿难用</li>
</ol>
</li>
<li>本文想法：<ol>
<li>在转换阶段，提出 端到端 方法，使得不需要 美国人英语target 也能做正确的 AC （accent conversion）</li>
</ol>
</li>
<li>贡献点：<ol>
<li>说是目前为止第一家实现在 转换阶段 不需要目标（美国target英语语音）就能实现转换的方案</li>
<li>细化实现了对 <strong>韵律特征</strong> 的建模：（例如说话速度和持续时间）</li>
<li>non-parallel</li>
</ol>
</li>
</ol>
<hr>
<h2 id="网络结构：（四部分）"><a href="#网络结构：（四部分）" class="headerlink" title="网络结构：（四部分）"></a>网络结构：（四部分）</h2><ol>
<li>a speaker encoder</li>
<li>a multi-speaker TTS model</li>
<li>an accented ASR model</li>
<li>a neural vocoder</li>
</ol>
<hr>
<ol>
<li>encoder 部分作用：生成speaker-embedding</li>
<li>TTS 部分 用的是tacotron2（音素 + embedding 预测 mel）</li>
<li></li>
</ol>
<hr>
<h2 id="Speaker-encoder"><a href="#Speaker-encoder" class="headerlink" title="Speaker encoder"></a>Speaker encoder</h2><ol>
<li>引用的模型：<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200816222235.png" alt="image-20200816222233997"></li>
<li>作用是：生成 <strong>speaker-embedding</strong>，用来和 TTS 结合，保证了生成语音 和 target 说话人 身份一致（类似VC和TTS结合？）</li>
<li>GE2E（generalized end-to-end）speaker veriﬁcation loss</li>
<li><strong>Baseline训练过程</strong>：</li>
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817111542.png" alt="image-20200817111540876"></li>
<li><strong>训练阶段训练两个转换模型</strong>：（DTW）<ol>
<li>Trans 1 : 将 <strong>印度英语</strong> 的ppg 和 <strong>美式英语</strong> 的ppg 利用 DTW 做对齐</li>
<li>Trans 2 ：将 <strong>印度英语</strong> 的 ppg 和 mel谱 做对应匹配</li>
<li>转换阶段：<ol>
<li>输入 <strong>印度英语</strong>到 Trans1</li>
<li>得到 <strong>美式英语</strong>ppg</li>
<li>将ppg 继续送入 Trans2，得到对应的 mel谱</li>
<li>用wave-Net 生成语音波形</li>
</ol>
</li>
</ol>
</li>
</ol>
<hr>
<h2 id="Multi-speaker-TTS-model-amp-amp-Multi-task-accented-ASR-model"><a href="#Multi-speaker-TTS-model-amp-amp-Multi-task-accented-ASR-model" class="headerlink" title="Multi-speaker TTS model  &amp;&amp;  Multi-task accented ASR model"></a>Multi-speaker TTS model  &amp;&amp;  Multi-task accented ASR model</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817114458.png" alt="image-20200817114456342"></p>
<ol>
<li>其中的TTS网络，摘用的这篇：（<strong>attention-based</strong> encoder-decoder model）<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817114716.png" alt="image-20200817114714573"></li>
<li><strong>text transcripts</strong> into <strong>phoneme sequences</strong>：TTS 中用phone 序列，加速收敛过程；</li>
</ol>
<h3 id="口音识别：Multi-task-accented-ASR-model"><a href="#口音识别：Multi-task-accented-ASR-model" class="headerlink" title="口音识别：Multi-task accented ASR model"></a>口音识别：Multi-task accented ASR model</h3><p>训练思路：</p>
<ol>
<li>前面有，用单说话人（美式英语）训练好的 <strong>speaker encoder</strong></li>
<li>接着有 用全部都是（美式英语）训练的 多说话人 <strong>TTS</strong> <strong>模型</strong>（TTS-loss）</li>
<li>在上面两个步骤基础上，本环节 用多个 <strong>美式英语</strong> &amp;&amp; 一个 <strong>印度英语</strong> 一起进行 <strong>多说话人</strong> 预训练</li>
<li>完成之后，再 <strong>全部</strong> 换成 <strong>印度口音英语多说话人</strong> 进行调整</li>
</ol>
<p>实践中的细节：</p>
<ol>
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817143548.png" alt="image-20200817143546689">)<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817143604.png" alt="image-20200817143601305"></li>
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817143645.png" alt="image-20200817140608795"></li>
</ol>
<p><strong>结构上</strong>：</p>
<ul>
<li>E2E attentioned encoder-decoder 结构：如上图（引用以前的文章思路）</li>
</ul>
<p><strong>改进 1 ：</strong></p>
<ul>
<li>为了增加 <strong>训练稳定性</strong>，参考了文献：<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817144039.png" alt="image-20200817144038371"></li>
<li>在 Encoder 之前，增加了一个 <strong>全连接</strong>，并增加了一个 <strong>CTC loss</strong></li>
</ul>
<p><strong>改进 2 ：</strong></p>
<ul>
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817144505.png" alt="image-20200817144503756"></li>
<li>在口音问题的思路上， 借鉴这篇论文：</li>
<li>在送给 ASR 之前，<strong>每帧</strong> 都concate上一个 <strong>accent embedding</strong>（这是由第一个步骤 <strong>speaker encoder</strong> 里面产生的 <strong>speaker-embedding</strong> 做一个 对单人所有语音的embedding整体均值 的结果）</li>
<li>并且在ASR的encoder最开始，再增加一个 <strong>accent classifier 口音分类器</strong>（上一步已经加过一次 FC 了）；这也能使面对口音时，有更好的健壮性</li>
</ul>
<hr>
<h2 id="🌟-Loss-环节"><a href="#🌟-Loss-环节" class="headerlink" title="🌟 Loss 环节"></a>🌟 Loss 环节</h2><ul>
<li><p>λ 1= <strong>0.5</strong>, λ 2= 0.1, λ 3= <strong>0.5</strong> and λ 4= 0.1</p>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817145536.png" alt="image-20200817145535742"></p>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817145554.png" alt="image-20200817145553156"></p>
</li>
</ul>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817145627.png" alt="image-20200817145626236"></p>
<ol>
<li><strong>TTSE-loss</strong>：<strong>MSE损失</strong>，对象是：1⃣️上一步已训好的 TTS 产生的声学序列（美式英语） &amp;&amp; ASR识别出的声学序列 做一个 <strong>均方差损失</strong></li>
<li><strong>CE-loss</strong>：phoneme label prediction （phone音素 的标签预测损失）</li>
<li><strong>ACC-loss</strong>：口音分类器的损失（类似stargan-vc里面的 speaker-classifier）</li>
</ol>
<hr>
<h2 id="4-后端声码器"><a href="#4-后端声码器" class="headerlink" title="4. 后端声码器"></a>4. 后端声码器</h2><ul>
<li>本文直接采用开源的 <a href="https://github.com/fatchord/WaveRNN" target="_blank" rel="noopener">WaveRNN</a> </li>
<li>没有额外加任何的embedding信息，觉得说 mel频谱已经能包含所有需要的声学细节了</li>
<li>语料：全部采用 <strong>美式英语</strong> 数据来训</li>
</ul>
<hr>
<h2 id="5-转换阶段"><a href="#5-转换阶段" class="headerlink" title="5. 转换阶段"></a>5. 转换阶段</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817153318.png" alt="image-20200817153317898"></p>
<ol>
<li>hs：一段语音，经过 speaker-encoder之后得到的 speaker-embedding</li>
<li>accent-embedding：针对同一个人的所有语音，做所有的 <strong>speaker-embedding</strong>的 <strong>均值</strong>（多个语音理解为多个通道计算）</li>
<li>（Accent embedding is the averaged speaker embeddings of the non-native-accented speaker.）</li>
<li>注：accnet-embedding 是 <strong>逐帧都要插入</strong>，speaker-embedding 是最后再concate上</li>
</ol>
<hr>
<h2 id="一些实现参数细节"><a href="#一些实现参数细节" class="headerlink" title="一些实现参数细节"></a>一些实现参数细节</h2><p>5.1部分（不复赘述）</p>
<hr>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>（笑容满面实验，是去掉 <strong>accent embedding</strong> 和 <strong>accent classiﬁer</strong>）</p>
<ul>
<li><p>MOS得分很高</p>
</li>
<li><p>相似性上，比baseline差很多（作者猜测是数据量还是不够大的缘故）</p>
</li>
<li><p>（但他们用的VCTK 比 VCC 的几十条还是多很多的）</p>
</li>
<li></li>
</ul>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817161213.png" alt="image-20200817161212167"></p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>VC</tag>
        <tag>论文阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>《《ONE-SHOT VOICE CONVERSION USING STAR-GAN》》论文总结</title>
    <url>/2020/07/14/90-%E3%80%8AONE-SHOT%20VOICE%20CONVERSION%20USING%20STAR-GAN%E3%80%8B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712160845.png" alt="image-20200712160843097"></p>
<a id="more"></a>

<ol>
<li><h2 id="解决的问题："><a href="#解决的问题：" class="headerlink" title="解决的问题："></a>解决的问题：</h2><ol>
<li>在原本的StarGan-VC中，实现了“<strong>未知speaker：source/target 都未知</strong>”的转换。称作“<strong>One-Shot</strong>”</li>
<li>其他文章其实也有做过类似功能：基于“<strong>超大数据集的  VC 模型</strong>”做自适应adaption调整：比如之前谢磊团队那篇 <strong>对WaveNet的改进模型</strong>，在处理未知说话人时，采用的是 额外20到50条数据的进一步收敛；</li>
</ol>
</li>
<li><h2 id="采用的主要方法："><a href="#采用的主要方法：" class="headerlink" title="采用的主要方法："></a>采用的主要方法：</h2><ol>
<li>把说话人信息看作 embedding。</li>
<li>但是不同于原本 StarGan-VC 代码实现中（非官方）用 One-Hot来做embedding。</li>
<li>也不是用的后来咱们讨论中，改用 embedding_lookup（）的方式（虽然已经比One-Hot concate 方式要好很多了）</li>
<li>而是采用2018年google的一篇文章，提取embedding的单独网络；（<strong>Global Style Token (GST)</strong>），用这个网络提取出来的embedding信息，可以表征说话人身份信息。</li>
<li>具体细节接下来说：</li>
</ol>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712162828.png" alt="image-20200712162827085"></p>
</li>
<li><h2 id="有价值的细节："><a href="#有价值的细节：" class="headerlink" title="有价值的细节："></a>有价值的细节：</h2><ol>
<li>这个GST 训练时，先有一堆说话人，每个人有很多数据（1⃣️）；</li>
<li>然后这个网络的功能就是：能把一个新来的 集合外数据（人/内容），扔进去，照样得到一个 speaker_embedding 信息；</li>
<li>这个embedding信息怎么来的呢？原来是由模型（1⃣️）训练集中的说话人embedding 融合出来的；所以最终的效果上，会是：新说话人声音特征，由训练集说话人特征组合而成；</li>
<li>这个GST当中， speaker ID 实现上，同样采用 one-hot 形式。</li>
</ol>
</li>
</ol>
<ol start="4">
<li><h2 id="以上三点，其实都只是前人的工作，本文拿来创新性应用。"><a href="#以上三点，其实都只是前人的工作，本文拿来创新性应用。" class="headerlink" title="以上三点，其实都只是前人的工作，本文拿来创新性应用。"></a>以上三点，其实都只是前人的工作，本文拿来创新性应用。</h2><ol>
<li>本文的细节创新：</li>
</ol>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712164238.png" alt="image-20200712164236074"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712164302.png" alt="image-20200712164301003"></p>
<ol>
<li>原本的 StarGan-VC 中，在Generator部分 的BottelNect部分，采用正常的 【Conv，Norm，GLU】结构，其中的Conv采用 <strong>5 个channel</strong>；</li>
<li>本文作者实验证明，这个 5 channel 太小了，影响了 reconstruction 音频质量；</li>
<li>但是尝试放大这个channel数，会发现有“<strong>信息泄露：information leakage</strong>”，有点像信号处理中的“<strong>频率泄漏：frequency leak</strong>”（后者采用加窗的方式规避这个问题）；</li>
<li>所谓的泄漏，就是出现了无关的信息：Generator 没能把source中的身份信息过滤干净，最后的声音四不像；（频率泄漏则是，在没加窗函数之前，做FFT会出现 本没有的 频率）<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712165144.png" alt="image-20200712165142509"></li>
<li>最后的效果上：<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712165232.png" alt="image-20200712165230066"></li>
<li>作者说，用了这种方式（SoftMax 代替 conv-2d 做 BottleNeck），在转换结果上，<strong>共振峰频率</strong>（<strong>frequencies of formants</strong>）会低一点：显示在能量图上，就是高频部分（上面）颜色会浅一点。</li>
<li><strong>🌟疑问点</strong>：这个 <strong>共振峰频率</strong> 低一点，<strong>能说明什么</strong>？？？？这样就能说明 说话内容信息泄漏会少一点吗？没搞懂；</li>
</ol>
</li>
</ol>
<h2 id="5-另一个操作改进点："><a href="#5-另一个操作改进点：" class="headerlink" title="5.  另一个操作改进点："></a>5.  另一个操作改进点：</h2><ol>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712172438.png" alt="image-20200712172437528"></p>
</li>
<li><p>在Generator的修改上如图：</p>
</li>
<li><p>对比原型：<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712171700.png" alt="image-20200712171657257"></p>
</li>
<li><p>小细节：</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200713175103.png" alt="image-20200713175101061"></p>
<ul>
<li><p>偶数：同时更新 D &amp;&amp; G</p>
</li>
<li><p>奇数：只更新 D</p>
</li>
<li><p>特征选用上：由实验经验，<strong>由36维 MFCC</strong> 改为用 <strong>96维 sp 谱包络（96-bin Mel spectral envelope）</strong></p>
</li>
<li><p>（这个和合成的应用上也有呼应， Mel 的训练合成，比MFCC reconstrruction 效果要好，更深原理 <strong>模糊</strong>）</p>
</li>
</ul>
</li>
</ol>
<h2 id="6-结果上："><a href="#6-结果上：" class="headerlink" title="6. 结果上："></a>6. 结果上：</h2><p>其实就是 VC 领域两个主观评价指标：Reconstruction 质量 和 Conversion 质量。</p>
<ol>
<li>这份Demo里，Reconstruction 的效果也不怎么好，在Conversion 转换效果上还凑合；这个和StarGan-VC 差不多；</li>
<li>Reconstruction 上，target 已知或 “增量式训练”过，数据效果上，提升不少；Conversion 效果也大差不差；<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712181558.png" alt="image-20200712181557111"></li>
</ol>
<h2 id="7-VC改进思路小结："><a href="#7-VC改进思路小结：" class="headerlink" title="7. VC改进思路小结："></a>7. VC改进思路小结：</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200713132212.png" alt="image-20200713132210312"></p>
<ul>
<li>在想能否写<strong>WorkShop</strong>论文</li>
<li>不知还有什么可改进的点，咱们可以接<strong>上WaveNet后端</strong>，类似谢磊上篇用的；</li>
<li>然后<strong>重构损失</strong>上，模型结构学习一下网易这篇。</li>
<li><strong>embedding</strong> 上，为的是实现 <strong>one-shot</strong>，再考虑是否有其他方法；</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>VC</tag>
        <tag>论文阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>《Voice Conversion with transformer network-samsung》论文总结</title>
    <url>/2020/06/14/92-Voice%20Conversion%20with%20transformer%20network-samsung%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>![image-20200617224333150](./Voice Conversion with transformer network-samsung论文总结/image-20200617224333150.png)</p>
<hr>
<a id="more"></a>

<h1 id="特点："><a href="#特点：" class="headerlink" title="特点："></a>特点：</h1><ol>
<li>应用场景是： 一对一、平行数据</li>
<li>不需要文本辅助</li>
<li>适用的场景，类似于小爱同学，用于在已有的预训练的 <strong>TTS</strong> 语音合成系统，实现音色转换。</li>
<li>技术上，以 LSTM-RNN 作为 base line 。</li>
<li>实现上，动用了 <strong>Transformer Architecture &amp;&amp; Context Preservation and Model Adaptation in an Attentional Seq2seq VC.</strong></li>
<li>闪光点：训练速度快了 2.72 倍（每个 step） &amp;&amp; 流畅度、相似度 比 base line 好一点</li>
</ol>
<hr>
<h1 id="注意点："><a href="#注意点：" class="headerlink" title="注意点："></a>注意点：</h1><ol>
<li>提到了一篇 2017 年的 <strong>VC 综述文章</strong>，之前没见到过，再过一遍；</li>
<li>![image-20200617225134062](./Voice Conversion with transformer network-samsung论文总结/image-20200617225134062.png)</li>
</ol>
<hr>
<h1 id="模型："><a href="#模型：" class="headerlink" title="模型："></a>模型：</h1><p>![image-20200617231000814](./Voice Conversion with transformer network-samsung论文总结/image-20200617231000814.png)</p>
<hr>
<ol>
<li>介绍了一些 Attention 和 Transformer 相关背景信息，以及在语音场景的常见应用</li>
<li>本文 用 Transformer 来进行 基于 sp 特征的 句到句的 音色转换</li>
</ol>
<hr>
<h1 id="三个Loss"><a href="#三个Loss" class="headerlink" title="三个Loss"></a>三个Loss</h1><ol>
<li>类似Transformer 的Loss</li>
<li>额外的：在Transformer 上进行的 MultiHead 数目的调整（以此加快训练速度）</li>
<li>![image-20200621184615655](/Users/huangshengjie/Library/Application Support/typora-user-images/image-20200621184615655.png)</li>
<li>目标真实 &amp; 转换出来的目标</li>
<li>![image-20200621184636954](/Users/huangshengjie/Library/Application Support/typora-user-images/image-20200621184636954.png)</li>
<li>Attention 的损失（Guided attention）：</li>
<li>![image-20200621184700921](/Users/huangshengjie/Library/Application Support/typora-user-images/image-20200621184700921.png)</li>
<li>内容保存程度 损失： source 和 恢复预测的 source  &amp;&amp; target 和恢复预测的 target </li>
</ol>
<hr>
<h1 id="学到的："><a href="#学到的：" class="headerlink" title="学到的："></a>学到的：</h1><ul>
<li><strong>消融实验</strong>：更换单一变量：观察指标是 <strong>固定训练步数，以 正确转换的语句数目 作为衡量指标</strong></li>
</ul>
<hr>
<h1 id="另一篇"><a href="#另一篇" class="headerlink" title="另一篇"></a>另一篇</h1><p><strong>（未看完）</strong></p>
<p>![image-20200621182450382](/Users/huangshengjie/Library/Application Support/typora-user-images/image-20200621182450382.png)</p>
<ul>
<li>谈到 WaveNet 的自适应改进，对她不够熟悉</li>
<li></li>
<li>另外看招聘需求大都是要做<strong>合成</strong>的，<strong>转换</strong>没有需求；</li>
<li>所以 花点时间 跑了一下 Tacotron（源码后端是用griff-Lim），花时间 再弄懂一下代码</li>
<li>接下来再弄懂一下 <strong>r9y9</strong> 的 <strong>WaveNET</strong> 代码</li>
</ul>
<hr>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>test</category>
      </categories>
      <tags>
        <tag>VC</tag>
        <tag>论文阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>《EMOTIONAL VOICE CONVERSION USING MULTITASK LEARNING WITH TEXT-TO-SPEECH》</title>
    <url>/2020/09/20/95-0920-EMOTIONAL%20VOICE%20CONVERSION%20USING%20MULTITASK%20LEARNING%20WITH%20TEXT-TO-SPEECH/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>0920-论文总结</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200921003657.png" alt="image-20200916195211586"></p>
<p>2020.09.20</p>
<a id="more"></a>

<h2 id="本周完成的工作："><a href="#本周完成的工作：" class="headerlink" title="本周完成的工作："></a>本周完成的工作：</h2><p>（写了比较多的<strong>验证实验代码</strong>，助于理解原理）</p>
<ol>
<li><p>学习掌握了 <strong>LMDB</strong> 格式数据的处理（创建/插入/读取/修改）</p>
</li>
<li><p>实现了 <strong>Numpy 类型数据转 lmdb</strong>（<strong>librosa</strong>提取出来的<strong>mel</strong>数据需要处理成 <strong>连续存储</strong> <strong>np.ascontiguousarray（）</strong>）</p>
</li>
<li><p>从 lmdb 读取数据，并转换成 numpy（<strong>np.fromstring(value, dtype=np.float32)</strong>），并完整复原成语音。</p>
</li>
<li><p><strong>Mel —&gt;(griffin)—&gt;wav</strong>：对比试验了几个版本的 tacotron 的语音数据处理代码， 结合网上资料，总结一套转换效果质量较好的 代码：（mag -&gt; mel ; <strong>mel -&gt; mag</strong>; mag -&gt;wave）【很多资料版本在借用<strong>griffin</strong>实现 <strong>mel 转 幅度谱 mag</strong> 环节，写的不够好甚至没写清楚】</p>
<hr>
</li>
<li><p>数据处理环节：NVAE 中的 图像处理是【n，n】，所以采用 【<strong>256帧，n_mels=256</strong>】的参数来提取 <strong>mel</strong>（<strong>80 维的griffin复原效果太差</strong></p>
<hr>
</li>
<li><p>🌟<strong>NVAE图像**</strong>训爆了<strong>：和作者联系，问题定位在 batch太小（原32，咱们用 4【GPU限制】）情况下，</strong>learning_rate太大：1e-2 改 1e-3</p>
</li>
<li><p>在epoch <strong>5</strong> 掉链子：<strong>warm_up</strong>环节刚过，学习率有变，所以导致数据算成了 <strong>NAN</strong>；模型保存也只保存到 <strong>epoch 1</strong>，<strong>改部分代码，先 一个epoch一个epoch保存 ckpt</strong></p>
</li>
</ol>
<hr>
<h2 id="可改进："><a href="#可改进：" class="headerlink" title="可改进："></a>可改进：</h2><ol>
<li>晚上先试着跑起来</li>
<li>后面尝试改进 NVAE 代码成 <strong>自适应 数据尺寸【m，n】（m != n）</strong></li>
<li>在tensorboard上看下怎么展示中间步骤语音 .wav</li>
</ol>
<hr>
<hr>
<h2 id="论文涉猎"><a href="#论文涉猎" class="headerlink" title="论文涉猎"></a>论文涉猎</h2><h2 id="情感语音转换（TTS-VC-多任务学习）"><a href="#情感语音转换（TTS-VC-多任务学习）" class="headerlink" title="情感语音转换（TTS + VC 多任务学习）"></a><strong>情感语音转换</strong>（TTS + VC 多任务学习）</h2><ul>
<li><h3 id="VC领域-痛点：保存语言信息，情感信息-和-多对多VC方面，VC的性能仍然很差"><a href="#VC领域-痛点：保存语言信息，情感信息-和-多对多VC方面，VC的性能仍然很差" class="headerlink" title="VC领域 痛点：保存语言信息，情感信息 和 多对多VC方面，VC的性能仍然很差"></a>VC领域 <strong>痛点</strong>：<strong>保存语言信息</strong>，<strong>情感信息</strong> 和 <strong>多对多VC</strong>方面，VC的性能仍然很差</h3></li>
<li><h3 id="解决的问题：在-2017-年一篇-“情感VC转换”-基础上，提升“转换后内容保留程度”（即，降低WER）（retaining-linguistic-contents）"><a href="#解决的问题：在-2017-年一篇-“情感VC转换”-基础上，提升“转换后内容保留程度”（即，降低WER）（retaining-linguistic-contents）" class="headerlink" title="解决的问题：在 2017 年一篇 “情感VC转换” 基础上，提升“转换后内容保留程度”（即，降低WER）（retaining linguistic contents）"></a><strong>解决的问题</strong>：在 2017 年一篇 “情感VC转换” 基础上，<strong>提升“转换后内容保留程度”</strong>（即，降低WER）（retaining linguistic contents）</h3></li>
</ul>
<hr>
<ul>
<li>有<strong>提供源码</strong>，缺 demo 展示（文件夹下载需代理，网速极其慢， 300+m 大小 /  3kb/s）</li>
</ul>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200916195217.png" alt="image-20200916195211586"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200919174026.png" alt="image-20200919174025442"></p>
<hr>
<ul>
<li>最新的 VC 思路是 序列到序列（Sequence 2 Seq），但是容易丢失语音信息<ul>
<li>可以通过文本监督来矫正：<ul>
<li>但是对齐是个问题；</li>
<li>另外这样也失去了 S2S 的优势了</li>
</ul>
</li>
</ul>
</li>
<li>本文思路：<ul>
<li>利用 <strong>多任务学习的 TTS 模型</strong>，来帮助 VC 模型 <strong>捕获语言信息</strong>并<strong>保持训练稳定性</strong>。</li>
<li>TTS 框架来源 tacotron（有局部的稍微改动）：（Style Encoder 也是借鉴这篇）<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200921003719.png" alt="image-20200919190909558"></li>
<li>VC 框架：另外并联一个 “Content Encoder”</li>
</ul>
</li>
</ul>
<hr>
<h2 id="要点："><a href="#要点：" class="headerlink" title="要点："></a>要点：</h2><ul>
<li><strong>并不像</strong>传统做 <strong>情感转换</strong>那样，在训练阶段就提取 情感标签（one-hot形式）；</li>
<li>在整个网络中，也不会将 <strong>情感标签</strong> 当作一个条件作为输入 （<strong>联想一下之前的 pitch 标签</strong>）</li>
<li></li>
<li>可以在<strong>单个模型中</strong>执行<strong>VC</strong>和<strong>TTS</strong></li>
</ul>
<hr>
<h2 id="网络结构："><a href="#网络结构：" class="headerlink" title="网络结构："></a>网络结构：</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200919173504.png" alt="image-20200919173502738"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200919185336.png" alt="image-20200919185334992"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200919204207.png" alt="image-20200919204205843"></p>
<hr>
<h2 id="TTS-支线：（以-tacotron-为原型）"><a href="#TTS-支线：（以-tacotron-为原型）" class="headerlink" title="TTS 支线：（以 tacotron 为原型）"></a>TTS 支线：（以 tacotron 为原型）</h2><ol>
<li><strong>模仿的是</strong>：</li>
</ol>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200919190911.png" alt="image-20200919190909558"></p>
<ol start="2">
<li><strong>框架</strong>：</li>
</ol>
<ul>
<li>text encoder, </li>
<li>decoder,</li>
<li>attention, </li>
<li>and post processor</li>
</ul>
<ol start="3">
<li><strong>改动：(参考 [17] 文献)</strong><ol>
<li>文字向量<strong>context vector $C_t$</strong> 被用在 <strong>AttentionRNN</strong> 的每个循环内（context vector c (t)utilizes is used for every iteration in attention RNN）[原本是怎么样的？查一下]</li>
<li>在 <strong>CBHG</strong> (Convolution Bank + Highway + bi-GRU) 模块中，增加了 *<em>残余连接 (residual connection) *</em>模块</li>
</ol>
</li>
</ol>
<hr>
<hr>
<h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200919194706.png" alt="image-20200919194704841"></p>
<ul>
<li>Loss 就是直接比较： <strong>mel谱</strong> 差距 &amp;&amp; <strong>线性谱</strong> 差距</li>
</ul>
<hr>
<h2 id="实验参数："><a href="#实验参数：" class="headerlink" title="实验参数："></a>实验参数：</h2><ol>
<li>大体上都是和 taco 部分的语音预处理手法相似</li>
<li>数据集：<ol>
<li>韩国某个30岁男子，用七种情绪，每种情绪说 3k 句；共 2.1w 句；</li>
<li>其中情感：（neutral, happiness, sadness, anger, fear, surprise, and disgust)</li>
<li>去除静音之后，共约 29.2 h</li>
</ol>
</li>
<li>🌟值得一提的几点：<ol>
<li>去除静音，不是像taco那样，用 <strong>librosa.effect.trim()</strong> ，而是用voice activity detection algorithm （VAD 算法：开源）</li>
<li>在做 TTS-taco-like 部分里，<strong>字符处理</strong>有特点：【在转换为 <strong>one-hot embedding</strong> 这种表示形式之前会分解为 <strong>开始</strong>，<strong>核心</strong> 和 <strong>尾声</strong>（<strong>onset, nucleus, and coda</strong>）】</li>
<li>256 character embedding, 32 dimensions for $h_c$</li>
</ol>
</li>
</ol>
<hr>
<h2 id="🌟重要的一个验证实验"><a href="#🌟重要的一个验证实验" class="headerlink" title="🌟重要的一个验证实验"></a>🌟重要的一个验证实验</h2><p>——（<strong>内容一致性</strong>验证 <strong>Linguistic consistency</strong>）</p>
<ol>
<li>每种情感 取20条句子</li>
<li>用 StyleEncoder 提取 “<strong>style vector</strong>”，并用<strong>余弦相似度</strong>来查看<strong>情感分离程度</strong>（验证情感特征提取的有效性）</li>
</ol>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200920161149.png" alt="image-20200920161147834"></p>
<ol>
<li>每种情感 句子 的<strong>内容 $X_s$</strong> 保持不变，选取七种句子（同内容），做“从中性情感”到“其他六种情感”的转换</li>
<li>结果上看，<strong>log mel</strong> 语谱图 <strong>尺寸形状大差不差</strong></li>
<li>有些许差异的地方：<ol>
<li>时间偏移，</li>
<li>频率偏移，</li>
<li>暂停持续时间</li>
</ol>
</li>
<li>总体上能实现，<strong>由一种情感，随意 VC 转换到其他情感</strong> 的能力</li>
</ol>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200920161207.png" alt="image-20200920161206290"></p>
<hr>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200920145005.png" alt="image-20200920145003900"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200919205459.png" alt="image-20200919205457583"></p>
<ol>
<li>在联合训练的帮助下：<ol>
<li>VCTTS-VC 在内容保留能力上的效果，比单纯的 VC ，正确率要提高不少</li>
<li>另一方面，VCTTS-TTS 比单纯的 TTS 没有太大进步，甚至有一点点下降</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>test</category>
      </categories>
      <tags>
        <tag>VC</tag>
        <tag>论文阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>《PITCHNET —— UNSUPERVISED SINGING VOICE CONVERSION WITH PITCH ADVERSARIAL NETWORK》</title>
    <url>/2020/09/07/94-%E3%80%8APITCHNET%20%E2%80%94%E2%80%94%20UNSUPERVISED%20SINGING%20VOICE%20CONVERSION%20WITH%20PITCH%20ADVERSARIAL%20NETWORK%E3%80%8B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>0906组会分享</p>
<h2 id="0906组会分享：歌唱转换-singing-voice-conversion"><a href="#0906组会分享：歌唱转换-singing-voice-conversion" class="headerlink" title="0906组会分享：歌唱转换 singing voice conversion"></a>0906组会分享：<strong>歌唱转换 singing voice conversion</strong></h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200905170752.png" alt="image-20200905170750870"></p>
<ul>
<li><p>效果概览：<a href="https://tencent-ailab.github.io/pitch-net/" target="_blank" rel="noopener">https://tencent-ailab.github.io/pitch-net/</a></p>
</li>
<li><p>MOS评分：</p>
<ul>
<li>Baseline ：2.92</li>
<li>PitchNet：3.75</li>
</ul>
</li>
</ul>
<a id="more"></a>



<ul>
<li><h2 id="论文的-baseline-对象："><a href="#论文的-baseline-对象：" class="headerlink" title="论文的 baseline 对象："></a>论文的 <strong>baseline</strong> 对象：<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200905171530.png" alt="image-20200905171528462"></h2></li>
<li><h2 id="BaseLine-的贡献和缺点"><a href="#BaseLine-的贡献和缺点" class="headerlink" title="BaseLine 的贡献和缺点"></a><strong>BaseLine 的贡献和缺点</strong></h2></li>
</ul>
<p>贡献：</p>
<ol>
<li>利用 AE 模式下的无监督方式，解决了 平行语料的问题</li>
<li>组成模块：<strong>WaveNetlike Encoder</strong> &amp;&amp;  <strong>WaveNet  autoregressive Decoder</strong> &amp;&amp; <strong>Speaker-Embedding Table</strong>（leanable）</li>
</ol>
<p>缺点：</p>
<ol>
<li>和StarGan-VC之类的 单纯说话VC 任务不同，歌唱转换的效果重点是：说话人相似性 &amp;&amp; 音调合理性</li>
<li>BaseLine 在音调处理上差强人意，总而言之，“<strong>语音和音调的联合表示</strong>”是难点</li>
</ol>
<hr>
<ul>
<li><h2 id="解决的问题："><a href="#解决的问题：" class="headerlink" title="解决的问题："></a><strong>解决的问题</strong>：</h2></li>
</ul>
<ol>
<li>走调（音高 Pitch 失控现象）问题的缓解<ol>
<li>在 Baseline 的 <strong>AE无监督</strong> 模式下，附带一个额外的<strong>音调回归网络（GAN）</strong>可以将音调信息从潜在空间中分离出来</li>
<li><strong>目的是</strong>：让原本的<strong>Encoder</strong>在单纯学习 <strong>说话人无关的语言信息</strong> 基础上，再额外剥离掉 <strong>音高信息</strong> （<strong>not only singer-invariant but also pitch-invariant representation</strong>）</li>
</ol>
</li>
<li>至于音高信息，当成一个独立问题来解决：设立独立模块来提取 <strong>source</strong> 的<strong>音高信息</strong>，以此配合 <strong>Decoder</strong> ，来<strong>操控</strong> <strong>生成语音</strong> 的<strong>音高</strong> <ol>
<li>（根据作者相关 Git 项目下的 issue 讨论，这部分工作借用 Kaldi 来完成）</li>
<li>【实现 <strong>音高可控性</strong> 】</li>
</ol>
</li>
</ol>
<hr>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a><strong>模型</strong></h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200905223846.png" alt="image-20200905223845590"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200905223859.png" alt="image-20200905223857883"></p>
<ul>
<li><strong>Singer Classiﬁcation Network</strong> <strong>Pitch Regression Network</strong> 是为了使得 <strong>Encoder</strong> 能够在抽取特征向量 <strong>z</strong> 时，把 说话人信息 &amp;&amp; 音调信息 都剥离掉</li>
<li></li>
<li></li>
</ul>
<hr>
<h2 id="Training-Loss：三个Loss"><a href="#Training-Loss：三个Loss" class="headerlink" title="Training Loss：三个Loss"></a><strong>Training Loss：三个Loss</strong></h2><ol>
<li><strong>singer classiﬁcation loss</strong></li>
<li><strong>pitch regression loss</strong></li>
<li><strong>reconstruction loss</strong></li>
</ol>
<h2 id="几个公式的清晰理解："><a href="#几个公式的清晰理解：" class="headerlink" title="几个公式的清晰理解："></a>几个公式的清晰理解：</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200906141104.png" alt="image-20200906141103220"></p>
<ol>
<li>说明了AE网络的Decoder产生结果 所需要的输入元素（Encoder结果+Kaldi抽取的音高+Embedding信息）</li>
<li>介绍重构损失：在Decoder时配合<strong>source</strong>的embedding，和<strong>source</strong>语音做<strong>Loss</strong></li>
</ol>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200906141139.png" alt="image-20200906141138234"></p>
<ol start="3">
<li>一个 <strong>说话人分类器</strong>，同理为 StarGan-VC 的Domain-Classification，分类对象，是<strong>Encoder</strong>结果&amp;&amp;<strong>Embedding</strong></li>
<li>另一个<strong>音高分类器</strong>，Pitch信息由 Kaldi 提取</li>
<li>总的<strong>Loss</strong>结构：因为重构损失⬇️越好，另一方面，对于两个分类器，我们希望我们的 <strong>Encoder</strong>提取出来的隐变量 能彻底剥离 说话人信息 &amp;&amp; Pitch信息，所以这两个的误差，对于结果来说，应该越大越好。（所以这两个部分是配合 负号）</li>
<li>上述两个 C分类器的损失，对于<strong>Encoder</strong>和<strong>Classifier</strong>来说，是完全相反的，【<strong>就是典型的GAN对抗思路</strong>】，所以他们的训练，采取“你一次我一次的过程”<ul>
<li>训练中，缩小 $$L_{total}$$ 时，促进的是 <strong>Encoder</strong> 将两个元素剥离</li>
<li>缩小 $L_{ad}$ 时，促进的是 <strong>Classifier</strong> 具有更强的分类能力，能看透隐变量 <strong>z</strong> 的实质和归属</li>
</ul>
</li>
</ol>
<p>Ps.整体思路比较清晰，就是<strong>自回归</strong>那个部分不知道是怎么具体实现的</p>
<hr>
<p>Quantitative and Qualitative Experiments**【定量 和 定性 实验】</p>
<hr>
<p>Tencent :LPCNet, </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python convert_tfrecord_to_lmdb.py --dataset&#x3D;celeba --tfr_path&#x3D;&#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;DATA_DIR&#x2F;celeba&#x2F;celeba-tfr --lmdb_path&#x3D;&#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;DATA_DIR&#x2F;celeba&#x2F;celeba-lmdb --split&#x3D;train</span><br><span class="line">python convert_tfrecord_to_lmdb.py --dataset&#x3D;celeba --tfr_path&#x3D;&#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;DATA_DIR&#x2F;celeba&#x2F;celeba-tfr --lmdb_path&#x3D;&#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;DATA_DIR&#x2F;celeba&#x2F;celeba-lmdb --split&#x3D;validation</span><br></pre></td></tr></table></figure>





<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export EXPR_ID&#x3D;&#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;EXPR_ID</span><br><span class="line">export DATA_DIR&#x3D;&#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;DATA_DIR&#x2F;celeba&#x2F;celeba-lmdb</span><br><span class="line">export CHECKPOINT_DIR&#x3D;&#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;CHECKPOINT_DIR</span><br><span class="line">export CODE_DIR&#x3D;&#x2F;data&#x2F;hsj&#x2F;NVAE</span><br><span class="line">cd &#x2F;data&#x2F;hsj&#x2F;NVAE</span><br><span class="line">CUDA_VISIBLE_DEVICES&#x3D;1  </span><br><span class="line">python train.py --data &#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;DATA_DIR&#x2F;celeba&#x2F;celeba-lmdb --root &#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;CHECKPOINT_DIR --save &#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;EXPR_ID --dataset celeba_64 \</span><br><span class="line">        --num_channels_enc 64 --num_channels_dec 64 --epochs 90 --num_postprocess_cells 2 --num_preprocess_cells 2 \</span><br><span class="line">        --num_latent_scales 3 --num_latent_per_group 20 --num_cell_per_cond_enc 2 --num_cell_per_cond_dec 2 \</span><br><span class="line">        --num_preprocess_blocks 1 --num_postprocess_blocks 1 --weight_decay_norm 1e-1 --num_groups_per_scale 20 \</span><br><span class="line">        --batch_size 16 --num_nf 1 --ada_groups --num_process_per_node 8 --use_se --res_dist --fast_adamax</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CelebA64 数据预处理</span><br><span class="line"></span><br><span class="line">cd $CODE_DIR&#x2F;scripts</span><br><span class="line">python create_celeba64_lmdb.py --split train --img_path .&#x2F;DATA_DIR&#x2F;celeba_org&#x2F;celeba --lmdb_path  .&#x2F;DATA_DIR&#x2F;celeba64_lmdb</span><br><span class="line">python create_celeba64_lmdb.py --split valid --img_path .&#x2F;DATA_DIR&#x2F;celeba_org&#x2F;celeba --lmdb_path  .&#x2F;DATA_DIR&#x2F;celeba64_lmdb</span><br><span class="line">python create_celeba64_lmdb.py --split test  --img_path .&#x2F;DATA_DIR&#x2F;celeba_org&#x2F;celeba --lmdb_path  .&#x2F;DATA_DIR&#x2F;celeba64_lmdb</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python create_celeba64_lmdb.py --split train --img_path ..&#x2F;DATA_DIR&#x2F;celeba_org&#x2F;celeba --lmdb_path ..&#x2F;DATA_DIR&#x2F;celeba64_lmdb</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python convert_tfrecord_to_lmdb.py --dataset&#x3D;celeba --split&#x3D;validation</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python train.py --data $DATA_DIR&#x2F;celeba64_lmdb --root $CHECKPOINT_DIR --save $EXPR_ID --dataset celeba_64 \</span><br><span class="line">        --num_channels_enc 64 --num_channels_dec 64 --epochs 90 --num_postprocess_cells 2 --num_preprocess_cells 2 \</span><br><span class="line">        --num_latent_scales 3 --num_latent_per_group 20 --num_cell_per_cond_enc 2 --num_cell_per_cond_dec 2 \</span><br><span class="line">        --num_preprocess_blocks 1 --num_postprocess_blocks 1 --weight_decay_norm 1e-1 --num_groups_per_scale 20 \</span><br><span class="line">        --batch_size 16 --num_nf 1 --ada_groups --num_process_per_node 8 --use_se --res_dist --fast_adamax</span><br></pre></td></tr></table></figure>

<p>/Users/huangshengjie/Desktop/NVAE/scripts/data1/datasets/imagenet-oord</p>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>VC</tag>
        <tag>论文阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>0830组会分享——几种卷积辨析 &amp;&amp; SELayer论文及代码实现</title>
    <url>/2020/08/31/97-0830%E7%BB%84%E4%BC%9A%E5%88%86%E4%BA%AB%E2%80%94%E2%80%94%E5%87%A0%E7%A7%8D%E5%8D%B7%E7%A7%AF%E8%BE%A8%E6%9E%90%20&amp;&amp;%20SELayer%E8%AE%BA%E6%96%87%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="几种卷积类型辨析"><a href="#几种卷积类型辨析" class="headerlink" title="几种卷积类型辨析"></a>几种卷积类型辨析</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv = nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">1</span>, groups=<span class="number">3</span>)</span><br><span class="line">conv.weight.data.size()</span><br><span class="line"></span><br><span class="line"><span class="comment"># output = torch.Size([6, 2, 1, 1])</span></span><br></pre></td></tr></table></figure>

<p>一种分类方法：</p>
<p>几种卷积示意：（分组卷积 <strong>group_convolution</strong>；深度卷积 <strong>depthwise convolution</strong>； 全局深度卷积 <strong>global depthwise convolution</strong>）</p>
<ol>
<li>groups 默认值为1， 对应的是<strong>常规卷积</strong>操作</li>
<li>groups &gt; 1， 且能够同时被in_channel / out_channel整除，对应<strong>group_convolution</strong></li>
<li>groups == input_channel == out_channel , 对应<strong>depthwise convolution</strong>,为条件2的特殊情况</li>
<li>在条件3的基础上，各卷积核的 H == input_height; W == input_width, 对应为 <strong>global depthwise convolution</strong>, 为条件3的特殊情况</li>
<li></li>
</ol>
<hr>
<p>另一种分类方法：<strong>主要分三类：正常卷积、分组卷积、深度分离卷积</strong></p>
<a id="more"></a>

<!--more-->





<ol>
<li><h2 id="正常卷积："><a href="#正常卷积：" class="headerlink" title="正常卷积："></a><strong>正常卷积：</strong></h2></li>
<li><p>参数量 = cin *  $K_h$ * $K_w$ * cout</p>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830111253.png" alt="常规卷积示意图"></p>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830103246.jpeg" alt="img"></p>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830103436.png" alt="img"></p>
</li>
<li></li>
<li><h2 id="分组卷积示意图："><a href="#分组卷积示意图：" class="headerlink" title="分组卷积示意图："></a><strong>分组卷积示意图：</strong></h2></li>
<li><p>参数量： (cin * $K_h$ * $K_w$* cout ) / Groups</p>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830111236.png" alt="分组卷积示意图"></p>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830103353.jpeg" alt="img"></p>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830103314.png" alt="img"></p>
</li>
<li><pre><code class="python"><span class="class"><span class="keyword">class</span> <span class="title">GroupConv</span><span class="params">(nn.Module)</span>:</span>
  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_ch, out_ch, groups)</span>:</span>
      super(GroupConv, self).__init__()
      self.conv = nn.Conv2d(
          in_channels=in_ch,
          out_channels=out_ch,
          kernel_size=<span class="number">3</span>,
          stride=<span class="number">1</span>,
          padding=<span class="number">1</span>,
          groups=groups
      )

  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span>
      out = self.conv(input)
      <span class="keyword">return</span> out

</code></pre>
</li>
</ol>
<pre><code># 测试
conv = CSDN_Tem(16, 32, 4)
print(summary(conv, (16, 64, 64), batch_size=1))

********************************
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [1, 32, 64, 64]           1,184
================================================================
Total params: 1,184
Trainable params: 1,184
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.25
Forward/backward pass size (MB): 1.00
Params size (MB): 0.00
Estimated Total Size (MB): 1.25
----------------------------------------------------------------

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">13. </span><br><span class="line"></span><br><span class="line">14. </span><br><span class="line"></span><br><span class="line">15. ## [深度可分离卷积(Depthwise Separable Convolution)][https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_30793735&#x2F;article&#x2F;details&#x2F;88915612]</span><br><span class="line"></span><br><span class="line">16. 参数量： $C_&#123;in&#125;$ * $K_h$ * $K_w$ + $C_&#123;out&#125;$ * 1 * 1</span><br><span class="line"></span><br><span class="line">17. 理解上，可以看作是， 先做一次 cin &#x3D;&#x3D; cout 的 分组卷积， 并且 groups &#x3D;&#x3D; channels，即每个通道作为一组； 然后再对上述结果，做一次 **逐点卷积(Pointwise Convolution)** 实现通道数改变的，这个过程使用大小为 **Cin * 1 * 1 ** 的卷积核实现，数量为 **Cout** 个</span><br><span class="line"></span><br><span class="line">18. &#96;&#96;&#96;python</span><br><span class="line">    class DepthSepConv(nn.Module):</span><br><span class="line">        def __init__(self, in_ch, out_ch):</span><br><span class="line">            super(DepthSepConv, self).__init__()</span><br><span class="line">            self.depth_conv &#x3D; nn.Conv2d(</span><br><span class="line">                in_channels&#x3D;in_ch,</span><br><span class="line">                out_channels&#x3D;in_ch,</span><br><span class="line">                kernel_size&#x3D;3,</span><br><span class="line">                stride&#x3D;1,</span><br><span class="line">                padding&#x3D;1,</span><br><span class="line">                groups&#x3D;in_ch</span><br><span class="line">            )</span><br><span class="line">            self.point_conv &#x3D; nn.Conv2d(</span><br><span class="line">                in_channels&#x3D;in_ch,</span><br><span class="line">                out_channels&#x3D;out_ch,</span><br><span class="line">                kernel_size&#x3D;1,</span><br><span class="line">                stride&#x3D;1,</span><br><span class="line">                padding&#x3D;0,</span><br><span class="line">                groups&#x3D;1</span><br><span class="line">            )</span><br><span class="line">    </span><br><span class="line">        def forward(self, input):</span><br><span class="line">            out &#x3D; self.depth_conv(input)</span><br><span class="line">            out &#x3D; self.point_conv(out)</span><br><span class="line">            return out</span><br><span class="line">    </span><br><span class="line">          </span><br><span class="line">    # 测试</span><br><span class="line">    conv &#x3D; DepthSepConv(16, 32)</span><br><span class="line">    print(summary(conv, (16, 64, 64), batch_size&#x3D;1))</span><br><span class="line">    </span><br><span class="line">    ************************</span><br><span class="line">    ----------------------------------------------------------------</span><br><span class="line">            Layer (type)               Output Shape         Param #</span><br><span class="line">    &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">                Conv2d-1            [1, 16, 64, 64]             160</span><br><span class="line">                Conv2d-2            [1, 32, 64, 64]             544</span><br><span class="line">    &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">    Total params: 704</span><br><span class="line">    Trainable params: 704</span><br><span class="line">    Non-trainable params: 0</span><br><span class="line">    ----------------------------------------------------------------</span><br></pre></td></tr></table></figure></code></pre><ol start="19">
<li></li>
</ol>
<hr>
<h1 id="SELayer"><a href="#SELayer" class="headerlink" title="SELayer"></a>SELayer</h1><p><strong>Squeeze-and-Excitation Networks</strong></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830153344.png" alt="image-20200830114911949"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830153339.png" alt="image-20200830140039015"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.Size([<span class="number">2</span>, <span class="number">512</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">b = <span class="number">2</span> c = <span class="number">512</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">512</span>])</span><br><span class="line">After LinearFC, tmp.shape = torch.Size([<span class="number">2</span>, <span class="number">32</span>])</span><br><span class="line">After RELU, tmp.shape = torch.Size([<span class="number">2</span>, <span class="number">32</span>])</span><br><span class="line">After LinearFC2, tmp.shape = torch.Size([<span class="number">2</span>, <span class="number">512</span>])</span><br><span class="line">After SIGMOID, tmp.shape = torch.Size([<span class="number">2</span>, <span class="number">512</span>])</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">512</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">        Layer (type)               Output Shape         Param <span class="comment">#</span></span><br><span class="line">================================================================</span><br><span class="line"> AdaptiveAvgPool2d<span class="number">-1</span>            [<span class="number">-1</span>, <span class="number">512</span>, <span class="number">1</span>, <span class="number">1</span>]               <span class="number">0</span></span><br><span class="line">            Linear<span class="number">-2</span>                   [<span class="number">-1</span>, <span class="number">32</span>]          <span class="number">16</span>,<span class="number">384</span></span><br><span class="line">              ReLU<span class="number">-3</span>                   [<span class="number">-1</span>, <span class="number">32</span>]               <span class="number">0</span></span><br><span class="line">            Linear<span class="number">-4</span>                  [<span class="number">-1</span>, <span class="number">512</span>]          <span class="number">16</span>,<span class="number">384</span></span><br><span class="line">           Sigmoid<span class="number">-5</span>                  [<span class="number">-1</span>, <span class="number">512</span>]               <span class="number">0</span></span><br><span class="line">            Linear<span class="number">-6</span>                   [<span class="number">-1</span>, <span class="number">32</span>]          <span class="number">16</span>,<span class="number">384</span></span><br><span class="line">              ReLU<span class="number">-7</span>                   [<span class="number">-1</span>, <span class="number">32</span>]               <span class="number">0</span></span><br><span class="line">            Linear<span class="number">-8</span>                  [<span class="number">-1</span>, <span class="number">512</span>]          <span class="number">16</span>,<span class="number">384</span></span><br><span class="line">           Sigmoid<span class="number">-9</span>                  [<span class="number">-1</span>, <span class="number">512</span>]               <span class="number">0</span></span><br><span class="line">================================================================</span><br><span class="line">Total params: <span class="number">65</span>,<span class="number">536</span></span><br><span class="line">Trainable params: <span class="number">65</span>,<span class="number">536</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">Input size (MB): <span class="number">8.00</span></span><br><span class="line">Forward/backward <span class="keyword">pass</span> size (MB): <span class="number">0.02</span></span><br><span class="line">Params size (MB): <span class="number">0.25</span></span><br><span class="line">Estimated Total Size (MB): <span class="number">8.27</span></span><br><span class="line">----------------------------------------------------------------</span><br></pre></td></tr></table></figure>



<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830153225.png" alt="image-20200830153224613"></p>
]]></content>
      <categories>
        <category>组会</category>
      </categories>
      <tags>
        <tag>组会</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PGAN 训练笔记</title>
    <url>/2020/10/08/PGAN%20%E8%AE%AD%E7%BB%83%E7%AC%94%E8%AE%B0-99/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>PGAN 训练笔记</p>
<a id="more"></a>

<hr>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201208221357.png" alt="image-20201208221355969"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;path&#x2F;to&#x2F;database  &#x3D;  &#x2F;big_data&#x2F;hsj&#x2F;ParallelWaveGAN&#x2F;egs&#x2F;aishell3&#x2F;wav</span><br></pre></td></tr></table></figure>

<!--more--><!--more-->

<hr>
<ul>
<li>目前使用的是，所有 AiShell3数据集，但是用的是 single_speaker 的代码</li>
<li>数据的处理用到了 sklearn.StandardScaler 的 「均值、方差」归一化，（对训练集），所以咱们StarGAN-vc2中也暂时不自己写 数据预处理的环节了，用他处理好的 .npy 来读取，然后再做 【截取固定帧数】的操作</li>
</ul>
<hr>
<h2 id="StarGAN-VC2-一些参数："><a href="#StarGAN-VC2-一些参数：" class="headerlink" title="StarGAN-VC2 一些参数："></a>StarGAN-VC2 一些参数：</h2><ul>
<li><strong>batch_size</strong>=8</li>
<li><strong>learning_rate_G</strong> = 0.0002 ｜ <strong>learning_rate_D</strong> = 0.0001</li>
<li>每条句子切成 ：<strong>128 帧</strong> </li>
</ul>
<ul>
<li><strong>22050Hz</strong></li>
<li><strong>34 维度 sp</strong></li>
<li><strong>帧移</strong>： 5ms</li>
<li></li>
</ul>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201209191607.png" alt="image-20201209191606378"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201209191843.png" alt="image-20201209191842901"></p>
<hr>
<h2 id="VC-2-结构："><a href="#VC-2-结构：" class="headerlink" title="VC 2 结构："></a>VC 2 结构：</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201209193704.png" alt="image-20201209193703036"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201209193747.png" alt="image-20201209193746702"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201209193731.png" alt="image-20201209193730230"></p>
<hr>
<h2 id="VC-1-结构"><a href="#VC-1-结构" class="headerlink" title="VC 1 结构"></a>VC 1 结构</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201209193905.png" alt="image-20201209193903655"></p>
<hr>
<h2 id="Torch官方算卷积的公式"><a href="#Torch官方算卷积的公式" class="headerlink" title="Torch官方算卷积的公式"></a>Torch官方算卷积的公式</h2><p><a href="https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html#torch.nn.Conv2d" target="_blank" rel="noopener">https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html#torch.nn.Conv2d</a></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201209202938.png" alt="image-20201209202936687"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201209202948.png" alt="image-20201209202947175"></p>
<hr>
<p><a href="https://blog.csdn.net/qiu931110/article/details/104292129![image-20201210134725352](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201210134726.png)" target="_blank" rel="noopener">https://blog.csdn.net/qiu931110/article/details/104292129![image-20201210134725352](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201210134726.png)</a></p>
<p><a href="https://www.jianshu.com/p/d8b77cc02410" target="_blank" rel="noopener">https://www.jianshu.com/p/d8b77cc02410</a></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201210134801.png" alt="image-20201210134800412"></p>
<hr>
<h2 id="Tensorflow-版本的-CIN"><a href="#Tensorflow-版本的-CIN" class="headerlink" title="Tensorflow 版本的 CIN"></a>Tensorflow 版本的 <a href="https://github.com/MingtaoGuo/Conditional-Instance-Norm-for-n-Style-Transfer/blob/master/ops.py#L5" target="_blank" rel="noopener">CIN</a></h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201210152752.png" alt="image-20201210152751386"></p>
<p>参考这两份：</p>
<ul>
<li><strong>Torch</strong>：<a href="https://github.com/ChinmayLad/neural-style-transfer/blob/dc44cd261fd9bdf684440ef104cec65fe5be15c5/normalization.py" target="_blank" rel="noopener">https://github.com/ChinmayLad/neural-style-transfer/blob/dc44cd261fd9bdf684440ef104cec65fe5be15c5/normalization.py</a></li>
<li><strong>TensorFlow</strong>:<a href="https://github.com/MingtaoGuo/Conditional-Instance-Norm-for-n-Style-Transfer/blob/master/ops.py#L5" target="_blank" rel="noopener">https://github.com/MingtaoGuo/Conditional-Instance-Norm-for-n-Style-Transfer/blob/master/ops.py#L5</a></li>
<li><a href="https://github.com/MingtaoGuo/Conditional-Instance-Norm-for-n-Style-Transfer/issues/1" target="_blank" rel="noopener">对应回答</a> issue</li>
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201210163236.png" alt="image-20201210163234409"></li>
<li></li>
</ul>
<p>1211:</p>
<h1 id="CIN-个人理解："><a href="#CIN-个人理解：" class="headerlink" title="CIN 个人理解："></a>CIN 个人理解：</h1><p><strong>输入：</strong>one-hot = 【1，0，0，0，】 ；特征 x =【8，512，32】=【batch， channel=每个单词用多少特征来表示俄mbedding， 句子长度=含有32个单词】</p>
<p><strong>过程：</strong></p>
<ol>
<li>先是构造一张特征表，尺寸为【说话人数目 / 特征种类， 每个说话人 用多少维度的embedding 来表示 / 每个单词的特征通道数「一维角度来看」】 = 【4， 512】</li>
<li>然后，用对应的一个说话人的 one-hot 来取这张表上的特征值 【1，num_styles=4】x【num_styles=4，num_in=特征数=512】</li>
<li>在这个过程中，我们用Torch实现时，可以用 $nn.linear()$ 函数（看作全连接）来表示矩阵乘法：nn.linears（in_channel = 4, out_channel = 512）——<strong>nn.linear（）中的参数是可训练的！会加入到整张图的参数列表中去</strong></li>
<li>然后把 one-hot 特征送进全连接网络，得到对应的一个特征【512维】，$\gamma$ 和 $\beta$ 都是这么处理的</li>
<li>最后 ， 两个特征参数，和前面送进来的并经过 <strong>均值 &amp; 归一化 处理过的 x</strong> ，相互配合起来，得到当前层的 <strong>CIN</strong> 结果</li>
</ol>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201211171011.png" alt="image-20201211171010810"></p>
<hr>
<h2 id="关于PixelShuffle"><a href="#关于PixelShuffle" class="headerlink" title="关于PixelShuffle"></a>关于PixelShuffle</h2><ol>
<li><a href="https://blog.csdn.net/g11d111/article/details/82855946" target="_blank" rel="noopener">https://blog.csdn.net/g11d111/article/details/82855946</a></li>
<li><a href="https://blog.csdn.net/u014636245/article/details/98071626" target="_blank" rel="noopener">https://blog.csdn.net/u014636245/article/details/98071626</a></li>
<li><a href="https://pytorch.org/docs/0.3.1/_modules/torch/nn/modules/pixelshuffle.html" target="_blank" rel="noopener">https://pytorch.org/docs/0.3.1/_modules/torch/nn/modules/pixelshuffle.html</a></li>
</ol>
<hr>
<h2 id="关于-GSP：global-sum-pooling"><a href="#关于-GSP：global-sum-pooling" class="headerlink" title="关于 GSP：global sum pooling"></a>关于 GSP：global sum pooling</h2><hr>
<h2 id="关于-inner-product-向量内积-https-zhuanlan-zhihu-com-p-212461087"><a href="#关于-inner-product-向量内积-https-zhuanlan-zhihu-com-p-212461087" class="headerlink" title="关于 inner product 向量内积 https://zhuanlan.zhihu.com/p/212461087"></a>关于 inner product 向量内积 <a href="https://zhuanlan.zhihu.com/p/212461087" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/212461087</a></h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201211013638.png" alt="image-20201211013636895"></p>
<hr>
<h1 id="整个模型-尺寸信息："><a href="#整个模型-尺寸信息：" class="headerlink" title="整个模型 尺寸信息："></a>整个模型 尺寸信息：</h1><ol>
<li>原本 <strong>Generator</strong> 尺寸：<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201211172645.png" alt="image-20201211172644404"></li>
</ol>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>VC</tag>
        <tag>论文阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>《EFFECTIVE WAVENET ADAPTATION FOR VOICE CONVERSION WITH LIMITED DATA》</title>
    <url>/2020/06/27/93-%E3%80%8AEFFECTIVE%20WAVENET%20ADAPTATION%20FOR%20VOICE%20CONVERSION%20WITH%20LIMITED%20DATA%E3%80%8B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627153546.png" alt="image-20200627153541122"></p>
<a id="more"></a>

<h1 id="1-模型"><a href="#1-模型" class="headerlink" title="1.模型"></a>1.模型</h1><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627153718.png" alt="image-20200627153716639"></p>
<hr>
<h1 id="2-模型特点-amp-amp-训练需求"><a href="#2-模型特点-amp-amp-训练需求" class="headerlink" title="2.模型特点 &amp;&amp; 训练需求"></a>2.模型特点 &amp;&amp; 训练需求</h1><ul>
<li>大数据集 + 少数据集（target）、many-to-many（没强调）</li>
<li>不采用“独立模型”的思路（e.g.不根据性别来分组训练），而是先用 多说话人的大量数据集，训练 <strong>Speaker Independent (SI) WaveNet model</strong></li>
<li>再用 少量<strong>Target Speaker</strong>数据进行微调；</li>
<li>（和上一篇 三星论文 思路有点像，但三星侧重转换模型（引入MultiHead Attention），他的 <strong>WaveNet</strong> 就用现成的；</li>
<li>本文则 侧重后端声码器 <strong>WaveNet</strong> 的优化：<strong>速度</strong> 和 <strong>质量</strong>）</li>
</ul>
<hr>
<h1 id="3-改进-WaveNet-的思路"><a href="#3-改进-WaveNet-的思路" class="headerlink" title="3.改进 WaveNet 的思路"></a>3.改进 WaveNet 的思路</h1><ul>
<li><strong>phonetic posteriorgram (PPG)</strong> （<strong>音素后验概率</strong>）和 语音波形（时域信号） 直接映射（本来呢？）</li>
<li><strong>🌟singular value decomposition (SVD)</strong>（<strong>奇异值分解</strong>）：减少 WaveNet 的训练参数量（<strong>重点</strong>）</li>
<li><strong>between PPGs and the corresponding time-domain speech signals of the same speaker.</strong>：模型的预训练，是在同一个说话人的 <strong>PPG</strong> 特征 和对应的 <strong>时域信号</strong> 之间进行训练；</li>
<li></li>
</ul>
<hr>
<h1 id="4-关于PPG-amp-amp-SVD"><a href="#4-关于PPG-amp-amp-SVD" class="headerlink" title="4.关于PPG &amp;&amp; SVD"></a>4.关于PPG &amp;&amp; SVD</h1><ul>
<li><del>PPG 有一个其他人自己写的 python 包，但是没有正规的开源工具包，很多论文都直接说用到了这个特征，却从没交代怎么提取，从哪来的。【<strong>请教老师</strong>】</del></li>
<li><strong>在Deep VC项目里面的 Train1.py 部分，出来的就是语音的 PPG</strong>，（它是想预先训一个ASR模型）</li>
<li>或者用Kaldi来求；</li>
<li>本质都是，训练一个 <strong>phonetic recognition system.</strong>，然后用这个识别网络去识别（过程和识别出MFCC特征很像）；<strong>怎么 VC 领域又给牵扯到 ASR 领域去了，四不像</strong></li>
</ul>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627162338.png" alt="image-20200627162336086"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627175055.png" alt="image-20200627174816852"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627175051.png" alt="image-20200627175049966"></p>
<hr>
<h1 id="5-其他"><a href="#5-其他" class="headerlink" title="5.其他"></a>5.其他</h1><h2 id="这类模型：-原本的转换流程："><a href="#这类模型：-原本的转换流程：" class="headerlink" title="* 这类模型： 原本的转换流程："></a>* 这类模型： 原本的转换流程：</h2><h3 id="一句话——-gt-PPG特征不是直接合成出语音，而要经过转换；"><a href="#一句话——-gt-PPG特征不是直接合成出语音，而要经过转换；" class="headerlink" title="* 一句话——&gt;PPG特征不是直接合成出语音，而要经过转换；"></a>* 一句话——&gt;PPG特征不是直接合成出语音，而要经过转换；</h3><ol>
<li>从 .wav 中提取 <strong>source</strong> 的 PPG 特征（自注：需要额外训练一个声学模型，用来提取PPG）</li>
<li>用 “提前用大量 多说话人数据集 训练的” <strong>SI</strong> <strong>Conversion</strong> <strong>Model</strong>， 将 PPG 特征转化成 声学特征（<strong>mel ？</strong>）</li>
<li>再将 前一步骤的声学特征，扔进 <strong>经过（用 Target 语音）适应性调整的 WavaNet 声码器</strong>，以此合成最终转换语音；</li>
</ol>
<hr>
<h2 id="本文改进的转换流程："><a href="#本文改进的转换流程：" class="headerlink" title="*本文改进的转换流程："></a>*本文改进的转换流程：</h2><h3 id="特征转换模型-和-语音生成模型-是分开训练的；"><a href="#特征转换模型-和-语音生成模型-是分开训练的；" class="headerlink" title="* 特征转换模型 和 语音生成模型 是分开训练的；"></a>* 特征转换模型 和 语音生成模型 是分开训练的；</h3><h3 id="但是训练完之后，在转换步骤里，它利用（PPG）作为-本地条件-直接生成🌟时域语音信号"><a href="#但是训练完之后，在转换步骤里，它利用（PPG）作为-本地条件-直接生成🌟时域语音信号" class="headerlink" title="* 但是训练完之后，在转换步骤里，它利用（PPG）作为 本地条件 直接生成🌟时域语音信号"></a>* 但是训练完之后，在转换步骤里，它利用（PPG）作为 本地条件 <strong>直接生成🌟时域语音信号</strong></h3><p>即：输入 source 语音（提取特征后）给WaveNet 模型，然后WaveNet<strong>直接转换出来</strong> Target 语音；</p>
<ol>
<li><strong>SI</strong> WaveNet Conversion Model 的训练（用多说话人大数据量训练）(其实就是让WaveNet学会根据给定特征，<strong>重建出语音波形</strong>，模型的输入就是大量独立的语音，从而实现：让模型学会 <strong>与说话人无关的波形重建能力</strong>)</li>
<li>上述模型的适应性调整 <strong>adaption</strong>（基于target语料）</li>
<li><strong>run-time conversion</strong></li>
</ol>
<p>具体的，结合图片：</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627194348.png" alt="image-20200627194342790"></p>
<h2 id="在第一步中："><a href="#在第一步中：" class="headerlink" title="在第一步中："></a>在第一步中：</h2><p>五个特征【<strong>PPG、Energy、F0、V/UV、BAP</strong>】（BAP 待查）</p>
<ol>
<li><p>为了训练说话人无关的 SI WaveNet Conversion Model，先从多说话人的数据集中，读取 <strong>PPG</strong> 特征（另外训练的声学特征提取模型），用来表征 <strong>说话内容</strong></p>
</li>
<li><p><strong>Energy</strong> 用的是 <strong>梅尔倒谱</strong> 的<strong>第一维度</strong>用来表征 能量轮廓（这和我们说的 mel-cepstral 用来代表<strong>频谱图的轮廓信息</strong> 相联系）</p>
<p><img src="/2020/06/27/93-%E3%80%8AEFFECTIVE%20WAVENET%20ADAPTATION%20FOR%20VOICE%20CONVERSION%20WITH%20LIMITED%20DATA%E3%80%8B/huangshengjie/Documents/2020/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99hexo%E6%B5%8B%E8%AF%95/Typora%E5%8D%9A%E5%AE%A2%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9/20200627195116.png" alt="image-20200627195114177"></p>
</li>
<li><p><strong>F0</strong> 取 log 对树</p>
</li>
<li><p><strong>V/UV</strong> 用来表示 发声/不发声 的一个标志（实现的话，我想可以用 f0 来判断当前帧 有没有人声；只有发声了，f0 才大于 0 ）</p>
</li>
<li><p><strong>BAPs</strong> ：还没查，指向一篇 06 年日本的文章，说法是，这个特征对语音波形的 <strong>重建</strong> 很有帮助</p>
</li>
</ol>
<p>这五个特征，concate 到一起，输入 SI WaveNet Conv Model 训练；</p>
<hr>
<h2 id="步骤二"><a href="#步骤二" class="headerlink" title="步骤二"></a>步骤二</h2><ul>
<li>用少量的 Target 内容语音，重复上述过程；</li>
<li>作用就是在前述的 SI 模型中，添加一点 SD（依赖于当前的 Target Speaker）</li>
</ul>
<hr>
<h2 id="步骤三："><a href="#步骤三：" class="headerlink" title="步骤三："></a>步骤三：</h2><p>具体转换实现时：</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627201313.png" alt="image-20200627201311810"></p>
<ul>
<li>转换时，输入source语音；</li>
<li>提取该source语音的五个特征；</li>
<li>对其中的 $logf0$ 做微调：其中 $\mu$ 表示均值，$\sigma$ 表示方差，$logf0_y$ 表示转换好的Target $ logf0 $</li>
<li>上述$logf0_y$和其他四个 source 的特征，一起送入第二步微调完的模型，做转换；</li>
<li>完事了；</li>
</ul>
<h2 id="以上是整体的优化方案；"><a href="#以上是整体的优化方案；" class="headerlink" title="以上是整体的优化方案；"></a>以上是整体的优化方案；</h2><h2 id="以下还有一点：对-SI-WaveNet-结构本身再做调整："><a href="#以下还有一点：对-SI-WaveNet-结构本身再做调整：" class="headerlink" title="以下还有一点：对 SI WaveNet 结构本身再做调整："></a>以下还有一点：对 SI WaveNet 结构本身再做调整：</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627204424.png" alt="image-20200627204422535"></p>
<ul>
<li><p>在 2019 也是这群人，发了一篇关于 WaveNet 内部结构改造：</p>
<ul>
<li><strong>data-efﬁcient SD WaveNet vocoder</strong></li>
</ul>
</li>
<li><p>本文则对上面的改造再做优化： <strong>SD</strong> 改造为 <strong>SVD</strong>（singular value decomposition）<strong>奇异值分解</strong></p>
</li>
<li><p>以期降低复杂度，减少训练参数</p>
</li>
<li><p>具体实现上：</p>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627204710.png" alt="image-20200627204708474"></p>
</li>
<li><p><strong>：</strong>在 每一个 <strong>扩展卷积层</strong> 后面，再加一个 <strong>1 x 1</strong> 的卷积层；</p>
</li>
<li><p>说是这样就能显著减少 <strong>模型参数量</strong>；</p>
</li>
<li><p>——&gt;训练时间减少，效果还和19年的文章效果差不多；</p>
</li>
</ul>
<p><strong>Ps</strong>.（TF 当中倒是有一个单独的 SVD 工具，但是应该是针对更具体的计算公式的，和这里的 在WaveNet 模型内部优化方法不太一样？不确定？）</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627230800.png" alt="image-20200627230757346"></p>
<hr>
<h1 id="6-杂项整理"><a href="#6-杂项整理" class="headerlink" title="6.杂项整理"></a>6.杂项整理</h1><ul>
<li>数据集：VC的常规数据集两个：CMU-ARCTIC  &amp;&amp;  <strong>CSTR-VCTK</strong>（跑过torch版stargan了：109人，44 h，每人三百条左右语音，都是平行数据；两个大类：16K &amp; 48K；另外还配有文本，还可以用作合成数据）</li>
<li>本文把 VCTK 全拿来训练 SI WaveNet 了；</li>
<li>转换步骤，用的 ARCTIC 数据集；</li>
<li>其他一些实现细节：</li>
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627212421.png" alt="image-20200627212417782"></li>
<li>其他需要的作为对比的 <strong>Baseline</strong> 模型的构建参数给了挺多；不展开了；<ul>
<li>• AMA-WORLD:</li>
<li>• AMA-WaveNet:</li>
<li>• WaveNet-adp:</li>
<li>•WaveNet-SVD-adp:【本文提出的】</li>
</ul>
</li>
</ul>
<h2 id="新增一个-主观评价指标"><a href="#新增一个-主观评价指标" class="headerlink" title="* 新增一个 主观评价指标"></a>* 新增一个 主观评价指标</h2><h3 id="AB-and-XAB-测试：【A-B】中选一个-【不选-A-B】-三个中选一个"><a href="#AB-and-XAB-测试：【A-B】中选一个-【不选-A-B】-三个中选一个" class="headerlink" title="AB and XAB 测试：【A/B】中选一个 / 【不选/A/B】 三个中选一个"></a>AB and XAB 测试：【A/B】中选一个 / 【不选/A/B】 三个中选一个</h3><ul>
<li>multiple stimuli with hidden reference and anchor (<strong>MUSHRA</strong>)</li>
<li>“主观评估中间声音质量的方法”</li>
<li>–：让听众在两者之间选择一个更优秀的结果；置信区间取 95%</li>
<li></li>
</ul>
<h2 id="新增一个-Objective-evaluation-客观评价指标"><a href="#新增一个-Objective-evaluation-客观评价指标" class="headerlink" title="* 新增一个 Objective evaluation 客观评价指标"></a>* 新增一个 Objective evaluation 客观评价指标</h2><ul>
<li><p><strong>RMSE</strong>（root mean squared error）：均方根误差；【单位（dB）】</p>
</li>
<li><p>：evaluate distortion between the target and converted speech.</p>
</li>
<li><p>原理和 MCD 差不多；MCD 评测的是经过 DTW 的语音 Mel 谱特征；</p>
</li>
<li><p>🌟<a href="https://www.w3cschool.cn/tensorflow_python/tensorflow_python-15ev2z8o.html" target="_blank" rel="noopener"><strong>Tensorflow 有对应的 API</strong></a>：</p>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627225534.png" alt="image-20200627225531701"></p>
</li>
<li><p>RMSE 他在这里处理的对象比较细致：</p>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627213700.png" alt="image-20200627213656629"></p>
</li>
<li><h2 id="疑问："><a href="#疑问：" class="headerlink" title="疑问："></a>疑问：</h2></li>
</ul>
<ol>
<li><strong>frequency bin</strong>：频率槽；这个参数的 <strong>频率间隔</strong> 一般设置多少？</li>
</ol>
<ul>
<li>是按照 <strong>1HZ</strong> 来分隔吗？？？</li>
<li>【WORLD特征的帧长是 5ms（5ms frame shift）】</li>
</ul>
<ol start="2">
<li>这个 <strong>magnitude</strong> 值，是直接用 <strong>当前频率的 频谱图幅值</strong> 吗？</li>
</ol>
<h1 id="7-其他疑问点："><a href="#7-其他疑问点：" class="headerlink" title="7.其他疑问点："></a>7.其他疑问点：</h1><ul>
<li><ol>
<li><strong>The speech is encoded by 8 bits µ -law.</strong> ： 8 bits µ -law 是什么规范；</li>
</ol>
</li>
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627213249.png" alt="image-20200627213248518"></p>
</li>
<li><ol start="2">
<li><strong>PPG</strong> 的 <strong>具体构建网络</strong> 应该是怎么样的，有统一的代码模型吗。有点凌乱；</li>
</ol>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627214950.png" alt="image-20200627214947808"></p>
</li>
<li></li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
        <category>test</category>
      </categories>
      <tags>
        <tag>VC</tag>
        <tag>论文阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell_find命令备忘小结</title>
    <url>/2023/07/08/Shell_find%E5%91%BD%E4%BB%A4%E5%A4%87%E5%BF%98%E5%B0%8F%E7%BB%93/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id><a href="#" class="headerlink" title></a></h2><p><a href="https://www.cnblogs.com/huyuanblog/p/10136286.html" target="_blank" rel="noopener">参考文章</a></p>
<h2 id="问题来源：Kaldi-librispeech-example：数据预处理脚本文件："><a href="#问题来源：Kaldi-librispeech-example：数据预处理脚本文件：" class="headerlink" title="问题来源：Kaldi librispeech example：数据预处理脚本文件："></a>问题来源：<strong>Kaldi</strong> librispeech example：数据预处理脚本文件：</h2><p><strong>path = kaldi/egs/librispeech/s5/local/da ta_prep.sh</strong></p>
<a id="more"></a>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 双重<span class="keyword">for</span>循环</span></span><br><span class="line"><span class="meta">#</span><span class="bash">第一层循环，按说话人</span></span><br><span class="line">for reader_dir in $(find -L $src -mindepth 1 -maxdepth 1 -type d | sort); do</span><br><span class="line">  reader=$(basename $reader_dir)</span><br><span class="line">  if ! [ $reader -eq $reader ]; then  # not integer.</span><br><span class="line">    echo "$0: unexpected subdirectory name $reader"</span><br><span class="line">    exit 1;</span><br><span class="line">  fi</span><br><span class="line">  </span><br><span class="line"><span class="meta">	#</span><span class="bash"> 从说话人信息文件中提取性别</span></span><br><span class="line">  reader_gender=$(egrep "^$reader[ ]+\|" $spk_file | awk -F'|' '&#123;gsub(/[ ]+/, ""); print tolower($2)&#125;')</span><br><span class="line">  if [ "$reader_gender" != 'm' ] &amp;&amp; [ "$reader_gender" != 'f' ]; then</span><br><span class="line">    echo "Unexpected gender: '$reader_gender'"</span><br><span class="line">    exit 1;</span><br><span class="line">  fi</span><br><span class="line">  </span><br><span class="line"><span class="meta">	#</span><span class="bash">第二重<span class="keyword">for</span>循环，按章节</span></span><br><span class="line">  for chapter_dir in $(find -L $reader_dir/ -mindepth 1 -maxdepth 1 -type d | sort); do</span><br><span class="line">    chapter=$(basename $chapter_dir)</span><br><span class="line">    if ! [ "$chapter" -eq "$chapter" ]; then</span><br><span class="line">      echo "$0: unexpected chapter-subdirectory name $chapter"</span><br><span class="line">      exit 1;</span><br><span class="line">    fi</span><br><span class="line"></span><br><span class="line">    find -L $chapter_dir/ -iname "*.flac" | sort | xargs -I% basename % .flac | \</span><br><span class="line">      awk -v "dir=$chapter_dir" '&#123;printf "%s flac -c -d -s %s/%s.flac |\n", $0, dir, $0&#125;' &gt;&gt;$wav_scp|| exit 1</span><br><span class="line"></span><br><span class="line">    chapter_trans=$chapter_dir/$&#123;reader&#125;-$&#123;chapter&#125;.trans.txt</span><br><span class="line">    [ ! -f  $chapter_trans ] &amp;&amp; echo "$0: expected file $chapter_trans to exist" &amp;&amp; exit 1</span><br><span class="line">    cat $chapter_trans &gt;&gt;$trans</span><br><span class="line"></span><br><span class="line">    # NOTE: For now we are using per-chapter utt2spk. That is each chapter is considered</span><br><span class="line">    #       to be a different speaker. This is done for simplicity and because we want</span><br><span class="line">    #       e.g. the CMVN to be calculated per-chapter</span><br><span class="line">    awk -v "reader=$reader" -v "chapter=$chapter" '&#123;printf "%s %s-%s\n", $1, reader, chapter&#125;' \</span><br><span class="line">      &lt;$chapter_trans &gt;&gt;$utt2spk || exit 1</span><br><span class="line"></span><br><span class="line">    # reader -&gt; gender map (again using per-chapter granularity)</span><br><span class="line">    echo "$&#123;reader&#125;-$&#123;chapter&#125; $reader_gender" &gt;&gt;$spk2gender</span><br><span class="line">  done</span><br><span class="line">done</span><br></pre></td></tr></table></figure>



<h3 id="命令语句解析"><a href="#命令语句解析" class="headerlink" title="命令语句解析"></a>命令语句解析</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(find -L $src -mindepth 1 -maxdepth 1 -type d | sort)</span><br></pre></td></tr></table></figure>

<blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"><span class="comment"># 使用格式</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">find [-H] [-L] [-P] [-D debugopts] [-Olevel] [path...] [expression]</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">和符号链接相关的选项:</span></span><br><span class="line">   -P   不跟踪符号链接(默认行为)</span><br><span class="line">   -L   当 find 检查或打印有关文件的信息时, 所使用的信息应取自链接指向的文件的属性, 而不是链接本身</span><br><span class="line">   -H   和 -L 参数刚好相反, 当 find 检查或打印有关文件的信息时, 所使用的信息应取自符号链接的属性</span><br></pre></td></tr></table></figure>



<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">EXPRESSIONS(表达式):</span></span><br><span class="line"> OPTIONS(选项):</span><br><span class="line">   -d、-depth       在查找文件时, 首先查找当前目录中的文件, 然后再在其子目录中查找</span><br><span class="line">   -maxdepth n       find 查找目录的最大深度</span><br><span class="line">   -mindepth n       find 从指定的目录的第几层深度开始查找</span><br><span class="line">   -mount         查找文件时不跨越文件系统的 mount 点</span><br><span class="line">   -follow         和 ``-``L 参数类似</span><br><span class="line">   -regextype       指定后面所使用的正则表达式语法, 默认为 emacs</span><br><span class="line">     posix-awk        类 awk 的正则表达式语法 </span><br><span class="line">     posix-basic       基本正则表达式</span><br><span class="line">     posix-egrep       不使用正则表达式</span><br><span class="line">     posix-extended     扩展正则表达式</span><br></pre></td></tr></table></figure>



<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">-<span class="built_in">type</span></span></span><br><span class="line">       b   块设备</span><br><span class="line">       c   字符设备</span><br><span class="line">       d   目录</span><br><span class="line">       p   命名管道</span><br><span class="line">       f   文件</span><br><span class="line">       l   链接文件</span><br><span class="line">       s   socket 文件</span><br></pre></td></tr></table></figure>


</blockquote>
<hr>
<p>🌟 想起顾芯怡教的一招</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs dfs -ls     - hdfs://haruna/home/byte_arnold_hl_speech_asr/user/huanglu.thu19/corpus/edu/chinglish_haitian_2kh_16k/wav_ark/k190/*.scp | wc -l</span><br></pre></td></tr></table></figure>

<blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">🌟 wc -l  <span class="comment"># 统计文件个数</span></span></span><br></pre></td></tr></table></figure>



<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">🌟 ls -l *.wav | wc -l  <span class="comment"># 统计某个目录下 某种后缀的文件个数</span></span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">ls -l *.wav | grep <span class="string">"^-"</span> | wc -l</span></span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">grep <span class="string">"^-"</span>  <span class="comment"># 过滤ls的输出信息，只保留一般文件，只保留目录是grep "^d"。</span></span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>🌟 在自己电脑下，还是得 <strong>cd 到指定路径下</strong>才行，字节的 HDFS 数据库是只能那么读取列表，所以才可以那么用</p>
<hr>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> basename /tmp/<span class="built_in">test</span>/file.txt</span></span><br><span class="line">file.txt</span><br><span class="line"><span class="meta">$</span><span class="bash"> basename /tmp/<span class="built_in">test</span>/file.txt .txt</span></span><br><span class="line">file</span><br></pre></td></tr></table></figure>

<blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">basename [pathname] [suffix]</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">basename [string] [suffix]</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">suffix为后缀，如果suffix被指定了，basename会将pathname或string中的suffix去掉。</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">$ basename <span class="variable">$PWD</span>/1027.md </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">1027.md</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">$ basename <span class="variable">$PWD</span>/1027.md .md</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">1027</span></span><br></pre></td></tr></table></figure>
</blockquote>
<hr>
<h3 id="🌟awk-命令：一种处理文本文件的语言"><a href="#🌟awk-命令：一种处理文本文件的语言" class="headerlink" title="🌟awk 命令：一种处理文本文件的语言"></a><strong>🌟awk</strong> 命令：一种处理文本文件的语言</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 从说话人信息文件中提取性别</span></span><br><span class="line">reader_gender=$(egrep "^$reader[ ]+\|" $spk_file | awk -F'|' '&#123;gsub(/[ ]+/, ""); print tolower($2)&#125;')</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>-[Kaldi] -[Shell]</category>
      </categories>
      <tags>
        <tag>-[Shell] -[Kaldi]</tag>
      </tags>
  </entry>
  <entry>
    <title>《ONE-SHOT VOICE CONVERSION BY VECTOR QUANTIZATION》</title>
    <url>/2020/10/18/96-%E3%80%8AONE-SHOT%20VOICE%20CONVERSION%20BY%20VECTOR%20QUANTIZATION%E3%80%8B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h3 id="1018组会"><a href="#1018组会" class="headerlink" title="1018组会"></a>1018组会</h3><h3 id="VQVC-简析"><a href="#VQVC-简析" class="headerlink" title="VQVC 简析"></a>VQVC 简析</h3><ul>
<li>《ONE-SHOT VOICE CONVERSION BY VECTOR QUANTIZATION》</li>
<li><a href="https://ericwudayi.github.io/VQVC-DEMO/" target="_blank" rel="noopener">https://ericwudayi.github.io/VQVC-DEMO/</a></li>
</ul>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018132533.png" alt="image-20201018132532040"></p>
<ul>
<li>《VQVC+: One-Shot Voice Conversion by Vector Quantization and U-Net architecture》</li>
</ul>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018001018.png" alt="image-20201018001016143"></p>
<a id="more"></a>

<hr>
<h2 id="1-特点"><a href="#1-特点" class="headerlink" title="1. 特点"></a>1. 特点</h2><ul>
<li>模型结构：AE 类型，U-Net 框架</li>
<li>常见 VC 特征解纠缠思路：The disentangle capability is achieved by vector quantization (VQ), adversarial training, or instance normalization (IN)<ul>
<li>VQ</li>
<li>对抗</li>
<li>IN</li>
</ul>
</li>
<li>VQVC：对 <strong>隐变量</strong> 进行 <strong>量化</strong></li>
</ul>
<hr>
<h2 id="2-相关"><a href="#2-相关" class="headerlink" title="2. 相关"></a>2. 相关</h2><h3 id="（一）直接转换语音，不需要单独-“特征解纠缠”"><a href="#（一）直接转换语音，不需要单独-“特征解纠缠”" class="headerlink" title="（一）直接转换语音，不需要单独 “特征解纠缠”"></a>（一）直接转换语音，不需要单独 “特征解纠缠”</h3><ol>
<li>CycleGAN &amp;&amp; StarGAN 专注于解决 <strong>“多对多问题”</strong> </li>
<li>BLOW：基于Flow的方式：<strong>直接对 波形 进行转换，而不用转换成 语音特征</strong></li>
</ol>
<h3 id><a href="#" class="headerlink" title></a></h3><h3 id="（二）-speaker-amp-amp-linguistic-解耦"><a href="#（二）-speaker-amp-amp-linguistic-解耦" class="headerlink" title="（二） speaker &amp;&amp; linguistic 解耦"></a>（二） speaker &amp;&amp; linguistic 解耦</h3><ol>
<li>AutoVC—— AE 结构，通过控制 <strong>隐变量</strong> 尺寸（layer dimension）来实现解纠缠</li>
<li>IN 法<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201101114502.png" alt="image-20201018105402931"></li>
<li>VQVC</li>
</ol>
<hr>
<h2 id="3-加入-U-Net-的原因："><a href="#3-加入-U-Net-的原因：" class="headerlink" title="3. 加入 U-Net 的原因："></a>3. 加入 U-Net 的原因：</h2><ul>
<li>以上这些 特征解纠缠 的思路可行，但是由于这些额外限制，会使得重构音质受损</li>
<li>U-Net 的重构能力超级强，所以本身是没法做 VC 的，因为她 解纠缠 能力不好</li>
<li>本文尝试 结合 U-Net 的强大重构能力，以及 VQ 的 特征解耦能力，使得转换的语音，在相似度提高的同时，解决流畅度问题；</li>
</ul>
<hr>
<h2 id="4-VQVC-思路："><a href="#4-VQVC-思路：" class="headerlink" title="4. VQVC 思路："></a>4. VQVC 思路：</h2><ul>
<li>内容：离散码</li>
<li>说话人信息：连续语音信号 - 离散码</li>
</ul>
<ol>
<li>先做 Encode 得到隐变量 Z；</li>
<li>再对这个隐变量做 量化，（结合 codebook ），提取出 内容向量 C（content embedding）——不同说话人，说同样一句话，得到的向量趋于相似向量值；</li>
<li>将 Z 和 C 做量化减法，得到 Speaker Embedding——S</li>
</ol>
<p>内容：离散码<br>说话人信息：连续语音信号-离散码</p>
<ol start="4">
<li></li>
</ol>
<hr>
<ul>
<li>Codebook 本身是可训练的</li>
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018152001.png" alt="image-20201018152000315"></li>
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018152107.png" alt="image-20201018152105943"></li>
</ul>
<p>公式（1）：</p>
<ul>
<li><p>x（带帽）表示的是 语音 x 中的一个小片段；</p>
</li>
<li><p><strong>argmin</strong> 函数：求取 使得函数 y = f(x) 取得最小值的 x 值集合</p>
</li>
<li><p>将 Encode 之后的 隐变量 z，拿来和 Codebook 中的向量 求取差值最小项（欧式距离）</p>
</li>
<li><p>最后的 $C_x$ 是将上述 x^ 做一个整体的 concate 得到的</p>
</li>
</ul>
<p>公式（2）：</p>
<p>结合（VQVC+）的部分公式 来看：<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018152951.png" alt="image-20201018152950185"></p>
<ul>
<li>$E_t$ 表示的是，在所有 T 帧上，求取 Z 和 C 的<strong>期望值 s</strong>，最后将 s 重复 T 次，以此表征 speaker embedding；</li>
<li>重复 T 次是为了让 s 的尺寸 和 C 一样，并方便后续操作</li>
</ul>
<p>公式（3）：</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018153359.png" alt="image-20201018153358420"></p>
<ul>
<li>第一个损失： <strong>重构损失</strong> </li>
</ul>
<p>公式（4）、（5）：</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018153716.png" alt="image-20201018153714828"></p>
<ul>
<li>第二个损失：隐变量损失 L(latent)</li>
<li>目的是更好地从 隐变量Z 中提取 Content Embedding，来代表整个语音片段的内容部分</li>
<li>🌟值得一提的是：为了让 C 更好地表征内容，在这个环节中，Codebook不做更新：防止网络为了缩小 Loss，而把 Codebook 往 说话人信息方向靠，使得提取出来的 Content Embedding 含有 Spealer 信息；</li>
</ul>
<p>公式（5）：</p>
<ul>
<li>两个 Loss 的叠加</li>
</ul>
<hr>
<h2 id="关于-IN："><a href="#关于-IN：" class="headerlink" title="关于 IN："></a>关于 IN：</h2><ul>
<li>本图来自（VQVC+）：</li>
</ul>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018154453.png" alt="image-20201018154451621"></p>
<ul>
<li>在 Encode 得到隐变量 z 之后，在进一步提取 content embedding 之前，需要先做一次 IN（Instant Norm）</li>
<li>IN 本身也被其他论文证明，具有</li>
<li>特征解耦 的功效（这里 IN 是必须操作）</li>
<li>这样能预先一步过滤 说话人信息</li>
</ul>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018160224.png" alt="image-20201018160153595"></p>
<hr>
<h2 id="一些特征处理细节："><a href="#一些特征处理细节：" class="headerlink" title="一些特征处理细节："></a>一些特征处理细节：</h2><ul>
<li>n_mels = 160</li>
<li>GriffinLim</li>
<li>24K</li>
<li>一些额外的规范化normalize 处理：<ul>
<li>减去均值，除以均方差，处理到【0，1】</li>
</ul>
</li>
</ul>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018155430.png" alt="image-20201018155429424"></p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018155439.png" alt="image-20201018155438508"></p>
<hr>
<h2 id="结果："><a href="#结果：" class="headerlink" title="结果："></a>结果：</h2><ol>
<li>消融实验</li>
</ol>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018162111.png" alt="image-20201018162109398"></p>
<ol start="2">
<li>ABX 实验<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018162140.png" alt="image-20201018162138602"></li>
</ol>
<ol start="3">
<li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018162224.png" alt="image-20201018162222357"></p>
</li>
<li><p>Speaker Embedding 区分度：语音片段长度120帧；CodeBook 尺寸 32；训练使用数据 20 人<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018162242.png" alt="image-20201018162241525"></p>
</li>
</ol>
<h2 id="疑问点："><a href="#疑问点：" class="headerlink" title="疑问点："></a>疑问点：</h2><ul>
<li><p>量化部分的具体操作看不懂，说的不详细</p>
</li>
<li><p>没有代码对照</p>
</li>
</ul>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018155903.png" alt="image-20201018155901510"></p>
<hr>
<h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><ul>
<li>创新意义 &gt; 实用意义</li>
<li>没脱离 AE 结构，还是在隐变量 特征解耦上做文章，效果不突出；</li>
<li>Demo 听起来，语气语调信息损失严重</li>
<li>当作已有方法总结中的一种，不建议使用</li>
</ul>
<hr>
<ul>
<li>开题报告</li>
<li>PPT</li>
<li></li>
</ul>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>VC</tag>
        <tag>论文阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Sox环境解决办法</title>
    <url>/2023/07/08/Sox%E7%8E%AF%E5%A2%83%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p><a href="https://www.bbsmax.com/A/ZOJPRymxdv/" target="_blank" rel="noopener">参考文章</a></p>
<ol>
<li>下载<a href="https://sourceforge.net/projects/sox/" target="_blank" rel="noopener">sox-14.4.1.tar.gz</a></li>
</ol>
<p>🌟（不能简单通过pip install 来安装）</p>
<ol start="2">
<li>安装sox文件</li>
</ol>
<p>　　1）解压　　tar -zxvf sox-14.4.1.tar.gz</p>
<p>　　2）进入sox14.4.1目录中执行./configure</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./configure --prefix=/ssd3/other/huangsj/sox_install</span><br><span class="line"><span class="meta">#</span><span class="bash"> 加上prefix，在自定义目录安装，不然机器环境太乱，会找不到</span></span><br></pre></td></tr></table></figure>

<p>　　3）执行 make命令</p>
<p>　　4）执行make install命令</p>
<ol start="3">
<li>添加环境变量</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br><span class="line"></span><br><span class="line">export PATH=/ssd3/other/huangsj/sox_install/bin:$PATH</span><br><span class="line"></span><br><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>ok，再来运行一下 librispeech  ./run.sh</li>
</ol>
<hr>
]]></content>
      <categories>
        <category>-[kaldi]</category>
      </categories>
      <tags>
        <tag>-[Sox] -[kaldi]</tag>
      </tags>
  </entry>
  <entry>
    <title>《AiShell3》</title>
    <url>/2020/11/08/%E3%80%8AAiShell3%E3%80%8B-98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="1108组会："><a href="#1108组会：" class="headerlink" title="1108组会："></a>1108组会：</h2><ul>
<li><input checked disabled type="checkbox"> 完成<strong>软著申请</strong></li>
<li><input checked disabled type="checkbox"> 完成 <strong>开题PPT 修改</strong></li>
<li><input checked disabled type="checkbox"> 完成 VC综述 <strong>论文整理</strong></li>
<li><input disabled type="checkbox"> <strong>尚未完成</strong> 开题综述 <strong>主体部分</strong>（花了较多时间看<strong>格式处理</strong>）</li>
<li><input checked disabled type="checkbox"> 阅读<strong>《AiShell-3》</strong>论文：值得分享一个亮点（speaker-embedding-cycle-consistence Loss）</li>
</ul>
<a id="more"></a>





<hr>
<ul>
<li><a href="https://github.com/sos1sos2Sixteen/aishell-3-baseline-fc" target="_blank" rel="noopener">源码</a></li>
<li><a href="https://sos1sos2sixteen.github.io/aishell3/" target="_blank" rel="noopener">Demo</a></li>
<li></li>
</ul>
<p><strong>Boild-polit</strong> 数据集在15043上有？</p>
<p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201108153939.png" alt="image-20201108153938426"></p>
<hr>
<ul>
<li><p>前端：Tacotron</p>
</li>
<li><p>后端：MelGAN</p>
</li>
<li><p>🌟特点：在多说话人合成任务上，为了进一步增加相似度，提出了“<strong>speaker identity feedback constraint</strong>”</p>
</li>
<li><p>公式上体现：</p>
<ul>
<li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201108153544.png" alt="image-20201108153543937"></li>
</ul>
</li>
<li><p><strong>部分</strong>，先<strong>预训练</strong>，然后在训练 <strong>Tacotron</strong> 的时候参数<strong>不再参与训练</strong> <strong>Frozen</strong></p>
</li>
</ul>
<hr>
<h2 id="另一些亮点："><a href="#另一些亮点：" class="headerlink" title="另一些亮点："></a>另一些亮点：</h2><ol>
<li><p>Tacotron2 中，对长序列语音的合成，表现乏力；</p>
<ul>
<li><p>通常改进方法是：从 <strong>hybrid-attention mechanism</strong> 改进为 <strong>purely location-based attention mechanisms</strong> ，即 Attention 机制的改进</p>
</li>
<li><p>但是这么弄，会使得 长句子的 韵律表现很差</p>
</li>
<li><p>本文转用 <strong>data augmentation</strong> <strong>数据增强</strong> 来处理长句子合成问题</p>
</li>
<li><p>扩充后的数据用于<strong>微调收敛于原始数据集</strong>的<strong>TTS模型</strong>。</p>
</li>
</ul>
</li>
<li><p>在语音合成任务中，之前较少看见 VAD 操作，一般在识别任务上用的比较多；</p>
<ul>
<li>本文在数据预处理上，用 基于能量谱的 VAD 来对训练集 语音开始部分的静音帧进行去除</li>
<li>帮助加速后续的 <strong>优化对齐环节</strong> </li>
</ul>
</li>
<li></li>
</ol>
<hr>
<p>🌟备注：</p>
<ol>
<li>在公司里 &amp;&amp; VCC2020中，很多队伍提到，用 <strong>24k</strong> 的生成效果比 <strong>16k</strong> 提升显著，本文是用<strong>16k</strong>，之后可从这个点做稍微提升</li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>论文阅读笔记</category>
      </categories>
      <tags>
        <tag>VC</tag>
        <tag>论文阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>语音赛道产业版图</title>
    <url>/2022/06/17/%E8%AF%AD%E9%9F%B3%E8%B5%9B%E9%81%93%E4%BA%A7%E4%B8%9A%E7%89%88%E5%9B%BE-100/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20220617140240.png" alt="image-20220617140240495"></p>
]]></content>
  </entry>
</search>
