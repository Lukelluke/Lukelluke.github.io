<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>黄圣杰</title>
  
  <subtitle>bjfuhsj.top</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-10-19T10:14:56.030Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>黄圣杰</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Pytorch —— tensor 乘法的笔记</title>
    <link href="http://yoursite.com/2020/12/11/Pytorch%20%E2%80%94%E2%80%94%20tensor%20%E4%B9%98%E6%B3%95%E7%9A%84%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/12/11/Pytorch%20%E2%80%94%E2%80%94%20tensor%20%E4%B9%98%E6%B3%95%E7%9A%84%E7%AC%94%E8%AE%B0/</id>
    <published>2020-12-11T08:25:48.204Z</published>
    <updated>2021-10-19T10:14:56.030Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="Pytorch-——-tensor-乘法的笔记"><a href="#Pytorch-——-tensor-乘法的笔记" class="headerlink" title="Pytorch —— tensor 乘法的笔记"></a>Pytorch —— tensor 乘法的笔记</h2><hr><p>参考链接：</p><ul><li><p><a href="https://blog.csdn.net/zjhao666/article/details/97756377" target="_blank" rel="noopener">一个关于pytorch的tensor点乘的小问题</a></p></li><li><p><a href="https://blog.csdn.net/qq_36704378/article/details/108173371" target="_blank" rel="noopener">torch.Tensor的4种乘法</a></p></li><li><p><a href="https://blog.csdn.net/sinat_35907936/article/details/105329984" target="_blank" rel="noopener">Pytorch张量操作（包括torch.stack()理解、广播(broadcastable)的理解）</a></p></li><li><p>🌟<a href="https://pytorch.org/docs/stable/torch.html?highlight=mul#torch.mul" target="_blank" rel="noopener">官方链接</a></p></li></ul><hr><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Pytorch-——-tensor-乘法的笔记&quot;&gt;&lt;a href=&quot;#Pytorch-——-tensor-乘法的笔记&quot; class=&quot;headerlink&quot; title=&quot;Pytorch —— tensor 乘法的笔记&quot;&gt;&lt;/a&gt;Pytorch —— tensor 乘法的笔记&lt;/h2&gt;&lt;hr&gt;
&lt;p&gt;参考链接：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/zjhao666/article/details/97756377&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;一个关于pytorch的tensor点乘的小问题&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/qq_36704378/article/details/108173371&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;torch.Tensor的4种乘法&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/sinat_35907936/article/details/105329984&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Pytorch张量操作（包括torch.stack()理解、广播(broadcastable)的理解）&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;🌟&lt;a href=&quot;https://pytorch.org/docs/stable/torch.html?highlight=mul#torch.mul&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方链接&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>PGAN 训练笔记</title>
    <link href="http://yoursite.com/2020/12/08/PGAN%20%E8%AE%AD%E7%BB%83%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/12/08/PGAN%20%E8%AE%AD%E7%BB%83%E7%AC%94%E8%AE%B0/</id>
    <published>2020-12-08T14:13:21.568Z</published>
    <updated>2021-10-19T10:17:31.285Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>PGAN 训练笔记</p><a id="more"></a><hr><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201208221357.png" alt="image-20201208221355969"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;path&#x2F;to&#x2F;database  &#x3D;  &#x2F;big_data&#x2F;hsj&#x2F;ParallelWaveGAN&#x2F;egs&#x2F;aishell3&#x2F;wav</span><br></pre></td></tr></table></figure><!--more--><!--more--><hr><ul><li>目前使用的是，所有 AiShell3数据集，但是用的是 single_speaker 的代码</li><li>数据的处理用到了 sklearn.StandardScaler 的 「均值、方差」归一化，（对训练集），所以咱们StarGAN-vc2中也暂时不自己写 数据预处理的环节了，用他处理好的 .npy 来读取，然后再做 【截取固定帧数】的操作</li></ul><hr><h2 id="StarGAN-VC2-一些参数："><a href="#StarGAN-VC2-一些参数：" class="headerlink" title="StarGAN-VC2 一些参数："></a>StarGAN-VC2 一些参数：</h2><ul><li><strong>batch_size</strong>=8</li><li><strong>learning_rate_G</strong> = 0.0002 ｜ <strong>learning_rate_D</strong> = 0.0001</li><li>每条句子切成 ：<strong>128 帧</strong> </li></ul><ul><li><strong>22050Hz</strong></li><li><strong>34 维度 sp</strong></li><li><strong>帧移</strong>： 5ms</li><li></li></ul><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201209191607.png" alt="image-20201209191606378"></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201209191843.png" alt="image-20201209191842901"></p><hr><h2 id="VC-2-结构："><a href="#VC-2-结构：" class="headerlink" title="VC 2 结构："></a>VC 2 结构：</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201209193704.png" alt="image-20201209193703036"></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201209193747.png" alt="image-20201209193746702"></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201209193731.png" alt="image-20201209193730230"></p><hr><h2 id="VC-1-结构"><a href="#VC-1-结构" class="headerlink" title="VC 1 结构"></a>VC 1 结构</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201209193905.png" alt="image-20201209193903655"></p><hr><h2 id="Torch官方算卷积的公式"><a href="#Torch官方算卷积的公式" class="headerlink" title="Torch官方算卷积的公式"></a>Torch官方算卷积的公式</h2><p><a href="https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html#torch.nn.Conv2d" target="_blank" rel="noopener">https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html#torch.nn.Conv2d</a></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201209202938.png" alt="image-20201209202936687"></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201209202948.png" alt="image-20201209202947175"></p><hr><p><a href="https://blog.csdn.net/qiu931110/article/details/104292129![image-20201210134725352](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201210134726.png)" target="_blank" rel="noopener">https://blog.csdn.net/qiu931110/article/details/104292129![image-20201210134725352](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201210134726.png)</a></p><p><a href="https://www.jianshu.com/p/d8b77cc02410" target="_blank" rel="noopener">https://www.jianshu.com/p/d8b77cc02410</a></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201210134801.png" alt="image-20201210134800412"></p><hr><h2 id="Tensorflow-版本的-CIN"><a href="#Tensorflow-版本的-CIN" class="headerlink" title="Tensorflow 版本的 CIN"></a>Tensorflow 版本的 <a href="https://github.com/MingtaoGuo/Conditional-Instance-Norm-for-n-Style-Transfer/blob/master/ops.py#L5" target="_blank" rel="noopener">CIN</a></h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201210152752.png" alt="image-20201210152751386"></p><p>参考这两份：</p><ul><li><strong>Torch</strong>：<a href="https://github.com/ChinmayLad/neural-style-transfer/blob/dc44cd261fd9bdf684440ef104cec65fe5be15c5/normalization.py" target="_blank" rel="noopener">https://github.com/ChinmayLad/neural-style-transfer/blob/dc44cd261fd9bdf684440ef104cec65fe5be15c5/normalization.py</a></li><li><strong>TensorFlow</strong>:<a href="https://github.com/MingtaoGuo/Conditional-Instance-Norm-for-n-Style-Transfer/blob/master/ops.py#L5" target="_blank" rel="noopener">https://github.com/MingtaoGuo/Conditional-Instance-Norm-for-n-Style-Transfer/blob/master/ops.py#L5</a></li><li><a href="https://github.com/MingtaoGuo/Conditional-Instance-Norm-for-n-Style-Transfer/issues/1" target="_blank" rel="noopener">对应回答</a> issue</li><li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201210163236.png" alt="image-20201210163234409"></li><li></li></ul><p>1211:</p><h1 id="CIN-个人理解："><a href="#CIN-个人理解：" class="headerlink" title="CIN 个人理解："></a>CIN 个人理解：</h1><p><strong>输入：</strong>one-hot = 【1，0，0，0，】 ；特征 x =【8，512，32】=【batch， channel=每个单词用多少特征来表示俄mbedding， 句子长度=含有32个单词】</p><p><strong>过程：</strong></p><ol><li>先是构造一张特征表，尺寸为【说话人数目 / 特征种类， 每个说话人 用多少维度的embedding 来表示 / 每个单词的特征通道数「一维角度来看」】 = 【4， 512】</li><li>然后，用对应的一个说话人的 one-hot 来取这张表上的特征值 【1，num_styles=4】x【num_styles=4，num_in=特征数=512】</li><li>在这个过程中，我们用Torch实现时，可以用 $nn.linear()$ 函数（看作全连接）来表示矩阵乘法：nn.linears（in_channel = 4, out_channel = 512）——<strong>nn.linear（）中的参数是可训练的！会加入到整张图的参数列表中去</strong></li><li>然后把 one-hot 特征送进全连接网络，得到对应的一个特征【512维】，$\gamma$ 和 $\beta$ 都是这么处理的</li><li>最后 ， 两个特征参数，和前面送进来的并经过 <strong>均值 &amp; 归一化 处理过的 x</strong> ，相互配合起来，得到当前层的 <strong>CIN</strong> 结果</li></ol><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201211171011.png" alt="image-20201211171010810"></p><hr><h2 id="关于PixelShuffle"><a href="#关于PixelShuffle" class="headerlink" title="关于PixelShuffle"></a>关于PixelShuffle</h2><ol><li><a href="https://blog.csdn.net/g11d111/article/details/82855946" target="_blank" rel="noopener">https://blog.csdn.net/g11d111/article/details/82855946</a></li><li><a href="https://blog.csdn.net/u014636245/article/details/98071626" target="_blank" rel="noopener">https://blog.csdn.net/u014636245/article/details/98071626</a></li><li><a href="https://pytorch.org/docs/0.3.1/_modules/torch/nn/modules/pixelshuffle.html" target="_blank" rel="noopener">https://pytorch.org/docs/0.3.1/_modules/torch/nn/modules/pixelshuffle.html</a></li></ol><hr><h2 id="关于-GSP：global-sum-pooling"><a href="#关于-GSP：global-sum-pooling" class="headerlink" title="关于 GSP：global sum pooling"></a>关于 GSP：global sum pooling</h2><hr><h2 id="关于-inner-product-向量内积-https-zhuanlan-zhihu-com-p-212461087"><a href="#关于-inner-product-向量内积-https-zhuanlan-zhihu-com-p-212461087" class="headerlink" title="关于 inner product 向量内积 https://zhuanlan.zhihu.com/p/212461087"></a>关于 inner product 向量内积 <a href="https://zhuanlan.zhihu.com/p/212461087" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/212461087</a></h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201211013638.png" alt="image-20201211013636895"></p><hr><h1 id="整个模型-尺寸信息："><a href="#整个模型-尺寸信息：" class="headerlink" title="整个模型 尺寸信息："></a>整个模型 尺寸信息：</h1><ol><li>原本 <strong>Generator</strong> 尺寸：<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201211172645.png" alt="image-20201211172644404"></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PGAN 训练笔记&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>ppg-ac &amp; Deep-Voice操作手册</title>
    <link href="http://yoursite.com/2020/11/29/ppg-ac%20&amp;%20Deep-Voice%E6%93%8D%E4%BD%9C%E6%89%8B%E5%86%8C/"/>
    <id>http://yoursite.com/2020/11/29/ppg-ac%20&amp;%20Deep-Voice%E6%93%8D%E4%BD%9C%E6%89%8B%E5%86%8C/</id>
    <published>2020-11-29T11:42:05.334Z</published>
    <updated>2021-10-19T10:19:38.813Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><a id="more"></a><!--more--><!--more--><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES&#x3D;0 python generate_synthesis.py --ppg2mel_model &#x2F;ssd3&#x2F;other&#x2F;huangsj&#x2F;fac-via-ppg&#x2F;interspeech19-stage&#x2F;ppg2speech-si-am-si-tacotron-bdl2ykwk-final&#x2F;tacotron_checkpoint_11000 --waveglow_model &#x2F;ssd3&#x2F;other&#x2F;huangsj&#x2F;fac-via-ppg&#x2F;interspeech19-stage&#x2F;ppg2speech-si-am-si-tacotron-bdl2ykwk-final&#x2F;waveglow_270000 --teacher_utterance_path &#x2F;ssd3&#x2F;other&#x2F;huangsj&#x2F;fac-via-ppg&#x2F;teacher_data&#x2F;YKWK --output_dir &#x2F;ssd3&#x2F;other&#x2F;huangsj&#x2F;fac-via-ppg&#x2F;output</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES&#x3D;0 python generate_synthesis.py --ppg2mel_model &#x2F;ssd3&#x2F;other&#x2F;huangsj&#x2F;fac-via-ppg&#x2F;interspeech19-stage&#x2F;ppg2speech-si-am-si-tacotron-bdl2ykwk-final&#x2F;tacotron_checkpoint_11000 --waveglow_model &#x2F;ssd3&#x2F;other&#x2F;huangsj&#x2F;fac-via-ppg&#x2F;interspeech19-stage&#x2F;ppg2speech-si-am-si-tacotron-bdl2ykwk-final&#x2F;waveglow_270000 --teacher_utterance_path &#x2F;ssd3&#x2F;other&#x2F;huangsj&#x2F;fac-via-ppg&#x2F;teacher_data --output_dir &#x2F;ssd3&#x2F;other&#x2F;huangsj&#x2F;fac-via-ppg&#x2F;output</span><br></pre></td></tr></table></figure><p>teacher_data/YKWK</p><!--more--><!--more--><!--more--><!--more--><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export PYTHONPATH&#x3D;&#x2F;ssd3&#x2F;other&#x2F;huangsj&#x2F;fac-via-ppg&#x2F;src:$PYTHONPATH</span><br><span class="line"></span><br><span class="line">--teacher_utterance_path  : 输入的是待矫正的语音</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES&#x3D;1 python generate_synthesis.py --ppg2mel_model &#x2F;ssd3&#x2F;other&#x2F;huangsj&#x2F;fac-via-ppg&#x2F;interspeech19-stage&#x2F;ppg2speech-si-am-si-tacotron-bdl2ykwk-final&#x2F;tacotron_checkpoint_11000 --waveglow_model &#x2F;ssd3&#x2F;other&#x2F;huangsj&#x2F;fac-via-ppg&#x2F;interspeech19-stage&#x2F;ppg2speech-si-am-si-tacotron-bdl2ykwk-final&#x2F;waveglow_270000 --teacher_utterance_path &#x2F;ssd3&#x2F;other&#x2F;huangsj&#x2F;fac-via-ppg&#x2F;teacher_data&#x2F;YKWK&#x2F;0023.wav --output_dir &#x2F;ssd3&#x2F;other&#x2F;huangsj&#x2F;fac-via-ppg&#x2F;output</span><br></pre></td></tr></table></figure><p> <a href="../fsdownload/韩式0001.wav">韩式0001.wav</a> </p><p> <a href="../fsdownload/美式0001.wav">美式0001.wav</a> </p><p> <a href="../fsdownload/韩式转美式.wav">韩式转美式.wav</a> </p><hr><h1 id="Deep-Voice-Conversion训练手册"><a href="#Deep-Voice-Conversion训练手册" class="headerlink" title="Deep-Voice-Conversion训练手册"></a><a href="https://github.com/andabi/deep-voice-conversion/issues/115" target="_blank" rel="noopener">Deep-Voice-Conversion训练手册</a></h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train1.py timit -gpu 0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train2.py timit -gpu 0</span><br></pre></td></tr></table></figure><p><a href="https://github.com/andabi/deep-voice-conversion/issues/39#issuecomment-476074396" target="_blank" rel="noopener">链接1</a></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201205192750.png" alt="image-20201205192749348"></p><p><a href="https://github.com/andabi/deep-voice-conversion/issues/93" target="_blank" rel="noopener">链接2</a></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201205192845.png" alt="image-20201205192844846"></p><p><a href="https://github.com/andabi/deep-voice-conversion/issues/115" target="_blank" rel="noopener">链接3</a></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201205193002.png" alt="image-20201205193001370"></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201205193103.png" alt="image-20201205193102248"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为了更通用，写了个代码来转换文件格式：</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">BASE = <span class="string">"/Users/huangshengjie/Desktop/TEST/"</span>   <span class="comment"># 所有文件的根目录</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(BASE):</span><br><span class="line">    <span class="keyword">if</span> len(files) &gt; <span class="number">0</span>:  <span class="comment"># 如果此目录有文件</span></span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:   <span class="comment"># 遍历此目录下的每一个文件</span></span><br><span class="line">            <span class="keyword">if</span> file.find(<span class="string">".WAV"</span>) != <span class="number">-1</span>:   <span class="comment"># 如果文件名中包含c2字样</span></span><br><span class="line">                new_file = file.replace(<span class="string">".WAV"</span>, <span class="string">".wav"</span>)  <span class="comment"># 则将其改成c1</span></span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    os.chdir(root)   <span class="comment"># 修改之前将当前工作目录切换到文件所在目录，否则os.rename会失败</span></span><br><span class="line">                    os.rename(file, new_file)  <span class="comment"># 调用操作系统的重命名功能</span></span><br><span class="line">                <span class="keyword">except</span> OSError <span class="keyword">as</span> e:</span><br><span class="line">                    <span class="keyword">print</span> (e)</span><br><span class="line">                    quit(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>杂记</title>
    <link href="http://yoursite.com/2020/11/29/1129%E6%B5%8B%E8%AF%95Torch%20gpu%20%E6%98%AF%E5%90%A6%E5%8F%AF%E7%94%A8/"/>
    <id>http://yoursite.com/2020/11/29/1129%E6%B5%8B%E8%AF%95Torch%20gpu%20%E6%98%AF%E5%90%A6%E5%8F%AF%E7%94%A8/</id>
    <published>2020-11-29T05:31:42.710Z</published>
    <updated>2020-12-05T10:50:24.115Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-测试-gpu-可用性"><a href="#1-测试-gpu-可用性" class="headerlink" title="1.测试  gpu 可用性"></a>1.测试  gpu 可用性</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>]=<span class="string">'1'</span></span><br><span class="line">print(torch.version.cuda)</span><br><span class="line">print(torch.__version__)</span><br><span class="line">print(torch.cuda.is_available())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>]=<span class="string">'0'</span></span><br><span class="line"><span class="keyword">import</span>  time</span><br><span class="line">tf.test.is_gpu_available()</span><br></pre></td></tr></table></figure><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge librosa</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda install tensorflow-gpu&#x3D;&#x3D;1.9.0  # 自动带cudatoolkit</span><br><span class="line"># 版本匹配信息：https:&#x2F;&#x2F;www.tensorflow.org&#x2F;install&#x2F;source#common_installation_problems</span><br></pre></td></tr></table></figure><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES&#x3D;&quot;1&quot; python train1.py</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES&#x3D;&quot;1&quot; python train1.py timit -gpu 1</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">10000</span>, <span class="number">1000</span>)</span><br><span class="line">b = torch.randn(<span class="number">1000</span>, <span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line">c = torch.matmul(a, b)</span><br><span class="line">t1 = time.time()</span><br><span class="line">print(a.device, t1 - t0, c.norm(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span>)</span><br><span class="line">a = a.to(device)</span><br><span class="line">b = b.to(device)</span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line">c = torch.matmul(a, b)</span><br><span class="line">t2 = time.time()</span><br><span class="line">print(a.device, t2 - t0, c.norm(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line">c = torch.matmul(a, b)</span><br><span class="line">t2 = time.time()</span><br><span class="line">print(a.device, t2 - t0, c.norm(<span class="number">2</span>))</span><br></pre></td></tr></table></figure><h1 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">conda -V </span><br><span class="line">conda info  </span><br><span class="line">conda config --get channels</span><br><span class="line">conda config --show</span><br><span class="line">vim ~&#x2F;.condarc    </span><br><span class="line"></span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F;</span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main&#x2F;</span><br><span class="line"></span><br><span class="line">conda config --remove-key channels</span><br></pre></td></tr></table></figure><h1 id="3-批处理文件改后缀"><a href="#3-批处理文件改后缀" class="headerlink" title="3. 批处理文件改后缀"></a>3. 批处理文件改后缀</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">BASE = <span class="string">"/Users/huangshengjie/Desktop/TEST/"</span>   <span class="comment"># 所有文件的根目录</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(BASE):</span><br><span class="line">    <span class="keyword">if</span> len(files) &gt; <span class="number">0</span>:  <span class="comment"># 如果此目录有文件</span></span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:   <span class="comment"># 遍历此目录下的每一个文件</span></span><br><span class="line">            <span class="keyword">if</span> file.find(<span class="string">".WAV"</span>) != <span class="number">-1</span>:   <span class="comment"># 如果文件名中包含c2字样</span></span><br><span class="line">                new_file = file.replace(<span class="string">".WAV"</span>, <span class="string">".wav"</span>)  <span class="comment"># 则将其改成c1</span></span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    os.chdir(root)   <span class="comment"># 修改之前将当前工作目录切换到文件所在目录，否则os.rename会失败</span></span><br><span class="line">                    os.rename(file, new_file)  <span class="comment"># 调用操作系统的重命名功能</span></span><br><span class="line">                <span class="keyword">except</span> OSError <span class="keyword">as</span> e:</span><br><span class="line">                    <span class="keyword">print</span> (e)</span><br><span class="line">                    quit(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="-[Torch]" scheme="http://yoursite.com/categories/Torch/"/>
    
    
      <category term="-[Torch]" scheme="http://yoursite.com/tags/Torch/"/>
    
  </entry>
  
  <entry>
    <title>Sox环境解决办法</title>
    <link href="http://yoursite.com/2020/11/29/Sox%E7%8E%AF%E5%A2%83%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <id>http://yoursite.com/2020/11/29/Sox%E7%8E%AF%E5%A2%83%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</id>
    <published>2020-11-29T01:59:01.017Z</published>
    <updated>2020-11-29T02:07:15.320Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><a href="https://www.bbsmax.com/A/ZOJPRymxdv/" target="_blank" rel="noopener">参考文章</a></p><ol><li>下载<a href="https://sourceforge.net/projects/sox/" target="_blank" rel="noopener">sox-14.4.1.tar.gz</a></li></ol><p>🌟（不能简单通过pip install 来安装）</p><ol start="2"><li>安装sox文件</li></ol><p>　　1）解压　　tar -zxvf sox-14.4.1.tar.gz</p><p>　　2）进入sox14.4.1目录中执行./configure</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./configure --prefix=/ssd3/other/huangsj/sox_install</span><br><span class="line"><span class="meta">#</span><span class="bash"> 加上prefix，在自定义目录安装，不然机器环境太乱，会找不到</span></span><br></pre></td></tr></table></figure><p>　　3）执行 make命令</p><p>　　4）执行make install命令</p><ol start="3"><li>添加环境变量</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br><span class="line"></span><br><span class="line">export PATH=/ssd3/other/huangsj/sox_install/bin:$PATH</span><br><span class="line"></span><br><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure><ol start="4"><li>ok，再来运行一下 librispeech  ./run.sh</li></ol><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="-[kaldi]" scheme="http://yoursite.com/categories/kaldi/"/>
    
    
      <category term="-[Sox] -[kaldi]" scheme="http://yoursite.com/tags/Sox-kaldi/"/>
    
  </entry>
  
  <entry>
    <title>Shell_find命令备忘小结</title>
    <link href="http://yoursite.com/2020/11/27/Shell_find%E5%91%BD%E4%BB%A4%E5%A4%87%E5%BF%98%E5%B0%8F%E7%BB%93/"/>
    <id>http://yoursite.com/2020/11/27/Shell_find%E5%91%BD%E4%BB%A4%E5%A4%87%E5%BF%98%E5%B0%8F%E7%BB%93/</id>
    <published>2020-11-27T05:20:47.426Z</published>
    <updated>2021-10-19T10:16:03.511Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id><a href="#" class="headerlink" title></a></h2><p><a href="https://www.cnblogs.com/huyuanblog/p/10136286.html" target="_blank" rel="noopener">参考文章</a></p><h2 id="问题来源：Kaldi-librispeech-example：数据预处理脚本文件："><a href="#问题来源：Kaldi-librispeech-example：数据预处理脚本文件：" class="headerlink" title="问题来源：Kaldi librispeech example：数据预处理脚本文件："></a>问题来源：<strong>Kaldi</strong> librispeech example：数据预处理脚本文件：</h2><p><strong>path = kaldi/egs/librispeech/s5/local/da ta_prep.sh</strong></p><a id="more"></a><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 双重<span class="keyword">for</span>循环</span></span><br><span class="line"><span class="meta">#</span><span class="bash">第一层循环，按说话人</span></span><br><span class="line">for reader_dir in $(find -L $src -mindepth 1 -maxdepth 1 -type d | sort); do</span><br><span class="line">  reader=$(basename $reader_dir)</span><br><span class="line">  if ! [ $reader -eq $reader ]; then  # not integer.</span><br><span class="line">    echo "$0: unexpected subdirectory name $reader"</span><br><span class="line">    exit 1;</span><br><span class="line">  fi</span><br><span class="line">  </span><br><span class="line"><span class="meta">#</span><span class="bash"> 从说话人信息文件中提取性别</span></span><br><span class="line">  reader_gender=$(egrep "^$reader[ ]+\|" $spk_file | awk -F'|' '&#123;gsub(/[ ]+/, ""); print tolower($2)&#125;')</span><br><span class="line">  if [ "$reader_gender" != 'm' ] &amp;&amp; [ "$reader_gender" != 'f' ]; then</span><br><span class="line">    echo "Unexpected gender: '$reader_gender'"</span><br><span class="line">    exit 1;</span><br><span class="line">  fi</span><br><span class="line">  </span><br><span class="line"><span class="meta">#</span><span class="bash">第二重<span class="keyword">for</span>循环，按章节</span></span><br><span class="line">  for chapter_dir in $(find -L $reader_dir/ -mindepth 1 -maxdepth 1 -type d | sort); do</span><br><span class="line">    chapter=$(basename $chapter_dir)</span><br><span class="line">    if ! [ "$chapter" -eq "$chapter" ]; then</span><br><span class="line">      echo "$0: unexpected chapter-subdirectory name $chapter"</span><br><span class="line">      exit 1;</span><br><span class="line">    fi</span><br><span class="line"></span><br><span class="line">    find -L $chapter_dir/ -iname "*.flac" | sort | xargs -I% basename % .flac | \</span><br><span class="line">      awk -v "dir=$chapter_dir" '&#123;printf "%s flac -c -d -s %s/%s.flac |\n", $0, dir, $0&#125;' &gt;&gt;$wav_scp|| exit 1</span><br><span class="line"></span><br><span class="line">    chapter_trans=$chapter_dir/$&#123;reader&#125;-$&#123;chapter&#125;.trans.txt</span><br><span class="line">    [ ! -f  $chapter_trans ] &amp;&amp; echo "$0: expected file $chapter_trans to exist" &amp;&amp; exit 1</span><br><span class="line">    cat $chapter_trans &gt;&gt;$trans</span><br><span class="line"></span><br><span class="line">    # NOTE: For now we are using per-chapter utt2spk. That is each chapter is considered</span><br><span class="line">    #       to be a different speaker. This is done for simplicity and because we want</span><br><span class="line">    #       e.g. the CMVN to be calculated per-chapter</span><br><span class="line">    awk -v "reader=$reader" -v "chapter=$chapter" '&#123;printf "%s %s-%s\n", $1, reader, chapter&#125;' \</span><br><span class="line">      &lt;$chapter_trans &gt;&gt;$utt2spk || exit 1</span><br><span class="line"></span><br><span class="line">    # reader -&gt; gender map (again using per-chapter granularity)</span><br><span class="line">    echo "$&#123;reader&#125;-$&#123;chapter&#125; $reader_gender" &gt;&gt;$spk2gender</span><br><span class="line">  done</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h3 id="命令语句解析"><a href="#命令语句解析" class="headerlink" title="命令语句解析"></a>命令语句解析</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(find -L $src -mindepth 1 -maxdepth 1 -type d | sort)</span><br></pre></td></tr></table></figure><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"><span class="comment"># 使用格式</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">find [-H] [-L] [-P] [-D debugopts] [-Olevel] [path...] [expression]</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">和符号链接相关的选项:</span></span><br><span class="line">   -P   不跟踪符号链接(默认行为)</span><br><span class="line">   -L   当 find 检查或打印有关文件的信息时, 所使用的信息应取自链接指向的文件的属性, 而不是链接本身</span><br><span class="line">   -H   和 -L 参数刚好相反, 当 find 检查或打印有关文件的信息时, 所使用的信息应取自符号链接的属性</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">EXPRESSIONS(表达式):</span></span><br><span class="line"> OPTIONS(选项):</span><br><span class="line">   -d、-depth       在查找文件时, 首先查找当前目录中的文件, 然后再在其子目录中查找</span><br><span class="line">   -maxdepth n       find 查找目录的最大深度</span><br><span class="line">   -mindepth n       find 从指定的目录的第几层深度开始查找</span><br><span class="line">   -mount         查找文件时不跨越文件系统的 mount 点</span><br><span class="line">   -follow         和 ``-``L 参数类似</span><br><span class="line">   -regextype       指定后面所使用的正则表达式语法, 默认为 emacs</span><br><span class="line">     posix-awk        类 awk 的正则表达式语法 </span><br><span class="line">     posix-basic       基本正则表达式</span><br><span class="line">     posix-egrep       不使用正则表达式</span><br><span class="line">     posix-extended     扩展正则表达式</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">-<span class="built_in">type</span></span></span><br><span class="line">       b   块设备</span><br><span class="line">       c   字符设备</span><br><span class="line">       d   目录</span><br><span class="line">       p   命名管道</span><br><span class="line">       f   文件</span><br><span class="line">       l   链接文件</span><br><span class="line">       s   socket 文件</span><br></pre></td></tr></table></figure></blockquote><hr><p>🌟 想起顾芯怡教的一招</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -ls     - hdfs://haruna/home/byte_arnold_hl_speech_asr/user/huanglu.thu19/corpus/edu/chinglish_haitian_2kh_16k/wav_ark/k190/*.scp | wc -l</span><br></pre></td></tr></table></figure><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">🌟 wc -l  <span class="comment"># 统计文件个数</span></span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">🌟 ls -l *.wav | wc -l  <span class="comment"># 统计某个目录下 某种后缀的文件个数</span></span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">ls -l *.wav | grep <span class="string">"^-"</span> | wc -l</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">grep <span class="string">"^-"</span>  <span class="comment"># 过滤ls的输出信息，只保留一般文件，只保留目录是grep "^d"。</span></span></span><br></pre></td></tr></table></figure></blockquote><p>🌟 在自己电脑下，还是得 <strong>cd 到指定路径下</strong>才行，字节的 HDFS 数据库是只能那么读取列表，所以才可以那么用</p><hr><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> basename /tmp/<span class="built_in">test</span>/file.txt</span></span><br><span class="line">file.txt</span><br><span class="line"><span class="meta">$</span><span class="bash"> basename /tmp/<span class="built_in">test</span>/file.txt .txt</span></span><br><span class="line">file</span><br></pre></td></tr></table></figure><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">basename [pathname] [suffix]</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">basename [string] [suffix]</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">suffix为后缀，如果suffix被指定了，basename会将pathname或string中的suffix去掉。</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">$ basename <span class="variable">$PWD</span>/1027.md </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">1027.md</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">$ basename <span class="variable">$PWD</span>/1027.md .md</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">1027</span></span><br></pre></td></tr></table></figure></blockquote><hr><h3 id="🌟awk-命令：一种处理文本文件的语言"><a href="#🌟awk-命令：一种处理文本文件的语言" class="headerlink" title="🌟awk 命令：一种处理文本文件的语言"></a><strong>🌟awk</strong> 命令：一种处理文本文件的语言</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 从说话人信息文件中提取性别</span></span><br><span class="line">reader_gender=$(egrep "^$reader[ ]+\|" $spk_file | awk -F'|' '&#123;gsub(/[ ]+/, ""); print tolower($2)&#125;')</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/huyuanblog/p/10136286.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;参考文章&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;问题来源：Kaldi-librispeech-example：数据预处理脚本文件：&quot;&gt;&lt;a href=&quot;#问题来源：Kaldi-librispeech-example：数据预处理脚本文件：&quot; class=&quot;headerlink&quot; title=&quot;问题来源：Kaldi librispeech example：数据预处理脚本文件：&quot;&gt;&lt;/a&gt;问题来源：&lt;strong&gt;Kaldi&lt;/strong&gt; librispeech example：数据预处理脚本文件：&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;path = kaldi/egs/librispeech/s5/local/da ta_prep.sh&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="-[Kaldi] -[Shell]" scheme="http://yoursite.com/categories/Kaldi-Shell/"/>
    
    
      <category term="-[Shell] -[Kaldi]" scheme="http://yoursite.com/tags/Shell-Kaldi/"/>
    
  </entry>
  
  <entry>
    <title>《AiShell3》</title>
    <link href="http://yoursite.com/2020/11/08/1108%E7%BB%84%E4%BC%9A%EF%BC%9A/"/>
    <id>http://yoursite.com/2020/11/08/1108%E7%BB%84%E4%BC%9A%EF%BC%9A/</id>
    <published>2020-11-08T07:16:54.000Z</published>
    <updated>2020-11-10T05:36:00.594Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><a id="more"></a><h2 id="1108组会："><a href="#1108组会：" class="headerlink" title="1108组会："></a>1108组会：</h2><ul><li><input checked disabled type="checkbox"> <p>完成<strong>软著申请</strong></p></li><li><input checked disabled type="checkbox"> <p>完成 <strong>开题PPT 修改</strong></p></li><li><input checked disabled type="checkbox"> <p>完成 VC综述 <strong>论文整理</strong></p></li><li><input disabled type="checkbox"> <p><strong>尚未完成</strong> 开题综述 <strong>主体部分</strong>（花了较多时间看<strong>格式处理</strong>）</p></li><li><input checked disabled type="checkbox"> <p>阅读<strong>《AiShell-3》</strong>论文：值得分享一个亮点（speaker-embedding-cycle-consistence Loss）</p></li></ul><hr><ul><li><a href="https://github.com/sos1sos2Sixteen/aishell-3-baseline-fc" target="_blank" rel="noopener">源码</a></li><li><a href="https://sos1sos2sixteen.github.io/aishell3/" target="_blank" rel="noopener">Demo</a></li><li></li></ul><p><strong>Boild-polit</strong> 数据集在15043上有？</p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201108153939.png" alt="image-20201108153938426"></p><hr><ul><li><p>前端：Tacotron</p></li><li><p>后端：MelGAN</p></li><li><p>🌟特点：在多说话人合成任务上，为了进一步增加相似度，提出了“<strong>speaker identity feedback constraint</strong>”</p></li><li><p>公式上体现：</p><ul><li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201108153544.png" alt="image-20201108153543937"></li></ul></li><li><p><strong>部分</strong>，先<strong>预训练</strong>，然后在训练 <strong>Tacotron</strong> 的时候参数<strong>不再参与训练</strong> <strong>Frozen</strong></p></li></ul><hr><h2 id="另一些亮点："><a href="#另一些亮点：" class="headerlink" title="另一些亮点："></a>另一些亮点：</h2><ol><li><p>Tacotron2 中，对长序列语音的合成，表现乏力；</p><ul><li><p>通常改进方法是：从 <strong>hybrid-attention mechanism</strong> 改进为 <strong>purely location-based attention mechanisms</strong> ，即 Attention 机制的改进</p></li><li><p>但是这么弄，会使得 长句子的 韵律表现很差</p></li><li><p>本文转用 <strong>data augmentation</strong> <strong>数据增强</strong> 来处理长句子合成问题</p></li><li><p>扩充后的数据用于<strong>微调收敛于原始数据集</strong>的<strong>TTS模型</strong>。</p></li></ul></li><li><p>在语音合成任务中，之前较少看见 VAD 操作，一般在识别任务上用的比较多；</p><ul><li>本文在数据预处理上，用 基于能量谱的 VAD 来对训练集 语音开始部分的静音帧进行去除</li><li>帮助加速后续的 <strong>优化对齐环节</strong> </li></ul></li><li></li></ol><hr><p>🌟备注：</p><ol><li>在公司里 &amp;&amp; VCC2020中，很多队伍提到，用 <strong>24k</strong> 的生成效果比 <strong>16k</strong> 提升显著，本文是用<strong>16k</strong>，之后可从这个点做稍微提升</li><li></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="VC" scheme="http://yoursite.com/tags/VC/"/>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>1103——WaveNet &amp; 机器学习 考点小结</title>
    <link href="http://yoursite.com/2020/11/03/WaveNet%E5%B0%8F%E7%BB%93/"/>
    <id>http://yoursite.com/2020/11/03/WaveNet%E5%B0%8F%E7%BB%93/</id>
    <published>2020-11-03T07:20:58.000Z</published>
    <updated>2020-11-03T08:01:02.047Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="WaveNet小结"><a href="#WaveNet小结" class="headerlink" title="WaveNet小结"></a>WaveNet小结</h2><a id="more"></a><ol><li><h4 id="WaveNet-一种语音合成的模型-https-www-pianshen-com-article-43431455160-image-20201016164508908-https-blog-1301959139-cos-ap-beijing-myqcloud-com-picGo-20201016164510-png"><a href="#WaveNet-一种语音合成的模型-https-www-pianshen-com-article-43431455160-image-20201016164508908-https-blog-1301959139-cos-ap-beijing-myqcloud-com-picGo-20201016164510-png" class="headerlink" title="WaveNet:一种语音合成的模型 https://www.pianshen.com/article/43431455160/![image-20201016164508908](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016164510.png)"></a>WaveNet:一种语音合成的模型 <a href="https://www.pianshen.com/article/43431455160/![image-20201016164508908](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016164510.png)" target="_blank" rel="noopener">https://www.pianshen.com/article/43431455160/![image-20201016164508908](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016164510.png)</a></h4></li><li><h3 id="WaveNet，一种端到端的语音合成模型https-zhuanlan-zhihu-com-p-51359150"><a href="#WaveNet，一种端到端的语音合成模型https-zhuanlan-zhihu-com-p-51359150" class="headerlink" title="WaveNet，一种端到端的语音合成模型https://zhuanlan.zhihu.com/p/51359150"></a>WaveNet，一种端到端的语音合成模型<img src="/2020/11/03/WaveNet%E5%B0%8F%E7%BB%93/huangshengjie/Documents/2020/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99hexo%E6%B5%8B%E8%AF%95/Typora%E5%8D%9A%E5%AE%A2%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9/20201016164427.png" alt="image-20201016164426034"><a href="https://zhuanlan.zhihu.com/p/51359150" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/51359150</a></h3></li></ol><hr><h3 id="预测："><a href="#预测：" class="headerlink" title="预测："></a>预测：</h3><ol><li><p>机器学习各种 Loss 总结：</p><ol><li><a href="https://blog.csdn.net/perfect1t/article/details/88199179" target="_blank" rel="noopener">https://blog.csdn.net/perfect1t/article/details/88199179</a></li><li><a href="https://www.cnblogs.com/guoyaohua/p/9217206.html" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/9217206.html</a></li><li><a href="https://www.cnblogs.com/lliuye/p/9549881.html" target="_blank" rel="noopener">https://www.cnblogs.com/lliuye/p/9549881.html</a></li></ol></li><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016181353.png" alt="image-20201016181352142"></p></li><li><p><img src="/2020/11/03/WaveNet%E5%B0%8F%E7%BB%93/huangshengjie/Documents/2020/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99hexo%E6%B5%8B%E8%AF%95/Typora%E5%8D%9A%E5%AE%A2%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9/image-20201019005049252.png" alt="image-20201019005049252"><a href="https://zhuanlan.zhihu.com/p/74874291" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/74874291</a></p></li><li><p>常见优化算法及其优缺点：</p><ol><li><p><a href="https://blog.csdn.net/qq_19446965/article/details/81591521" target="_blank" rel="noopener">https://blog.csdn.net/qq_19446965/article/details/81591521</a> </p></li><li><p><a href="http://www.julyedu.com/question/big/kp_id/23/ques_id/1524![image-20201016181800596](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016181802.png)![image-20201016181557363](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016181558.png)" target="_blank" rel="noopener">http://www.julyedu.com/question/big/kp_id/23/ques_id/1524![image-20201016181800596](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016181802.png)![image-20201016181557363](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016181558.png)</a></p></li><li><h3 id="深度学习中优化方法——momentum、Nesterov-Momentum、AdaGrad、Adadelta、RMSprop、Adam"><a href="#深度学习中优化方法——momentum、Nesterov-Momentum、AdaGrad、Adadelta、RMSprop、Adam" class="headerlink" title="深度学习中优化方法——momentum、Nesterov Momentum、AdaGrad、Adadelta、RMSprop、Adam"></a>深度学习中优化方法——momentum、Nesterov Momentum、AdaGrad、Adadelta、RMSprop、Adam</h3><ol><li><h3 id="https-blog-csdn-net-u012328159-article-details-80311892-image-20201019162755198-Users-huangshengjie-Documents-2020-个人网站hexo测试-Typora博客图片文件夹-20201019162757-png"><a href="#https-blog-csdn-net-u012328159-article-details-80311892-image-20201019162755198-Users-huangshengjie-Documents-2020-个人网站hexo测试-Typora博客图片文件夹-20201019162757-png" class="headerlink" title="https://blog.csdn.net/u012328159/article/details/80311892![image-20201019162755198](/Users/huangshengjie/Documents/2020/个人网站hexo测试/Typora博客图片文件夹/20201019162757.png)"></a><a href="https://blog.csdn.net/u012328159/article/details/80311892![image-20201019162755198](/Users/huangshengjie/Documents/2020/个人网站hexo测试/Typora博客图片文件夹/20201019162757.png)" target="_blank" rel="noopener">https://blog.csdn.net/u012328159/article/details/80311892![image-20201019162755198](/Users/huangshengjie/Documents/2020/个人网站hexo测试/Typora博客图片文件夹/20201019162757.png)</a></h3></li><li><p><a href="https://blog.csdn.net/u012328159/article/details/80252012![image-20201019162915626](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201019162916.png)" target="_blank" rel="noopener">https://blog.csdn.net/u012328159/article/details/80252012![image-20201019162915626](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201019162916.png)</a></p></li><li><p><a href="https://www.cnblogs.com/yangmang/p/7477802.html![image-20201019165031144](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201019165036.png)" target="_blank" rel="noopener">https://www.cnblogs.com/yangmang/p/7477802.html![image-20201019165031144](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201019165036.png)</a></p></li></ol></li></ol></li><li><p>常用激活函数：<a href="https://zhuanlan.zhihu.com/p/32610035![image-20201016224701337](/Users/huangshengjie/Documents/2020/个人网站hexo测试/Typora博客图片文件夹/20201016224703.png)" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32610035![image-20201016224701337](/Users/huangshengjie/Documents/2020/个人网站hexo测试/Typora博客图片文件夹/20201016224703.png)</a></p></li><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016224719.jpg" alt="img"></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/73214810![image-20201016224906446](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016224907.png)" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/73214810![image-20201016224906446](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201016224907.png)</a></p></li><li><p><a href="https://www.cnblogs.com/itmorn/p/11132494.html![image-20201019170355380](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201019170356.png)" target="_blank" rel="noopener">https://www.cnblogs.com/itmorn/p/11132494.html![image-20201019170355380](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201019170356.png)</a></p></li><li><p><a href="https://www.cnblogs.com/itmorn/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98/default.html?page=2![image-20201019170708536](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201019170710.png)" target="_blank" rel="noopener">https://www.cnblogs.com/itmorn/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98/default.html?page=2![image-20201019170708536](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201019170710.png)</a></p></li></ol><hr><ol><li><h6 id="A率13折线PCM编码的C语言实现-https-blog-csdn-net-u010480899-article-details-51172633-image-20201017001812322-Users-huangshengjie-Documents-2020-个人网站hexo测试-Typora博客图片文件夹-20201017001813-png"><a href="#A率13折线PCM编码的C语言实现-https-blog-csdn-net-u010480899-article-details-51172633-image-20201017001812322-Users-huangshengjie-Documents-2020-个人网站hexo测试-Typora博客图片文件夹-20201017001813-png" class="headerlink" title="A率13折线PCM编码的C语言实现 https://blog.csdn.net/u010480899/article/details/51172633![image-20201017001812322](/Users/huangshengjie/Documents/2020/个人网站hexo测试/Typora博客图片文件夹/20201017001813.png)"></a>A率13折线PCM编码的C语言实现 <a href="https://blog.csdn.net/u010480899/article/details/51172633![image-20201017001812322](/Users/huangshengjie/Documents/2020/个人网站hexo测试/Typora博客图片文件夹/20201017001813.png)" target="_blank" rel="noopener">https://blog.csdn.net/u010480899/article/details/51172633![image-20201017001812322](/Users/huangshengjie/Documents/2020/个人网站hexo测试/Typora博客图片文件夹/20201017001813.png)</a></h6></li><li><p><a href="https://blog.csdn.net/u012323667/article/details/79214336?utm_medium=distribute.pc_relevant_download.none-task-blog-blogcommendfrombaidu-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant_download.none-task-blog-blogcommendfrombaidu-1.nonecas![image-20201017003241394](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201017003242.png)" target="_blank" rel="noopener">https://blog.csdn.net/u012323667/article/details/79214336?utm_medium=distribute.pc_relevant_download.none-task-blog-blogcommendfrombaidu-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant_download.none-task-blog-blogcommendfrombaidu-1.nonecas![image-20201017003241394](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201017003242.png)</a></p></li><li><p>!                              <a href="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201017003854.png" target="_blank" rel="noopener">image-20201017003852920</a><a href="https://wenku.baidu.com/view/725f75bf1a37f111f1855b3c.html#![image-20201017003821332](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201017003822.png)" target="_blank" rel="noopener">https://wenku.baidu.com/view/725f75bf1a37f111f1855b3c.html#![image-20201017003821332](https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201017003822.png)</a></p></li></ol><p><strong>1</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;WaveNet小结&quot;&gt;&lt;a href=&quot;#WaveNet小结&quot; class=&quot;headerlink&quot; title=&quot;WaveNet小结&quot;&gt;&lt;/a&gt;WaveNet小结&lt;/h2&gt;
    
    </summary>
    
    
      <category term="语音" scheme="http://yoursite.com/categories/%E8%AF%AD%E9%9F%B3/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="语音合成" scheme="http://yoursite.com/tags/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90/"/>
    
  </entry>
  
  <entry>
    <title>《ONE-SHOT VOICE CONVERSION BY VECTOR QUANTIZATION》</title>
    <link href="http://yoursite.com/2020/10/18/10.18%E7%BB%84%E4%BC%9A-VQVC%20%E7%AE%80%E6%9E%90/"/>
    <id>http://yoursite.com/2020/10/18/10.18%E7%BB%84%E4%BC%9A-VQVC%20%E7%AE%80%E6%9E%90/</id>
    <published>2020-10-18T07:16:54.000Z</published>
    <updated>2020-11-01T03:45:22.177Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h3 id="1018组会"><a href="#1018组会" class="headerlink" title="1018组会"></a>1018组会</h3><h3 id="VQVC-简析"><a href="#VQVC-简析" class="headerlink" title="VQVC 简析"></a>VQVC 简析</h3><ul><li>《ONE-SHOT VOICE CONVERSION BY VECTOR QUANTIZATION》</li><li><a href="https://ericwudayi.github.io/VQVC-DEMO/" target="_blank" rel="noopener">https://ericwudayi.github.io/VQVC-DEMO/</a></li></ul><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018132533.png" alt="image-20201018132532040"></p><ul><li>《VQVC+: One-Shot Voice Conversion by Vector Quantization and U-Net architecture》</li></ul><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018001018.png" alt="image-20201018001016143"></p><a id="more"></a><hr><h2 id="1-特点"><a href="#1-特点" class="headerlink" title="1. 特点"></a>1. 特点</h2><ul><li>模型结构：AE 类型，U-Net 框架</li><li>常见 VC 特征解纠缠思路：The disentangle capability is achieved by vector quantization (VQ), adversarial training, or instance normalization (IN)<ul><li>VQ</li><li>对抗</li><li>IN</li></ul></li><li>VQVC：对 <strong>隐变量</strong> 进行 <strong>量化</strong></li></ul><hr><h2 id="2-相关"><a href="#2-相关" class="headerlink" title="2. 相关"></a>2. 相关</h2><h3 id="（一）直接转换语音，不需要单独-“特征解纠缠”"><a href="#（一）直接转换语音，不需要单独-“特征解纠缠”" class="headerlink" title="（一）直接转换语音，不需要单独 “特征解纠缠”"></a>（一）直接转换语音，不需要单独 “特征解纠缠”</h3><ol><li>CycleGAN &amp;&amp; StarGAN 专注于解决 <strong>“多对多问题”</strong> </li><li>BLOW：基于Flow的方式：<strong>直接对 波形 进行转换，而不用转换成 语音特征</strong></li></ol><h3 id><a href="#" class="headerlink" title></a></h3><h3 id="（二）-speaker-amp-amp-linguistic-解耦"><a href="#（二）-speaker-amp-amp-linguistic-解耦" class="headerlink" title="（二） speaker &amp;&amp; linguistic 解耦"></a>（二） speaker &amp;&amp; linguistic 解耦</h3><ol><li>AutoVC—— AE 结构，通过控制 <strong>隐变量</strong> 尺寸（layer dimension）来实现解纠缠</li><li>IN 法<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201101114502.png" alt="image-20201018105402931"></li><li>VQVC</li></ol><hr><h2 id="3-加入-U-Net-的原因："><a href="#3-加入-U-Net-的原因：" class="headerlink" title="3. 加入 U-Net 的原因："></a>3. 加入 U-Net 的原因：</h2><ul><li>以上这些 特征解纠缠 的思路可行，但是由于这些额外限制，会使得重构音质受损</li><li>U-Net 的重构能力超级强，所以本身是没法做 VC 的，因为她 解纠缠 能力不好</li><li>本文尝试 结合 U-Net 的强大重构能力，以及 VQ 的 特征解耦能力，使得转换的语音，在相似度提高的同时，解决流畅度问题；</li></ul><hr><h2 id="4-VQVC-思路："><a href="#4-VQVC-思路：" class="headerlink" title="4. VQVC 思路："></a>4. VQVC 思路：</h2><ul><li>内容：离散码</li><li>说话人信息：连续语音信号 - 离散码</li></ul><ol><li>先做 Encode 得到隐变量 Z；</li><li>再对这个隐变量做 量化，（结合 codebook ），提取出 内容向量 C（content embedding）——不同说话人，说同样一句话，得到的向量趋于相似向量值；</li><li>将 Z 和 C 做量化减法，得到 Speaker Embedding——S</li></ol><p>内容：离散码<br>说话人信息：连续语音信号-离散码</p><ol start="4"><li></li></ol><hr><ul><li>Codebook 本身是可训练的</li><li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018152001.png" alt="image-20201018152000315"></li><li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018152107.png" alt="image-20201018152105943"></li></ul><p>公式（1）：</p><ul><li><p>x（带帽）表示的是 语音 x 中的一个小片段；</p></li><li><p><strong>argmin</strong> 函数：求取 使得函数 y = f(x) 取得最小值的 x 值集合</p></li><li><p>将 Encode 之后的 隐变量 z，拿来和 Codebook 中的向量 求取差值最小项（欧式距离）</p></li><li><p>最后的 $C_x$ 是将上述 x^ 做一个整体的 concate 得到的</p></li></ul><p>公式（2）：</p><p>结合（VQVC+）的部分公式 来看：<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018152951.png" alt="image-20201018152950185"></p><ul><li>$E_t$ 表示的是，在所有 T 帧上，求取 Z 和 C 的<strong>期望值 s</strong>，最后将 s 重复 T 次，以此表征 speaker embedding；</li><li>重复 T 次是为了让 s 的尺寸 和 C 一样，并方便后续操作</li></ul><p>公式（3）：</p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018153359.png" alt="image-20201018153358420"></p><ul><li>第一个损失： <strong>重构损失</strong> </li></ul><p>公式（4）、（5）：</p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018153716.png" alt="image-20201018153714828"></p><ul><li>第二个损失：隐变量损失 L(latent)</li><li>目的是更好地从 隐变量Z 中提取 Content Embedding，来代表整个语音片段的内容部分</li><li>🌟值得一提的是：为了让 C 更好地表征内容，在这个环节中，Codebook不做更新：防止网络为了缩小 Loss，而把 Codebook 往 说话人信息方向靠，使得提取出来的 Content Embedding 含有 Spealer 信息；</li></ul><p>公式（5）：</p><ul><li>两个 Loss 的叠加</li></ul><hr><h2 id="关于-IN："><a href="#关于-IN：" class="headerlink" title="关于 IN："></a>关于 IN：</h2><ul><li>本图来自（VQVC+）：</li></ul><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018154453.png" alt="image-20201018154451621"></p><ul><li>在 Encode 得到隐变量 z 之后，在进一步提取 content embedding 之前，需要先做一次 IN（Instant Norm）</li><li>IN 本身也被其他论文证明，具有</li><li>特征解耦 的功效（这里 IN 是必须操作）</li><li>这样能预先一步过滤 说话人信息</li></ul><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018160224.png" alt="image-20201018160153595"></p><hr><h2 id="一些特征处理细节："><a href="#一些特征处理细节：" class="headerlink" title="一些特征处理细节："></a>一些特征处理细节：</h2><ul><li>n_mels = 160</li><li>GriffinLim</li><li>24K</li><li>一些额外的规范化normalize 处理：<ul><li>减去均值，除以均方差，处理到【0，1】</li></ul></li></ul><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018155430.png" alt="image-20201018155429424"></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018155439.png" alt="image-20201018155438508"></p><hr><h2 id="结果："><a href="#结果：" class="headerlink" title="结果："></a>结果：</h2><ol><li>消融实验</li></ol><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018162111.png" alt="image-20201018162109398"></p><ol start="2"><li>ABX 实验<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018162140.png" alt="image-20201018162138602"></li></ol><ol start="3"><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018162224.png" alt="image-20201018162222357"></p></li><li><p>Speaker Embedding 区分度：语音片段长度120帧；CodeBook 尺寸 32；训练使用数据 20 人<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018162242.png" alt="image-20201018162241525"></p></li></ol><h2 id="疑问点："><a href="#疑问点：" class="headerlink" title="疑问点："></a>疑问点：</h2><ul><li><p>量化部分的具体操作看不懂，说的不详细</p></li><li><p>没有代码对照</p></li></ul><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018155903.png" alt="image-20201018155901510"></p><hr><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><ul><li>创新意义 &gt; 实用意义</li><li>没脱离 AE 结构，还是在隐变量 特征解耦上做文章，效果不突出；</li><li>Demo 听起来，语气语调信息损失严重</li><li>当作已有方法总结中的一种，不建议使用</li></ul><hr><ul><li>开题报告</li><li>PPT</li><li></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1018组会&quot;&gt;&lt;a href=&quot;#1018组会&quot; class=&quot;headerlink&quot; title=&quot;1018组会&quot;&gt;&lt;/a&gt;1018组会&lt;/h3&gt;&lt;h3 id=&quot;VQVC-简析&quot;&gt;&lt;a href=&quot;#VQVC-简析&quot; class=&quot;headerlink&quot; title=&quot;VQVC 简析&quot;&gt;&lt;/a&gt;VQVC 简析&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;《ONE-SHOT VOICE CONVERSION BY VECTOR QUANTIZATION》&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://ericwudayi.github.io/VQVC-DEMO/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://ericwudayi.github.io/VQVC-DEMO/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018132533.png&quot; alt=&quot;image-20201018132532040&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;《VQVC+: One-Shot Voice Conversion by Vector Quantization and U-Net architecture》&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20201018001018.png&quot; alt=&quot;image-20201018001016143&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="VC" scheme="http://yoursite.com/tags/VC/"/>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>《EMOTIONAL VOICE CONVERSION USING MULTITASK LEARNING WITH TEXT-TO-SPEECH》</title>
    <link href="http://yoursite.com/2020/09/20/0920-EMOTIONAL%20VOICE%20CONVERSION%20USING%20MULTITASK%20LEARNING%20WITH%20TEXT-TO-SPEECH/"/>
    <id>http://yoursite.com/2020/09/20/0920-EMOTIONAL%20VOICE%20CONVERSION%20USING%20MULTITASK%20LEARNING%20WITH%20TEXT-TO-SPEECH/</id>
    <published>2020-09-20T12:20:58.000Z</published>
    <updated>2020-09-20T16:37:29.594Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>0920-论文总结</p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200921003657.png" alt="image-20200916195211586"></p><p>2020.09.20</p><a id="more"></a><h2 id="本周完成的工作："><a href="#本周完成的工作：" class="headerlink" title="本周完成的工作："></a>本周完成的工作：</h2><p>（写了比较多的<strong>验证实验代码</strong>，助于理解原理）</p><ol><li><p>学习掌握了 <strong>LMDB</strong> 格式数据的处理（创建/插入/读取/修改）</p></li><li><p>实现了 <strong>Numpy 类型数据转 lmdb</strong>（<strong>librosa</strong>提取出来的<strong>mel</strong>数据需要处理成 <strong>连续存储</strong> <strong>np.ascontiguousarray（）</strong>）</p></li><li><p>从 lmdb 读取数据，并转换成 numpy（<strong>np.fromstring(value, dtype=np.float32)</strong>），并完整复原成语音。</p></li><li><p><strong>Mel —&gt;(griffin)—&gt;wav</strong>：对比试验了几个版本的 tacotron 的语音数据处理代码， 结合网上资料，总结一套转换效果质量较好的 代码：（mag -&gt; mel ; <strong>mel -&gt; mag</strong>; mag -&gt;wave）【很多资料版本在借用<strong>griffin</strong>实现 <strong>mel 转 幅度谱 mag</strong> 环节，写的不够好甚至没写清楚】</p><hr></li><li><p>数据处理环节：NVAE 中的 图像处理是【n，n】，所以采用 【<strong>256帧，n_mels=256</strong>】的参数来提取 <strong>mel</strong>（<strong>80 维的griffin复原效果太差</strong></p><hr></li><li><p>🌟<strong>NVAE图像**</strong>训爆了<strong>：和作者联系，问题定位在 batch太小（原32，咱们用 4【GPU限制】）情况下，</strong>learning_rate太大：1e-2 改 1e-3</p></li><li><p>在epoch <strong>5</strong> 掉链子：<strong>warm_up</strong>环节刚过，学习率有变，所以导致数据算成了 <strong>NAN</strong>；模型保存也只保存到 <strong>epoch 1</strong>，<strong>改部分代码，先 一个epoch一个epoch保存 ckpt</strong></p></li></ol><hr><h2 id="可改进："><a href="#可改进：" class="headerlink" title="可改进："></a>可改进：</h2><ol><li>晚上先试着跑起来</li><li>后面尝试改进 NVAE 代码成 <strong>自适应 数据尺寸【m，n】（m != n）</strong></li><li>在tensorboard上看下怎么展示中间步骤语音 .wav</li></ol><hr><hr><h2 id="论文涉猎"><a href="#论文涉猎" class="headerlink" title="论文涉猎"></a>论文涉猎</h2><h2 id="情感语音转换（TTS-VC-多任务学习）"><a href="#情感语音转换（TTS-VC-多任务学习）" class="headerlink" title="情感语音转换（TTS + VC 多任务学习）"></a><strong>情感语音转换</strong>（TTS + VC 多任务学习）</h2><ul><li><h3 id="VC领域-痛点：保存语言信息，情感信息-和-多对多VC方面，VC的性能仍然很差"><a href="#VC领域-痛点：保存语言信息，情感信息-和-多对多VC方面，VC的性能仍然很差" class="headerlink" title="VC领域 痛点：保存语言信息，情感信息 和 多对多VC方面，VC的性能仍然很差"></a>VC领域 <strong>痛点</strong>：<strong>保存语言信息</strong>，<strong>情感信息</strong> 和 <strong>多对多VC</strong>方面，VC的性能仍然很差</h3></li><li><h3 id="解决的问题：在-2017-年一篇-“情感VC转换”-基础上，提升“转换后内容保留程度”（即，降低WER）（retaining-linguistic-contents）"><a href="#解决的问题：在-2017-年一篇-“情感VC转换”-基础上，提升“转换后内容保留程度”（即，降低WER）（retaining-linguistic-contents）" class="headerlink" title="解决的问题：在 2017 年一篇 “情感VC转换” 基础上，提升“转换后内容保留程度”（即，降低WER）（retaining linguistic contents）"></a><strong>解决的问题</strong>：在 2017 年一篇 “情感VC转换” 基础上，<strong>提升“转换后内容保留程度”</strong>（即，降低WER）（retaining linguistic contents）</h3></li></ul><hr><ul><li>有<strong>提供源码</strong>，缺 demo 展示（文件夹下载需代理，网速极其慢， 300+m 大小 /  3kb/s）</li></ul><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200916195217.png" alt="image-20200916195211586"></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200919174026.png" alt="image-20200919174025442"></p><hr><ul><li>最新的 VC 思路是 序列到序列（Sequence 2 Seq），但是容易丢失语音信息<ul><li>可以通过文本监督来矫正：<ul><li>但是对齐是个问题；</li><li>另外这样也失去了 S2S 的优势了</li></ul></li></ul></li><li>本文思路：<ul><li>利用 <strong>多任务学习的 TTS 模型</strong>，来帮助 VC 模型 <strong>捕获语言信息</strong>并<strong>保持训练稳定性</strong>。</li><li>TTS 框架来源 tacotron（有局部的稍微改动）：（Style Encoder 也是借鉴这篇）<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200921003719.png" alt="image-20200919190909558"></li><li>VC 框架：另外并联一个 “Content Encoder”</li></ul></li></ul><hr><h2 id="要点："><a href="#要点：" class="headerlink" title="要点："></a>要点：</h2><ul><li><strong>并不像</strong>传统做 <strong>情感转换</strong>那样，在训练阶段就提取 情感标签（one-hot形式）；</li><li>在整个网络中，也不会将 <strong>情感标签</strong> 当作一个条件作为输入 （<strong>联想一下之前的 pitch 标签</strong>）</li><li></li><li>可以在<strong>单个模型中</strong>执行<strong>VC</strong>和<strong>TTS</strong></li></ul><hr><h2 id="网络结构："><a href="#网络结构：" class="headerlink" title="网络结构："></a>网络结构：</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200919173504.png" alt="image-20200919173502738"></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200919185336.png" alt="image-20200919185334992"></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200919204207.png" alt="image-20200919204205843"></p><hr><h2 id="TTS-支线：（以-tacotron-为原型）"><a href="#TTS-支线：（以-tacotron-为原型）" class="headerlink" title="TTS 支线：（以 tacotron 为原型）"></a>TTS 支线：（以 tacotron 为原型）</h2><ol><li><strong>模仿的是</strong>：</li></ol><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200919190911.png" alt="image-20200919190909558"></p><ol start="2"><li><strong>框架</strong>：</li></ol><ul><li>text encoder, </li><li>decoder,</li><li>attention, </li><li>and post processor</li></ul><ol start="3"><li><strong>改动：(参考 [17] 文献)</strong><ol><li>文字向量<strong>context vector $C_t$</strong> 被用在 <strong>AttentionRNN</strong> 的每个循环内（context vector c (t)utilizes is used for every iteration in attention RNN）[原本是怎么样的？查一下]</li><li>在 <strong>CBHG</strong> (Convolution Bank + Highway + bi-GRU) 模块中，增加了 *<em>残余连接 (residual connection) *</em>模块</li></ol></li></ol><hr><hr><h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200919194706.png" alt="image-20200919194704841"></p><ul><li>Loss 就是直接比较： <strong>mel谱</strong> 差距 &amp;&amp; <strong>线性谱</strong> 差距</li></ul><hr><h2 id="实验参数："><a href="#实验参数：" class="headerlink" title="实验参数："></a>实验参数：</h2><ol><li>大体上都是和 taco 部分的语音预处理手法相似</li><li>数据集：<ol><li>韩国某个30岁男子，用七种情绪，每种情绪说 3k 句；共 2.1w 句；</li><li>其中情感：（neutral, happiness, sadness, anger, fear, surprise, and disgust)</li><li>去除静音之后，共约 29.2 h</li></ol></li><li>🌟值得一提的几点：<ol><li>去除静音，不是像taco那样，用 <strong>librosa.effect.trim()</strong> ，而是用voice activity detection algorithm （VAD 算法：开源）</li><li>在做 TTS-taco-like 部分里，<strong>字符处理</strong>有特点：【在转换为 <strong>one-hot embedding</strong> 这种表示形式之前会分解为 <strong>开始</strong>，<strong>核心</strong> 和 <strong>尾声</strong>（<strong>onset, nucleus, and coda</strong>）】</li><li>256 character embedding, 32 dimensions for $h_c$</li></ol></li></ol><hr><h2 id="🌟重要的一个验证实验"><a href="#🌟重要的一个验证实验" class="headerlink" title="🌟重要的一个验证实验"></a>🌟重要的一个验证实验</h2><p>——（<strong>内容一致性</strong>验证 <strong>Linguistic consistency</strong>）</p><ol><li>每种情感 取20条句子</li><li>用 StyleEncoder 提取 “<strong>style vector</strong>”，并用<strong>余弦相似度</strong>来查看<strong>情感分离程度</strong>（验证情感特征提取的有效性）</li></ol><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200920161149.png" alt="image-20200920161147834"></p><ol><li>每种情感 句子 的<strong>内容 $X_s$</strong> 保持不变，选取七种句子（同内容），做“从中性情感”到“其他六种情感”的转换</li><li>结果上看，<strong>log mel</strong> 语谱图 <strong>尺寸形状大差不差</strong></li><li>有些许差异的地方：<ol><li>时间偏移，</li><li>频率偏移，</li><li>暂停持续时间</li></ol></li><li>总体上能实现，<strong>由一种情感，随意 VC 转换到其他情感</strong> 的能力</li></ol><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200920161207.png" alt="image-20200920161206290"></p><hr><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200920145005.png" alt="image-20200920145003900"></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200919205459.png" alt="image-20200919205457583"></p><ol><li>在联合训练的帮助下：<ol><li>VCTTS-VC 在内容保留能力上的效果，比单纯的 VC ，正确率要提高不少</li><li>另一方面，VCTTS-TTS 比单纯的 TTS 没有太大进步，甚至有一点点下降</li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;0920-论文总结&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200921003657.png&quot; alt=&quot;image-20200916195211586&quot;&gt;&lt;/p&gt;
&lt;p&gt;2020.09.20&lt;/p&gt;
    
    </summary>
    
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="test" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/test/"/>
    
    
      <category term="VC" scheme="http://yoursite.com/tags/VC/"/>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>《PITCHNET —— UNSUPERVISED SINGING VOICE CONVERSION WITH PITCH ADVERSARIAL NETWORK》</title>
    <link href="http://yoursite.com/2020/09/07/0906%E7%BB%84%E4%BC%9A%E5%88%86%E4%BA%AB%EF%BC%9A%E6%AD%8C%E5%94%B1%E8%BD%AC%E6%8D%A2/"/>
    <id>http://yoursite.com/2020/09/07/0906%E7%BB%84%E4%BC%9A%E5%88%86%E4%BA%AB%EF%BC%9A%E6%AD%8C%E5%94%B1%E8%BD%AC%E6%8D%A2/</id>
    <published>2020-09-07T07:16:54.000Z</published>
    <updated>2020-09-20T16:35:13.328Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>0906组会分享</p><h2 id="0906组会分享：歌唱转换-singing-voice-conversion"><a href="#0906组会分享：歌唱转换-singing-voice-conversion" class="headerlink" title="0906组会分享：歌唱转换 singing voice conversion"></a>0906组会分享：<strong>歌唱转换 singing voice conversion</strong></h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200905170752.png" alt="image-20200905170750870"></p><ul><li><p>效果概览：<a href="https://tencent-ailab.github.io/pitch-net/" target="_blank" rel="noopener">https://tencent-ailab.github.io/pitch-net/</a></p></li><li><p>MOS评分：</p><ul><li>Baseline ：2.92</li><li>PitchNet：3.75</li></ul></li></ul><a id="more"></a><ul><li><h2 id="论文的-baseline-对象："><a href="#论文的-baseline-对象：" class="headerlink" title="论文的 baseline 对象："></a>论文的 <strong>baseline</strong> 对象：<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200905171530.png" alt="image-20200905171528462"></h2></li><li><h2 id="BaseLine-的贡献和缺点"><a href="#BaseLine-的贡献和缺点" class="headerlink" title="BaseLine 的贡献和缺点"></a><strong>BaseLine 的贡献和缺点</strong></h2></li></ul><p>贡献：</p><ol><li>利用 AE 模式下的无监督方式，解决了 平行语料的问题</li><li>组成模块：<strong>WaveNetlike Encoder</strong> &amp;&amp;  <strong>WaveNet  autoregressive Decoder</strong> &amp;&amp; <strong>Speaker-Embedding Table</strong>（leanable）</li></ol><p>缺点：</p><ol><li>和StarGan-VC之类的 单纯说话VC 任务不同，歌唱转换的效果重点是：说话人相似性 &amp;&amp; 音调合理性</li><li>BaseLine 在音调处理上差强人意，总而言之，“<strong>语音和音调的联合表示</strong>”是难点</li></ol><hr><ul><li><h2 id="解决的问题："><a href="#解决的问题：" class="headerlink" title="解决的问题："></a><strong>解决的问题</strong>：</h2></li></ul><ol><li>走调（音高 Pitch 失控现象）问题的缓解<ol><li>在 Baseline 的 <strong>AE无监督</strong> 模式下，附带一个额外的<strong>音调回归网络（GAN）</strong>可以将音调信息从潜在空间中分离出来</li><li><strong>目的是</strong>：让原本的<strong>Encoder</strong>在单纯学习 <strong>说话人无关的语言信息</strong> 基础上，再额外剥离掉 <strong>音高信息</strong> （<strong>not only singer-invariant but also pitch-invariant representation</strong>）</li></ol></li><li>至于音高信息，当成一个独立问题来解决：设立独立模块来提取 <strong>source</strong> 的<strong>音高信息</strong>，以此配合 <strong>Decoder</strong> ，来<strong>操控</strong> <strong>生成语音</strong> 的<strong>音高</strong> <ol><li>（根据作者相关 Git 项目下的 issue 讨论，这部分工作借用 Kaldi 来完成）</li><li>【实现 <strong>音高可控性</strong> 】</li></ol></li></ol><hr><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a><strong>模型</strong></h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200905223846.png" alt="image-20200905223845590"></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200905223859.png" alt="image-20200905223857883"></p><ul><li><strong>Singer Classiﬁcation Network</strong> <strong>Pitch Regression Network</strong> 是为了使得 <strong>Encoder</strong> 能够在抽取特征向量 <strong>z</strong> 时，把 说话人信息 &amp;&amp; 音调信息 都剥离掉</li><li></li><li></li></ul><hr><h2 id="Training-Loss：三个Loss"><a href="#Training-Loss：三个Loss" class="headerlink" title="Training Loss：三个Loss"></a><strong>Training Loss：三个Loss</strong></h2><ol><li><strong>singer classiﬁcation loss</strong></li><li><strong>pitch regression loss</strong></li><li><strong>reconstruction loss</strong></li></ol><h2 id="几个公式的清晰理解："><a href="#几个公式的清晰理解：" class="headerlink" title="几个公式的清晰理解："></a>几个公式的清晰理解：</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200906141104.png" alt="image-20200906141103220"></p><ol><li>说明了AE网络的Decoder产生结果 所需要的输入元素（Encoder结果+Kaldi抽取的音高+Embedding信息）</li><li>介绍重构损失：在Decoder时配合<strong>source</strong>的embedding，和<strong>source</strong>语音做<strong>Loss</strong></li></ol><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200906141139.png" alt="image-20200906141138234"></p><ol start="3"><li>一个 <strong>说话人分类器</strong>，同理为 StarGan-VC 的Domain-Classification，分类对象，是<strong>Encoder</strong>结果&amp;&amp;<strong>Embedding</strong></li><li>另一个<strong>音高分类器</strong>，Pitch信息由 Kaldi 提取</li><li>总的<strong>Loss</strong>结构：因为重构损失⬇️越好，另一方面，对于两个分类器，我们希望我们的 <strong>Encoder</strong>提取出来的隐变量 能彻底剥离 说话人信息 &amp;&amp; Pitch信息，所以这两个的误差，对于结果来说，应该越大越好。（所以这两个部分是配合 负号）</li><li>上述两个 C分类器的损失，对于<strong>Encoder</strong>和<strong>Classifier</strong>来说，是完全相反的，【<strong>就是典型的GAN对抗思路</strong>】，所以他们的训练，采取“你一次我一次的过程”<ul><li>训练中，缩小 $$L_{total}$$ 时，促进的是 <strong>Encoder</strong> 将两个元素剥离</li><li>缩小 $L_{ad}$ 时，促进的是 <strong>Classifier</strong> 具有更强的分类能力，能看透隐变量 <strong>z</strong> 的实质和归属</li></ul></li></ol><p>Ps.整体思路比较清晰，就是<strong>自回归</strong>那个部分不知道是怎么具体实现的</p><hr><p>Quantitative and Qualitative Experiments**【定量 和 定性 实验】</p><hr><p>Tencent :LPCNet, </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python convert_tfrecord_to_lmdb.py --dataset&#x3D;celeba --tfr_path&#x3D;&#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;DATA_DIR&#x2F;celeba&#x2F;celeba-tfr --lmdb_path&#x3D;&#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;DATA_DIR&#x2F;celeba&#x2F;celeba-lmdb --split&#x3D;train</span><br><span class="line">python convert_tfrecord_to_lmdb.py --dataset&#x3D;celeba --tfr_path&#x3D;&#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;DATA_DIR&#x2F;celeba&#x2F;celeba-tfr --lmdb_path&#x3D;&#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;DATA_DIR&#x2F;celeba&#x2F;celeba-lmdb --split&#x3D;validation</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">export EXPR_ID&#x3D;&#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;EXPR_ID</span><br><span class="line">export DATA_DIR&#x3D;&#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;DATA_DIR&#x2F;celeba&#x2F;celeba-lmdb</span><br><span class="line">export CHECKPOINT_DIR&#x3D;&#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;CHECKPOINT_DIR</span><br><span class="line">export CODE_DIR&#x3D;&#x2F;data&#x2F;hsj&#x2F;NVAE</span><br><span class="line">cd &#x2F;data&#x2F;hsj&#x2F;NVAE</span><br><span class="line">CUDA_VISIBLE_DEVICES&#x3D;1  </span><br><span class="line">python train.py --data &#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;DATA_DIR&#x2F;celeba&#x2F;celeba-lmdb --root &#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;CHECKPOINT_DIR --save &#x2F;data&#x2F;hsj&#x2F;NVAE&#x2F;EXPR_ID --dataset celeba_64 \</span><br><span class="line">        --num_channels_enc 64 --num_channels_dec 64 --epochs 90 --num_postprocess_cells 2 --num_preprocess_cells 2 \</span><br><span class="line">        --num_latent_scales 3 --num_latent_per_group 20 --num_cell_per_cond_enc 2 --num_cell_per_cond_dec 2 \</span><br><span class="line">        --num_preprocess_blocks 1 --num_postprocess_blocks 1 --weight_decay_norm 1e-1 --num_groups_per_scale 20 \</span><br><span class="line">        --batch_size 16 --num_nf 1 --ada_groups --num_process_per_node 8 --use_se --res_dist --fast_adamax</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CelebA64 数据预处理</span><br><span class="line"></span><br><span class="line">cd $CODE_DIR&#x2F;scripts</span><br><span class="line">python create_celeba64_lmdb.py --split train --img_path .&#x2F;DATA_DIR&#x2F;celeba_org&#x2F;celeba --lmdb_path  .&#x2F;DATA_DIR&#x2F;celeba64_lmdb</span><br><span class="line">python create_celeba64_lmdb.py --split valid --img_path .&#x2F;DATA_DIR&#x2F;celeba_org&#x2F;celeba --lmdb_path  .&#x2F;DATA_DIR&#x2F;celeba64_lmdb</span><br><span class="line">python create_celeba64_lmdb.py --split test  --img_path .&#x2F;DATA_DIR&#x2F;celeba_org&#x2F;celeba --lmdb_path  .&#x2F;DATA_DIR&#x2F;celeba64_lmdb</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python create_celeba64_lmdb.py --split train --img_path ..&#x2F;DATA_DIR&#x2F;celeba_org&#x2F;celeba --lmdb_path ..&#x2F;DATA_DIR&#x2F;celeba64_lmdb</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python convert_tfrecord_to_lmdb.py --dataset&#x3D;celeba --split&#x3D;validation</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python train.py --data $DATA_DIR&#x2F;celeba64_lmdb --root $CHECKPOINT_DIR --save $EXPR_ID --dataset celeba_64 \</span><br><span class="line">        --num_channels_enc 64 --num_channels_dec 64 --epochs 90 --num_postprocess_cells 2 --num_preprocess_cells 2 \</span><br><span class="line">        --num_latent_scales 3 --num_latent_per_group 20 --num_cell_per_cond_enc 2 --num_cell_per_cond_dec 2 \</span><br><span class="line">        --num_preprocess_blocks 1 --num_postprocess_blocks 1 --weight_decay_norm 1e-1 --num_groups_per_scale 20 \</span><br><span class="line">        --batch_size 16 --num_nf 1 --ada_groups --num_process_per_node 8 --use_se --res_dist --fast_adamax</span><br></pre></td></tr></table></figure><p>/Users/huangshengjie/Desktop/NVAE/scripts/data1/datasets/imagenet-oord</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;0906组会分享&lt;/p&gt;
&lt;h2 id=&quot;0906组会分享：歌唱转换-singing-voice-conversion&quot;&gt;&lt;a href=&quot;#0906组会分享：歌唱转换-singing-voice-conversion&quot; class=&quot;headerlink&quot; title=&quot;0906组会分享：歌唱转换 singing voice conversion&quot;&gt;&lt;/a&gt;0906组会分享：&lt;strong&gt;歌唱转换 singing voice conversion&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200905170752.png&quot; alt=&quot;image-20200905170750870&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;效果概览：&lt;a href=&quot;https://tencent-ailab.github.io/pitch-net/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://tencent-ailab.github.io/pitch-net/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;MOS评分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Baseline ：2.92&lt;/li&gt;
&lt;li&gt;PitchNet：3.75&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="VC" scheme="http://yoursite.com/tags/VC/"/>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>0830组会分享——几种卷积辨析 &amp;&amp; SELayer论文及代码实现</title>
    <link href="http://yoursite.com/2020/08/31/0830%E7%BB%84%E4%BC%9A%E5%88%86%E4%BA%AB/"/>
    <id>http://yoursite.com/2020/08/31/0830%E7%BB%84%E4%BC%9A%E5%88%86%E4%BA%AB/</id>
    <published>2020-08-31T11:20:58.000Z</published>
    <updated>2020-09-20T16:40:48.382Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="几种卷积类型辨析"><a href="#几种卷积类型辨析" class="headerlink" title="几种卷积类型辨析"></a>几种卷积类型辨析</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conv = nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">1</span>, groups=<span class="number">3</span>)</span><br><span class="line">conv.weight.data.size()</span><br><span class="line"></span><br><span class="line"><span class="comment"># output = torch.Size([6, 2, 1, 1])</span></span><br></pre></td></tr></table></figure><p>一种分类方法：</p><p>几种卷积示意：（分组卷积 <strong>group_convolution</strong>；深度卷积 <strong>depthwise convolution</strong>； 全局深度卷积 <strong>global depthwise convolution</strong>）</p><ol><li>groups 默认值为1， 对应的是<strong>常规卷积</strong>操作</li><li>groups &gt; 1， 且能够同时被in_channel / out_channel整除，对应<strong>group_convolution</strong></li><li>groups == input_channel == out_channel , 对应<strong>depthwise convolution</strong>,为条件2的特殊情况</li><li>在条件3的基础上，各卷积核的 H == input_height; W == input_width, 对应为 <strong>global depthwise convolution</strong>, 为条件3的特殊情况</li><li></li></ol><hr><p>另一种分类方法：<strong>主要分三类：正常卷积、分组卷积、深度分离卷积</strong></p><a id="more"></a><!--more--><ol><li><h2 id="正常卷积："><a href="#正常卷积：" class="headerlink" title="正常卷积："></a><strong>正常卷积：</strong></h2></li><li><p>参数量 = cin *  $K_h$ * $K_w$ * cout</p></li><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830111253.png" alt="常规卷积示意图"></p></li><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830103246.jpeg" alt="img"></p></li><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830103436.png" alt="img"></p></li><li></li><li><h2 id="分组卷积示意图："><a href="#分组卷积示意图：" class="headerlink" title="分组卷积示意图："></a><strong>分组卷积示意图：</strong></h2></li><li><p>参数量： (cin * $K_h$ * $K_w$* cout ) / Groups</p></li><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830111236.png" alt="分组卷积示意图"></p></li><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830103353.jpeg" alt="img"></p></li><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830103314.png" alt="img"></p></li><li><pre><code class="python"><span class="class"><span class="keyword">class</span> <span class="title">GroupConv</span><span class="params">(nn.Module)</span>:</span>  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_ch, out_ch, groups)</span>:</span>      super(GroupConv, self).__init__()      self.conv = nn.Conv2d(          in_channels=in_ch,          out_channels=out_ch,          kernel_size=<span class="number">3</span>,          stride=<span class="number">1</span>,          padding=<span class="number">1</span>,          groups=groups      )  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span>      out = self.conv(input)      <span class="keyword">return</span> out</code></pre></li></ol><pre><code># 测试conv = CSDN_Tem(16, 32, 4)print(summary(conv, (16, 64, 64), batch_size=1))********************************----------------------------------------------------------------        Layer (type)               Output Shape         Param #================================================================            Conv2d-1            [1, 32, 64, 64]           1,184================================================================Total params: 1,184Trainable params: 1,184Non-trainable params: 0----------------------------------------------------------------Input size (MB): 0.25Forward/backward pass size (MB): 1.00Params size (MB): 0.00Estimated Total Size (MB): 1.25----------------------------------------------------------------<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">13. </span><br><span class="line"></span><br><span class="line">14. </span><br><span class="line"></span><br><span class="line">15. ## [深度可分离卷积(Depthwise Separable Convolution)][https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_30793735&#x2F;article&#x2F;details&#x2F;88915612]</span><br><span class="line"></span><br><span class="line">16. 参数量： $C_&#123;in&#125;$ * $K_h$ * $K_w$ + $C_&#123;out&#125;$ * 1 * 1</span><br><span class="line"></span><br><span class="line">17. 理解上，可以看作是， 先做一次 cin &#x3D;&#x3D; cout 的 分组卷积， 并且 groups &#x3D;&#x3D; channels，即每个通道作为一组； 然后再对上述结果，做一次 **逐点卷积(Pointwise Convolution)** 实现通道数改变的，这个过程使用大小为 **Cin * 1 * 1 ** 的卷积核实现，数量为 **Cout** 个</span><br><span class="line"></span><br><span class="line">18. &#96;&#96;&#96;python</span><br><span class="line">    class DepthSepConv(nn.Module):</span><br><span class="line">        def __init__(self, in_ch, out_ch):</span><br><span class="line">            super(DepthSepConv, self).__init__()</span><br><span class="line">            self.depth_conv &#x3D; nn.Conv2d(</span><br><span class="line">                in_channels&#x3D;in_ch,</span><br><span class="line">                out_channels&#x3D;in_ch,</span><br><span class="line">                kernel_size&#x3D;3,</span><br><span class="line">                stride&#x3D;1,</span><br><span class="line">                padding&#x3D;1,</span><br><span class="line">                groups&#x3D;in_ch</span><br><span class="line">            )</span><br><span class="line">            self.point_conv &#x3D; nn.Conv2d(</span><br><span class="line">                in_channels&#x3D;in_ch,</span><br><span class="line">                out_channels&#x3D;out_ch,</span><br><span class="line">                kernel_size&#x3D;1,</span><br><span class="line">                stride&#x3D;1,</span><br><span class="line">                padding&#x3D;0,</span><br><span class="line">                groups&#x3D;1</span><br><span class="line">            )</span><br><span class="line">    </span><br><span class="line">        def forward(self, input):</span><br><span class="line">            out &#x3D; self.depth_conv(input)</span><br><span class="line">            out &#x3D; self.point_conv(out)</span><br><span class="line">            return out</span><br><span class="line">    </span><br><span class="line">          </span><br><span class="line">    # 测试</span><br><span class="line">    conv &#x3D; DepthSepConv(16, 32)</span><br><span class="line">    print(summary(conv, (16, 64, 64), batch_size&#x3D;1))</span><br><span class="line">    </span><br><span class="line">    ************************</span><br><span class="line">    ----------------------------------------------------------------</span><br><span class="line">            Layer (type)               Output Shape         Param #</span><br><span class="line">    &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">                Conv2d-1            [1, 16, 64, 64]             160</span><br><span class="line">                Conv2d-2            [1, 32, 64, 64]             544</span><br><span class="line">    &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">    Total params: 704</span><br><span class="line">    Trainable params: 704</span><br><span class="line">    Non-trainable params: 0</span><br><span class="line">    ----------------------------------------------------------------</span><br></pre></td></tr></table></figure></code></pre><ol start="19"><li></li></ol><hr><h1 id="SELayer"><a href="#SELayer" class="headerlink" title="SELayer"></a>SELayer</h1><p><strong>Squeeze-and-Excitation Networks</strong></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830153344.png" alt="image-20200830114911949"></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830153339.png" alt="image-20200830140039015"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">2</span>, <span class="number">512</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">b = <span class="number">2</span> c = <span class="number">512</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">512</span>])</span><br><span class="line">After LinearFC, tmp.shape = torch.Size([<span class="number">2</span>, <span class="number">32</span>])</span><br><span class="line">After RELU, tmp.shape = torch.Size([<span class="number">2</span>, <span class="number">32</span>])</span><br><span class="line">After LinearFC2, tmp.shape = torch.Size([<span class="number">2</span>, <span class="number">512</span>])</span><br><span class="line">After SIGMOID, tmp.shape = torch.Size([<span class="number">2</span>, <span class="number">512</span>])</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">512</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">        Layer (type)               Output Shape         Param <span class="comment">#</span></span><br><span class="line">================================================================</span><br><span class="line"> AdaptiveAvgPool2d<span class="number">-1</span>            [<span class="number">-1</span>, <span class="number">512</span>, <span class="number">1</span>, <span class="number">1</span>]               <span class="number">0</span></span><br><span class="line">            Linear<span class="number">-2</span>                   [<span class="number">-1</span>, <span class="number">32</span>]          <span class="number">16</span>,<span class="number">384</span></span><br><span class="line">              ReLU<span class="number">-3</span>                   [<span class="number">-1</span>, <span class="number">32</span>]               <span class="number">0</span></span><br><span class="line">            Linear<span class="number">-4</span>                  [<span class="number">-1</span>, <span class="number">512</span>]          <span class="number">16</span>,<span class="number">384</span></span><br><span class="line">           Sigmoid<span class="number">-5</span>                  [<span class="number">-1</span>, <span class="number">512</span>]               <span class="number">0</span></span><br><span class="line">            Linear<span class="number">-6</span>                   [<span class="number">-1</span>, <span class="number">32</span>]          <span class="number">16</span>,<span class="number">384</span></span><br><span class="line">              ReLU<span class="number">-7</span>                   [<span class="number">-1</span>, <span class="number">32</span>]               <span class="number">0</span></span><br><span class="line">            Linear<span class="number">-8</span>                  [<span class="number">-1</span>, <span class="number">512</span>]          <span class="number">16</span>,<span class="number">384</span></span><br><span class="line">           Sigmoid<span class="number">-9</span>                  [<span class="number">-1</span>, <span class="number">512</span>]               <span class="number">0</span></span><br><span class="line">================================================================</span><br><span class="line">Total params: <span class="number">65</span>,<span class="number">536</span></span><br><span class="line">Trainable params: <span class="number">65</span>,<span class="number">536</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">Input size (MB): <span class="number">8.00</span></span><br><span class="line">Forward/backward <span class="keyword">pass</span> size (MB): <span class="number">0.02</span></span><br><span class="line">Params size (MB): <span class="number">0.25</span></span><br><span class="line">Estimated Total Size (MB): <span class="number">8.27</span></span><br><span class="line">----------------------------------------------------------------</span><br></pre></td></tr></table></figure><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200830153225.png" alt="image-20200830153224613"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;几种卷积类型辨析&quot;&gt;&lt;a href=&quot;#几种卷积类型辨析&quot; class=&quot;headerlink&quot; title=&quot;几种卷积类型辨析&quot;&gt;&lt;/a&gt;几种卷积类型辨析&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;conv = nn.Conv2d(in_channels=&lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;, out_channels=&lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;, kernel_size=&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, groups=&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conv.weight.data.size()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# output = torch.Size([6, 2, 1, 1])&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;一种分类方法：&lt;/p&gt;
&lt;p&gt;几种卷积示意：（分组卷积 &lt;strong&gt;group_convolution&lt;/strong&gt;；深度卷积 &lt;strong&gt;depthwise convolution&lt;/strong&gt;； 全局深度卷积 &lt;strong&gt;global depthwise convolution&lt;/strong&gt;）&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;groups 默认值为1， 对应的是&lt;strong&gt;常规卷积&lt;/strong&gt;操作&lt;/li&gt;
&lt;li&gt;groups &amp;gt; 1， 且能够同时被in_channel / out_channel整除，对应&lt;strong&gt;group_convolution&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;groups == input_channel == out_channel , 对应&lt;strong&gt;depthwise convolution&lt;/strong&gt;,为条件2的特殊情况&lt;/li&gt;
&lt;li&gt;在条件3的基础上，各卷积核的 H == input_height; W == input_width, 对应为 &lt;strong&gt;global depthwise convolution&lt;/strong&gt;, 为条件3的特殊情况&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;另一种分类方法：&lt;strong&gt;主要分三类：正常卷积、分组卷积、深度分离卷积&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="组会" scheme="http://yoursite.com/categories/%E7%BB%84%E4%BC%9A/"/>
    
    
      <category term="组会" scheme="http://yoursite.com/tags/%E7%BB%84%E4%BC%9A/"/>
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>《END-TO-END ACCENT CONVERSION WITHOUT USING NATIVE UTTERANCES》论文总结</title>
    <link href="http://yoursite.com/2020/08/17/0817-%E7%BB%84%E4%BC%9A%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    <id>http://yoursite.com/2020/08/17/0817-%E7%BB%84%E4%BC%9A%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</id>
    <published>2020-08-17T11:20:58.000Z</published>
    <updated>2020-08-17T11:44:00.124Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200816133725.png" alt="image-20200816133724321"></p><p>展示demo：<a href="https://liusongxiang.github.io/end2endAC/" target="_blank" rel="noopener">https://liusongxiang.github.io/end2endAC/</a></p><a id="more"></a><ul><li>是在 VC 之上，再做的进一步 口音修正：</li><li>source 语音在带有口音（e.g. 咖喱味的英语）的前提下，做VC，但是 我们除了想要 target 的身份，还要 <strong>目标说话人口音</strong>（e.g. 纯正的母语英语）</li><li>所提到的整个网络模型，综合了 TTS 、VC、ASR 的方法，也有一种采众家之长的意思，但是麻烦的是，四个模型部分，需要分别训练</li></ul><hr><h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><ol><li>传统的口音转换：<ol><li>内容（content）、发音（pronounciation） 不变</li><li>把 本地source说话人 的口音（accent），转换成 非本地target说话人 的口音</li></ol></li><li>本文尝试source非母语：北印度人说的英语口音—&gt;北美正统英语口音</li><li>传统的局限：做转换时，需要具备的条件：<ol><li>需要有 印度人说的英语 作为source</li><li>需要有美国人说的英语作为 target</li><li>这导致了，在实际上也用途时，很臃肿难用</li></ol></li><li>本文想法：<ol><li>在转换阶段，提出 端到端 方法，使得不需要 美国人英语target 也能做正确的 AC （accent conversion）</li></ol></li><li>贡献点：<ol><li>说是目前为止第一家实现在 转换阶段 不需要目标（美国target英语语音）就能实现转换的方案</li><li>细化实现了对 <strong>韵律特征</strong> 的建模：（例如说话速度和持续时间）</li><li>non-parallel</li></ol></li></ol><hr><h2 id="网络结构：（四部分）"><a href="#网络结构：（四部分）" class="headerlink" title="网络结构：（四部分）"></a>网络结构：（四部分）</h2><ol><li>a speaker encoder</li><li>a multi-speaker TTS model</li><li>an accented ASR model</li><li>a neural vocoder</li></ol><hr><ol><li>encoder 部分作用：生成speaker-embedding</li><li>TTS 部分 用的是tacotron2（音素 + embedding 预测 mel）</li><li></li></ol><hr><h2 id="Speaker-encoder"><a href="#Speaker-encoder" class="headerlink" title="Speaker encoder"></a>Speaker encoder</h2><ol><li>引用的模型：<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200816222235.png" alt="image-20200816222233997"></li><li>作用是：生成 <strong>speaker-embedding</strong>，用来和 TTS 结合，保证了生成语音 和 target 说话人 身份一致（类似VC和TTS结合？）</li><li>GE2E（generalized end-to-end）speaker veriﬁcation loss</li><li><strong>Baseline训练过程</strong>：</li><li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817111542.png" alt="image-20200817111540876"></li><li><strong>训练阶段训练两个转换模型</strong>：（DTW）<ol><li>Trans 1 : 将 <strong>印度英语</strong> 的ppg 和 <strong>美式英语</strong> 的ppg 利用 DTW 做对齐</li><li>Trans 2 ：将 <strong>印度英语</strong> 的 ppg 和 mel谱 做对应匹配</li><li>转换阶段：<ol><li>输入 <strong>印度英语</strong>到 Trans1</li><li>得到 <strong>美式英语</strong>ppg</li><li>将ppg 继续送入 Trans2，得到对应的 mel谱</li><li>用wave-Net 生成语音波形</li></ol></li></ol></li></ol><hr><h2 id="Multi-speaker-TTS-model-amp-amp-Multi-task-accented-ASR-model"><a href="#Multi-speaker-TTS-model-amp-amp-Multi-task-accented-ASR-model" class="headerlink" title="Multi-speaker TTS model  &amp;&amp;  Multi-task accented ASR model"></a>Multi-speaker TTS model  &amp;&amp;  Multi-task accented ASR model</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817114458.png" alt="image-20200817114456342"></p><ol><li>其中的TTS网络，摘用的这篇：（<strong>attention-based</strong> encoder-decoder model）<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817114716.png" alt="image-20200817114714573"></li><li><strong>text transcripts</strong> into <strong>phoneme sequences</strong>：TTS 中用phone 序列，加速收敛过程；</li></ol><h3 id="口音识别：Multi-task-accented-ASR-model"><a href="#口音识别：Multi-task-accented-ASR-model" class="headerlink" title="口音识别：Multi-task accented ASR model"></a>口音识别：Multi-task accented ASR model</h3><p>训练思路：</p><ol><li>前面有，用单说话人（美式英语）训练好的 <strong>speaker encoder</strong></li><li>接着有 用全部都是（美式英语）训练的 多说话人 <strong>TTS</strong> <strong>模型</strong>（TTS-loss）</li><li>在上面两个步骤基础上，本环节 用多个 <strong>美式英语</strong> &amp;&amp; 一个 <strong>印度英语</strong> 一起进行 <strong>多说话人</strong> 预训练</li><li>完成之后，再 <strong>全部</strong> 换成 <strong>印度口音英语多说话人</strong> 进行调整</li></ol><p>实践中的细节：</p><ol><li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817143548.png" alt="image-20200817143546689">)<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817143604.png" alt="image-20200817143601305"></li><li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817143645.png" alt="image-20200817140608795"></li></ol><p><strong>结构上</strong>：</p><ul><li>E2E attentioned encoder-decoder 结构：如上图（引用以前的文章思路）</li></ul><p><strong>改进 1 ：</strong></p><ul><li>为了增加 <strong>训练稳定性</strong>，参考了文献：<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817144039.png" alt="image-20200817144038371"></li><li>在 Encoder 之前，增加了一个 <strong>全连接</strong>，并增加了一个 <strong>CTC loss</strong></li></ul><p><strong>改进 2 ：</strong></p><ul><li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817144505.png" alt="image-20200817144503756"></li><li>在口音问题的思路上， 借鉴这篇论文：</li><li>在送给 ASR 之前，<strong>每帧</strong> 都concate上一个 <strong>accent embedding</strong>（这是由第一个步骤 <strong>speaker encoder</strong> 里面产生的 <strong>speaker-embedding</strong> 做一个 对单人所有语音的embedding整体均值 的结果）</li><li>并且在ASR的encoder最开始，再增加一个 <strong>accent classifier 口音分类器</strong>（上一步已经加过一次 FC 了）；这也能使面对口音时，有更好的健壮性</li></ul><hr><h2 id="🌟-Loss-环节"><a href="#🌟-Loss-环节" class="headerlink" title="🌟 Loss 环节"></a>🌟 Loss 环节</h2><ul><li><p>λ 1= <strong>0.5</strong>, λ 2= 0.1, λ 3= <strong>0.5</strong> and λ 4= 0.1</p></li><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817145536.png" alt="image-20200817145535742"></p></li><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817145554.png" alt="image-20200817145553156"></p></li></ul><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817145627.png" alt="image-20200817145626236"></p><ol><li><strong>TTSE-loss</strong>：<strong>MSE损失</strong>，对象是：1⃣️上一步已训好的 TTS 产生的声学序列（美式英语） &amp;&amp; ASR识别出的声学序列 做一个 <strong>均方差损失</strong></li><li><strong>CE-loss</strong>：phoneme label prediction （phone音素 的标签预测损失）</li><li><strong>ACC-loss</strong>：口音分类器的损失（类似stargan-vc里面的 speaker-classifier）</li></ol><hr><h2 id="4-后端声码器"><a href="#4-后端声码器" class="headerlink" title="4. 后端声码器"></a>4. 后端声码器</h2><ul><li>本文直接采用开源的 <a href="https://github.com/fatchord/WaveRNN" target="_blank" rel="noopener">WaveRNN</a> </li><li>没有额外加任何的embedding信息，觉得说 mel频谱已经能包含所有需要的声学细节了</li><li>语料：全部采用 <strong>美式英语</strong> 数据来训</li></ul><hr><h2 id="5-转换阶段"><a href="#5-转换阶段" class="headerlink" title="5. 转换阶段"></a>5. 转换阶段</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817153318.png" alt="image-20200817153317898"></p><ol><li>hs：一段语音，经过 speaker-encoder之后得到的 speaker-embedding</li><li>accent-embedding：针对同一个人的所有语音，做所有的 <strong>speaker-embedding</strong>的 <strong>均值</strong>（多个语音理解为多个通道计算）</li><li>（Accent embedding is the averaged speaker embeddings of the non-native-accented speaker.）</li><li>注：accnet-embedding 是 <strong>逐帧都要插入</strong>，speaker-embedding 是最后再concate上</li></ol><hr><h2 id="一些实现参数细节"><a href="#一些实现参数细节" class="headerlink" title="一些实现参数细节"></a>一些实现参数细节</h2><p>5.1部分（不复赘述）</p><hr><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>（笑容满面实验，是去掉 <strong>accent embedding</strong> 和 <strong>accent classiﬁer</strong>）</p><ul><li><p>MOS得分很高</p></li><li><p>相似性上，比baseline差很多（作者猜测是数据量还是不够大的缘故）</p></li><li><p>（但他们用的VCTK 比 VCC 的几十条还是多很多的）</p></li><li></li></ul><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200817161213.png" alt="image-20200817161212167"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200816133725.png&quot; alt=&quot;image-20200816133724321&quot;&gt;&lt;/p&gt;
&lt;p&gt;展示demo：&lt;a href=&quot;https://liusongxiang.github.io/end2endAC/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://liusongxiang.github.io/end2endAC/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="VC" scheme="http://yoursite.com/tags/VC/"/>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>《Voice Conversion with transformer network-samsung》论文总结</title>
    <link href="http://yoursite.com/2020/08/15/NVAE%EF%BC%8C%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%B7%B1%E6%8C%96/"/>
    <id>http://yoursite.com/2020/08/15/NVAE%EF%BC%8C%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%B7%B1%E6%8C%96/</id>
    <published>2020-08-15T12:20:58.000Z</published>
    <updated>2020-08-17T11:52:30.177Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="NVAE"><a href="#NVAE" class="headerlink" title="NVAE"></a>NVAE</h1><ol><li><p>先大致搞清楚 VAE<img src="https://spaces.ac.cn/usr/uploads/2018/03/4168876662.png" alt="为了使模型具有生成能力，vae要求每个p(https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200810181426.png)都向正态分布看齐"></p><a id="more"></a></li><li><p>有两个 <strong>Encoder</strong>，一个求 $\mu$， 一个求 $\sigma$<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200810171034.png" alt="img"></p></li><li><p>说一下 VAE 中的「正态分布拟合」以及「从拟合的正态分布中采样」（Box-Muller 等等方法）<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200810171405.png" alt="image-20200810171404665"></p></li><li><p>VAE的名字中“变分”，是因为它的推导过程用到了<strong>KL散度及其性质</strong></p></li><li><p>说一说 <strong>VAE中的噪声</strong>（方差）</p><ol><li>增加重构难度，所以想减小它，让生成的数据更清晰</li><li>但是正是这个噪声，才是VAE精髓，增加了随机性</li></ol></li><li><p>噪声的好处（增加随机性） 和 坏处（导致采样结果成为确定性结果——均值 u ）</p></li><li><p>对噪声的处理：</p><ol><li>不直接让 方差 变为0（导致退化成 <strong>AE</strong> ）</li><li>而是配合着，让每段语音数据的 <strong>后验分布</strong> 朝着正态分布区靠近</li><li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200810162409.png" alt="image-20200810162407100"></li><li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200810162445.png" alt="image-20200810162443056"></li></ol></li><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200810170403.png" alt="image-20200810170401384"></p></li><li><p><strong>关于 后验分布 &amp;&amp; 先验分布 的理解</strong>（公式（2））</p><ol><li>后：单独的一段语音的高斯分布（标准正态）</li><li>先：某个说话人，所有语音的高斯分布集合（也是符合标准正态）</li></ol></li><li><p>VAE 中的 KL-loss <img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200810171818.png" alt="image-20200810171816918"></p></li><li><p>CVAE中的KL-loss（一类方法：实现标签的加入，从 <strong>无监督</strong> 转为 <strong>有监督</strong>）<img src="/2020/08/15/NVAE%EF%BC%8C%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%B7%B1%E6%8C%96/huangshengjie/Documents/2020/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99hexo%E6%B5%8B%E8%AF%95/Typora%E5%8D%9A%E5%AE%A2%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9/20200810172007.png" alt="image-20200810172006118"><strong>我们可以希望同一个类的样本都有一个专属的均值 $μ^Y$， （方差不变，还是单位方差），这个$μ^Y$让模型自己训练出来</strong></p></li><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200810172258.png" alt="img">【CVAE结构图】</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">python preprocess.py --resample_rate 16000 \</span><br><span class="line">                     --origin_wavpath .&#x2F;data&#x2F;VCTK-Data&#x2F;VCTK-Corpus&#x2F;wav48 \</span><br><span class="line">                     --target_wavpath .&#x2F;data&#x2F;VCTK-Data&#x2F;VCTK-Corpus&#x2F;wav16 \</span><br><span class="line">                     --mc_dir_train .&#x2F;data&#x2F;VCTK-Data&#x2F;mc&#x2F;train \</span><br><span class="line">                     --mc_dir_test .&#x2F;data&#x2F;VCTK-Data&#x2F;mc&#x2F;test \</span><br><span class="line">                     --speaker_dirs p262 p272 p229 p232 p292 p293 p360 p361 p248 p251</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">python main.py --train_data_dir .&#x2F;data&#x2F;VCTK-Data&#x2F;mc&#x2F;train \</span><br><span class="line">               --test_data_dir .&#x2F;data&#x2F;VCTK-Data&#x2F;mc&#x2F;test \</span><br><span class="line">               --use_tensorboard False \</span><br><span class="line">               --wav_dir .&#x2F;data&#x2F;VCTK-Data&#x2F;VCTK-Corpus&#x2F;wav16 \</span><br><span class="line">               --model_save_dir .&#x2F;data&#x2F;aca16sjb&#x2F;VCTK-Data&#x2F;models \</span><br><span class="line">               --sample_dir .&#x2F;data&#x2F;VCTK-Data&#x2F;samples \</span><br><span class="line">               --num_iters 200000 \</span><br><span class="line">               --batch_size 8 \</span><br><span class="line">               --speakers p262 p272 p229 p232 \</span><br><span class="line">               --num_speakers 4</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">python convert.py --resume_model 120000 \</span><br><span class="line">                  --num_speakers 4 \</span><br><span class="line">                  --speakers p262 p272 p229 p232 \</span><br><span class="line">                  --train_data_dir .&#x2F;data&#x2F;VCTK-Data&#x2F;mc&#x2F;train&#x2F; \</span><br><span class="line">                  --test_data_dir .&#x2F;data&#x2F;VCTK-Data&#x2F;mc&#x2F;test&#x2F; \</span><br><span class="line">                  --wav_dir .&#x2F;data&#x2F;VCTK-Data&#x2F;VCTK-Corpus&#x2F;wav16 \</span><br><span class="line">                  --model_save_dir .&#x2F;data&#x2F;aca16sjb&#x2F;VCTK-Data&#x2F;models \</span><br><span class="line">                  --convert_dir .&#x2F;data&#x2F;VCTK-Data&#x2F;converted</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;NVAE&quot;&gt;&lt;a href=&quot;#NVAE&quot; class=&quot;headerlink&quot; title=&quot;NVAE&quot;&gt;&lt;/a&gt;NVAE&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;先大致搞清楚 VAE&lt;img src=&quot;https://spaces.ac.cn/usr/uploads/2018/03/4168876662.png&quot; alt=&quot;为了使模型具有生成能力，vae要求每个p(https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200810181426.png)都向正态分布看齐&quot;&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;
    
    </summary>
    
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="VC" scheme="http://yoursite.com/tags/VC/"/>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>关于pyworld.load 读取音频和soundfile.read 差别</title>
    <link href="http://yoursite.com/2020/07/29/%E5%85%B3%E4%BA%8Epyworld.load%20%E8%AF%BB%E5%8F%96%E9%9F%B3%E9%A2%91%E5%92%8Csoundfile.read%20%E5%B7%AE%E5%88%AB/"/>
    <id>http://yoursite.com/2020/07/29/%E5%85%B3%E4%BA%8Epyworld.load%20%E8%AF%BB%E5%8F%96%E9%9F%B3%E9%A2%91%E5%92%8Csoundfile.read%20%E5%B7%AE%E5%88%AB/</id>
    <published>2020-07-29T05:40:35.697Z</published>
    <updated>2020-08-23T05:28:06.342Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><a id="more"></a><h1 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h1><p>在pyworld使用前，一般需要读取音频文件：</p><ul><li><p>librosa.load() 默认得到的是float32类型的数据，所以一般会再跟上 x.astype(np.float64)</p><ul><li>而恰恰是这么一个Numpy类型转换，会导致得到的 ap 特征中会含有 Nan 数据，这会导致最终的计算出现不必要的偏差；</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f"ap <span class="subst">&#123;np.isnan(ap).any()&#125;</span>"</span>)  <span class="comment"># 返回True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#统计nan个数</span></span><br><span class="line">t = ap</span><br><span class="line">t = t[np.isnan(t)]  <span class="comment"># 用切片法 + 条件限制，来得到 nan 值的切片</span></span><br><span class="line"><span class="comment"># t = t[np.where(np.isnan(t))]</span></span><br><span class="line">print(t.shape)  <span class="comment"># （4k+, ）</span></span><br></pre></td></tr></table></figure><ul><li><p>（这个问题和怎么计算得到ap无关：尝试了 world.wav2worl（）和 pyworld.harvest + cheaptrick + d4c路径，结果都一样）</p></li><li><p>Soundfile.read()  # 默认的数据返回值是 float64，所以可以直接得到所要求的数据格式</p></li><li><p>其中，对应的函数参数调整：sr 变为 samplerate ， mono 的单通道 改用 channels=1</p></li></ul><h2 id="Ps-附上代码和-issue网址"><a href="#Ps-附上代码和-issue网址" class="headerlink" title="Ps.附上代码和 issue网址"></a>Ps.附上代码和 issue网址</h2><p>[issue][<a href="https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder/issues/50]" target="_blank" rel="noopener">https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder/issues/50]</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pyworld <span class="keyword">as</span> world</span><br><span class="line"><span class="keyword">import</span> pyworld</span><br><span class="line"><span class="keyword">import</span> librosa</span><br><span class="line"><span class="keyword">import</span> librosa.display</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> soundfile</span><br><span class="line"></span><br><span class="line"><span class="comment"># wav,fs = librosa.load(os.getcwd()+"/bed (537).wav")</span></span><br><span class="line">wav, fs = soundfile.read(os.getcwd()+<span class="string">"/bed (537).wav"</span>)</span><br><span class="line"><span class="comment"># wav = wav.astype(np.float64)</span></span><br><span class="line"><span class="comment"># print(wav[0].type)  # 'numpy.float64' object has no attribute 'type'</span></span><br><span class="line">frame_period = <span class="number">5.0</span></span><br><span class="line">hop_length = int(fs * frame_period * <span class="number">0.001</span>)</span><br><span class="line">fftlen = world.get_cheaptrick_fft_size(fs)</span><br><span class="line"></span><br><span class="line">f0, timeaxis = pyworld.harvest(wav, fs, frame_period=frame_period, f0_floor=<span class="number">71.0</span>, f0_ceil=<span class="number">800.0</span>)</span><br><span class="line">sp = pyworld.cheaptrick(wav, f0, timeaxis, fs)</span><br><span class="line">ap = pyworld.d4c(wav, f0, timeaxis, fs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># f0, sp, ap = world.wav2world(x,sr,fftlen,frame_period)</span></span><br><span class="line">print(ap.shape)</span><br><span class="line">print(<span class="string">f"ap <span class="subst">&#123;np.isnan(ap).any()&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">wav = pyworld.synthesize(f0, sp, ap, fs, frame_period)</span><br><span class="line"><span class="comment"># wav = wav.astype(np.float32)</span></span><br><span class="line">soundfile.write(<span class="string">'test.wav'</span>, wav, fs)</span><br><span class="line"></span><br><span class="line">x, sr = soundfile.read(os.getcwd()+<span class="string">"/bed (537).wav"</span>)</span><br><span class="line"><span class="comment"># print(x[0].type)  # 'numpy.float64' object has no attribute 'type'</span></span><br><span class="line">f01, timeaxis1 = pyworld.harvest(x, sr, frame_period=frame_period, f0_floor=<span class="number">71.0</span>, f0_ceil=<span class="number">800.0</span>)</span><br><span class="line">sp1 = pyworld.cheaptrick(x, f01, timeaxis1, fs)</span><br><span class="line">ap1 = pyworld.d4c(x, f01, timeaxis1, fs)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">以上这样，直接用soundfile 读取成float64 数据，</span></span><br><span class="line"><span class="string">然后再直接用 soundfile.write 保存float64的文件 或者是 先转化成float32 再保存成文件， </span></span><br><span class="line"><span class="string">保存出来的文件再次用soundfile 读取出来，</span></span><br><span class="line"><span class="string">再次用测试ap是否有 nan值，都没有问题。</span></span><br><span class="line"><span class="string">综上，能用soundfile就避免用librosa，读取和写入文件都是这个道理；ßå</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'************'</span>)</span><br><span class="line">print(ap1.shape)</span><br><span class="line">print(<span class="string">f"ap1 <span class="subst">&#123;np.isnan(ap1).any()&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"ap <span class="subst">&#123;np.isnan(ap).any()&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # print(np.isnan(ap))</span></span><br><span class="line"><span class="comment"># t = ap</span></span><br><span class="line"><span class="comment"># t = t[np.isnan(t)]</span></span><br><span class="line"><span class="comment"># # t = t[np.where(np.isnan(t))]</span></span><br><span class="line"><span class="comment"># print(t.shape)</span></span><br><span class="line"></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ap:</span><br><span class="line">    <span class="comment"># if np.isnan(x):</span></span><br><span class="line">    count = count+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(count)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> np.isnan(ap).any():</span><br><span class="line"></span><br><span class="line">    f0, sp, ap = world.wav2world(np.absolute(x),fs,fftlen,frame_period)</span><br><span class="line">    print(<span class="string">f"ap abs <span class="subst">&#123;np.isnan(ap).any()&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">exit()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="-[语音]" scheme="http://yoursite.com/categories/%E8%AF%AD%E9%9F%B3/"/>
    
    
      <category term="语音" scheme="http://yoursite.com/tags/%E8%AF%AD%E9%9F%B3/"/>
    
  </entry>
  
  <entry>
    <title>关于 WORLD 中 code_spectral_envelope 和 MFCC 关系的理解</title>
    <link href="http://yoursite.com/2020/07/23/%E5%85%B3%E4%BA%8E%20WORLD%20%E4%B8%AD%20code_spectral_envelope%20%E5%92%8C%20MFCC%20%E5%85%B3%E7%B3%BB%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>http://yoursite.com/2020/07/23/%E5%85%B3%E4%BA%8E%20WORLD%20%E4%B8%AD%20code_spectral_envelope%20%E5%92%8C%20MFCC%20%E5%85%B3%E7%B3%BB%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2020-07-23T10:57:47.644Z</published>
    <updated>2020-08-17T11:49:01.796Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h1><ol><li><a href="https://github.com/mmorise/World/issues/90" target="_blank" rel="noopener">https://github.com/mmorise/World/issues/90</a> （有人提问 WORLD 提取得到的 mel spectrum 梅尔谱和传统概念上 经过一系列stft之后还要经过“三角滤波器组”的过程区别？）</li><li><a href="https://github.com/mmorise/World/issues/33" target="_blank" rel="noopener">https://github.com/mmorise/World/issues/33</a> （r9y9 在<a href="https://github.com/mmorise" target="_blank" rel="noopener">mmorise</a>/<strong><a href="https://github.com/mmorise/World" target="_blank" rel="noopener">World</a></strong> 下提问关于 编码/解码后音色变化问题；代码bug已解决；学习一下画图和使用特点）</li></ol><hr><a id="more"></a><h1 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h1><ol><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200723190241.png" alt="image-20200723190240435"></p></li><li><p>简言之，传统论文  <a href="https://www.sp.nitech.ac.jp/~tokuda/selected_pub/pdf/conference/tokuda_icslp1994.pdf" target="_blank" rel="noopener">Mel-cepstral analysis</a> 提到的方法，也就是正常思路的经过 FFT 后再经过三角滤波器组 得到的 mel 谱，之所以需要三角滤波，可以理解为是，因为MFCC是在 <strong>频谱图</strong> 上进行的操作，所以是未经过 <strong>平滑</strong> 操作的，所以需要滤波器；</p></li><li><p>而WORLD，是在频谱包络上进行的操作，本身已经是顺滑过的，所以得到的 sp 特征，看似流程上没有三角滤波，但是它在使用的时候，效果和SPTK、librosa、merlin之类工具得到的 MFCC 来处理的音频效果是差不多的。</p></li><li><p>所以，就可以理解，很多论文的实现上，作者们在遇到：MFCC 这个特征需要时，若非论文着重强调，是可以用 <strong>code_spectral_envelope</strong> ，并取维度参数为 36 等，来表示36维度（bin）的MFCC特征的。</p></li></ol><hr><p>以上，解决了一直没人能帮我说清楚的问题疑惑。</p><p>还是要多看看源码和 issue，和大佬们交流才进步的多。</p><hr><p>这行里，可能大佬很多，但是能真正带领小白入门的系统专家真的少。sigh。我可能适合做老师，喜欢把大家难懂的东西，娓娓道来，教会孩子们。😁</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;参考：&quot;&gt;&lt;a href=&quot;#参考：&quot; class=&quot;headerlink&quot; title=&quot;参考：&quot;&gt;&lt;/a&gt;参考：&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/mmorise/World/issues/90&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/mmorise/World/issues/90&lt;/a&gt; （有人提问 WORLD 提取得到的 mel spectrum 梅尔谱和传统概念上 经过一系列stft之后还要经过“三角滤波器组”的过程区别？）&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/mmorise/World/issues/33&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/mmorise/World/issues/33&lt;/a&gt; （r9y9 在&lt;a href=&quot;https://github.com/mmorise&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;mmorise&lt;/a&gt;/&lt;strong&gt;&lt;a href=&quot;https://github.com/mmorise/World&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;World&lt;/a&gt;&lt;/strong&gt; 下提问关于 编码/解码后音色变化问题；代码bug已解决；学习一下画图和使用特点）&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
    
    </summary>
    
    
      <category term="-[语音]" scheme="http://yoursite.com/categories/%E8%AF%AD%E9%9F%B3/"/>
    
    
      <category term="语音" scheme="http://yoursite.com/tags/%E8%AF%AD%E9%9F%B3/"/>
    
  </entry>
  
  <entry>
    <title>python 切片的一些混淆点（备忘）</title>
    <link href="http://yoursite.com/2020/07/16/%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/07/16/%E7%AC%94%E8%AE%B0/</id>
    <published>2020-07-16T12:02:54.899Z</published>
    <updated>2020-08-17T11:50:24.850Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="1-经常混淆的py切片细节"><a href="#1-经常混淆的py切片细节" class="headerlink" title="1.经常混淆的py切片细节"></a>1.经常混淆的py切片细节</h2><a id="more"></a><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200716200314.png" alt="image-20200716200313100"></p><h1 id="python-中的-1-和-1"><a href="#python-中的-1-和-1" class="headerlink" title="python 中的 [:-1] 和 [::-1]"></a>python 中的 [:-1] 和 [::-1]</h1><ul><li><p><a href="https://www.runoob.com/note/51257" target="_blank" rel="noopener">https://www.runoob.com/note/51257</a></p></li><li><pre><code class="python">a=<span class="string">'python'</span>b=a[::<span class="number">-1</span>]print(b) <span class="comment">#nohtyp</span>c=a[::<span class="number">-2</span>]print(c) <span class="comment">#nhy</span><span class="comment">#从后往前数的话，最后一个位置为-1</span>d=a[:<span class="number">-1</span>]  <span class="comment">#从位置0到位置-1之前的数</span>print(d)  <span class="comment">#pytho</span>e=a[:<span class="number">-2</span>]  <span class="comment">#从位置0到位置-2之前的数</span>print(e)  <span class="comment">#pyth</span><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* &#96;&#96;&#96;python</span><br><span class="line">  b &#x3D; a[i:j]   # 表示复制a[i]到a[j-1]，以生成新的list对象</span><br><span class="line">  </span><br><span class="line">  a &#x3D; [0,1,2,3,4,5,6,7,8,9]</span><br><span class="line">  b &#x3D; a[1:3]   # [1,2]</span><br><span class="line">  </span><br><span class="line">  # 当i缺省时，默认为0，即 a[:3]相当于 a[0:3]</span><br><span class="line">  # 当j缺省时，默认为len(alist), 即a[1:]相当于a[1:10]</span><br><span class="line">  # 当i,j都缺省时，a[:]就相当于完整复制一份a</span><br><span class="line">  </span><br><span class="line">  b &#x3D; a[i:j:s]    # 表示：i,j与上面的一样，但s表示步进，缺省为1.</span><br><span class="line">  # 所以a[i:j:1]相当于a[i:j]</span><br><span class="line">  </span><br><span class="line">  # 当s&lt;0时，i缺省时，默认为-1. j缺省时，默认为-len(a)-1</span><br><span class="line">  # 所以a[::-1]相当于 a[-1:-len(a)-1:-1]，也就是从最后一个元素到第一个元素复制一遍，即倒序。</span><br></pre></td></tr></table></figure></code></pre></li></ul><hr><h2 id="2-关于-torch-的pad：full模式（相对于tf中的same模式）"><a href="#2-关于-torch-的pad：full模式（相对于tf中的same模式）" class="headerlink" title="2.关于 torch 的pad：full模式（相对于tf中的same模式）"></a>2.关于 torch 的pad：full模式（相对于tf中的same模式）</h2><p>$$<br>              H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]<br>                        \times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor</p><pre><code>W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1]          \times (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor</code></pre><p>$$</p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200801174740.png" alt="image-20200801174738560"></p><h2 id="相比tensorflow，PyTorch需要用户清楚的知道的自己的卷积核选取对结果的影响。"><a href="#相比tensorflow，PyTorch需要用户清楚的知道的自己的卷积核选取对结果的影响。" class="headerlink" title="相比tensorflow，PyTorch需要用户清楚的知道的自己的卷积核选取对结果的影响。"></a>相比tensorflow，PyTorch需要用户清楚的知道的自己的卷积核选取对结果的影响。</h2><ul><li><strong>简单一点</strong>：先只看 t = kernel【0】// 2；</li><li>Kernel【0】为奇数，那么padding就等于 t;</li><li>否则，kernel【0】为偶数，那么padding就等于 【t - 1】</li></ul><hr><h2 id="3-反卷积"><a href="#3-反卷积" class="headerlink" title="3.反卷积"></a>3.反卷积</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200801184750.png" alt="image-20200801184748828"></p><p><a href="https://blog.csdn.net/g11d111/article/details/82665265" target="_blank" rel="noopener">https://blog.csdn.net/g11d111/article/details/82665265</a></p><ul><li>反卷积这部分用的少，公式其实就是正卷积中，in 和 out 对调；</li><li>只不过 反卷积没有了 正卷积 中 的 <strong>向下取整</strong> 的操作</li><li>所以在反卷积中， 需要按规矩公式，简单计算一下padding 尺寸参数；</li><li>最简单就是用局部代码输出看一下结果，看是否符合输入输出尺寸要求</li></ul><hr><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200801193707.png" alt="image-20200801193706275"></p><p><a href="https://blog.csdn.net/m0_37586991/article/details/87855342" target="_blank" rel="noopener">https://blog.csdn.net/m0_37586991/article/details/87855342</a></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200801194743.png" alt="image-20200801194739307"></p><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-经常混淆的py切片细节&quot;&gt;&lt;a href=&quot;#1-经常混淆的py切片细节&quot; class=&quot;headerlink&quot; title=&quot;1.经常混淆的py切片细节&quot;&gt;&lt;/a&gt;1.经常混淆的py切片细节&lt;/h2&gt;
    
    </summary>
    
    
      <category term="-[python]" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="-[python] -[编程]" scheme="http://yoursite.com/tags/python-%E7%BC%96%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>《《ONE-SHOT VOICE CONVERSION USING STAR-GAN》》论文总结</title>
    <link href="http://yoursite.com/2020/07/14/%E3%80%8AONE-SHOT%20VOICE%20CONVERSION%20USING%20STAR-GAN%E3%80%8B/"/>
    <id>http://yoursite.com/2020/07/14/%E3%80%8AONE-SHOT%20VOICE%20CONVERSION%20USING%20STAR-GAN%E3%80%8B/</id>
    <published>2020-07-14T12:20:58.000Z</published>
    <updated>2020-07-14T12:14:57.275Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712160845.png" alt="image-20200712160843097"></p><a id="more"></a><ol><li><h2 id="解决的问题："><a href="#解决的问题：" class="headerlink" title="解决的问题："></a>解决的问题：</h2><ol><li>在原本的StarGan-VC中，实现了“<strong>未知speaker：source/target 都未知</strong>”的转换。称作“<strong>One-Shot</strong>”</li><li>其他文章其实也有做过类似功能：基于“<strong>超大数据集的  VC 模型</strong>”做自适应adaption调整：比如之前谢磊团队那篇 <strong>对WaveNet的改进模型</strong>，在处理未知说话人时，采用的是 额外20到50条数据的进一步收敛；</li></ol></li><li><h2 id="采用的主要方法："><a href="#采用的主要方法：" class="headerlink" title="采用的主要方法："></a>采用的主要方法：</h2><ol><li>把说话人信息看作 embedding。</li><li>但是不同于原本 StarGan-VC 代码实现中（非官方）用 One-Hot来做embedding。</li><li>也不是用的后来咱们讨论中，改用 embedding_lookup（）的方式（虽然已经比One-Hot concate 方式要好很多了）</li><li>而是采用2018年google的一篇文章，提取embedding的单独网络；（<strong>Global Style Token (GST)</strong>），用这个网络提取出来的embedding信息，可以表征说话人身份信息。</li><li>具体细节接下来说：</li></ol><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712162828.png" alt="image-20200712162827085"></p></li><li><h2 id="有价值的细节："><a href="#有价值的细节：" class="headerlink" title="有价值的细节："></a>有价值的细节：</h2><ol><li>这个GST 训练时，先有一堆说话人，每个人有很多数据（1⃣️）；</li><li>然后这个网络的功能就是：能把一个新来的 集合外数据（人/内容），扔进去，照样得到一个 speaker_embedding 信息；</li><li>这个embedding信息怎么来的呢？原来是由模型（1⃣️）训练集中的说话人embedding 融合出来的；所以最终的效果上，会是：新说话人声音特征，由训练集说话人特征组合而成；</li><li>这个GST当中， speaker ID 实现上，同样采用 one-hot 形式。</li></ol></li></ol><ol start="4"><li><h2 id="以上三点，其实都只是前人的工作，本文拿来创新性应用。"><a href="#以上三点，其实都只是前人的工作，本文拿来创新性应用。" class="headerlink" title="以上三点，其实都只是前人的工作，本文拿来创新性应用。"></a>以上三点，其实都只是前人的工作，本文拿来创新性应用。</h2><ol><li>本文的细节创新：</li></ol><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712164238.png" alt="image-20200712164236074"></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712164302.png" alt="image-20200712164301003"></p><ol><li>原本的 StarGan-VC 中，在Generator部分 的BottelNect部分，采用正常的 【Conv，Norm，GLU】结构，其中的Conv采用 <strong>5 个channel</strong>；</li><li>本文作者实验证明，这个 5 channel 太小了，影响了 reconstruction 音频质量；</li><li>但是尝试放大这个channel数，会发现有“<strong>信息泄露：information leakage</strong>”，有点像信号处理中的“<strong>频率泄漏：frequency leak</strong>”（后者采用加窗的方式规避这个问题）；</li><li>所谓的泄漏，就是出现了无关的信息：Generator 没能把source中的身份信息过滤干净，最后的声音四不像；（频率泄漏则是，在没加窗函数之前，做FFT会出现 本没有的 频率）<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712165144.png" alt="image-20200712165142509"></li><li>最后的效果上：<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712165232.png" alt="image-20200712165230066"></li><li>作者说，用了这种方式（SoftMax 代替 conv-2d 做 BottleNeck），在转换结果上，<strong>共振峰频率</strong>（<strong>frequencies of formants</strong>）会低一点：显示在能量图上，就是高频部分（上面）颜色会浅一点。</li><li><strong>🌟疑问点</strong>：这个 <strong>共振峰频率</strong> 低一点，<strong>能说明什么</strong>？？？？这样就能说明 说话内容信息泄漏会少一点吗？没搞懂；</li></ol></li></ol><h2 id="5-另一个操作改进点："><a href="#5-另一个操作改进点：" class="headerlink" title="5.  另一个操作改进点："></a>5.  另一个操作改进点：</h2><ol><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712172438.png" alt="image-20200712172437528"></p></li><li><p>在Generator的修改上如图：</p></li><li><p>对比原型：<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712171700.png" alt="image-20200712171657257"></p></li><li><p>小细节：</p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200713175103.png" alt="image-20200713175101061"></p><ul><li><p>偶数：同时更新 D &amp;&amp; G</p></li><li><p>奇数：只更新 D</p></li><li><p>特征选用上：由实验经验，<strong>由36维 MFCC</strong> 改为用 <strong>96维 sp 谱包络（96-bin Mel spectral envelope）</strong></p></li><li><p>（这个和合成的应用上也有呼应， Mel 的训练合成，比MFCC reconstrruction 效果要好，更深原理 <strong>模糊</strong>）</p></li></ul></li></ol><h2 id="6-结果上："><a href="#6-结果上：" class="headerlink" title="6. 结果上："></a>6. 结果上：</h2><p>其实就是 VC 领域两个主观评价指标：Reconstruction 质量 和 Conversion 质量。</p><ol><li>这份Demo里，Reconstruction 的效果也不怎么好，在Conversion 转换效果上还凑合；这个和StarGan-VC 差不多；</li><li>Reconstruction 上，target 已知或 “增量式训练”过，数据效果上，提升不少；Conversion 效果也大差不差；<img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712181558.png" alt="image-20200712181557111"></li></ol><h2 id="7-VC改进思路小结："><a href="#7-VC改进思路小结：" class="headerlink" title="7. VC改进思路小结："></a>7. VC改进思路小结：</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200713132212.png" alt="image-20200713132210312"></p><ul><li>在想能否写<strong>WorkShop</strong>论文</li><li>不知还有什么可改进的点，咱们可以接<strong>上WaveNet后端</strong>，类似谢磊上篇用的；</li><li>然后<strong>重构损失</strong>上，模型结构学习一下网易这篇。</li><li><strong>embedding</strong> 上，为的是实现 <strong>one-shot</strong>，再考虑是否有其他方法；</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200712160845.png&quot; alt=&quot;image-20200712160843097&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="VC" scheme="http://yoursite.com/tags/VC/"/>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>《EFFECTIVE WAVENET ADAPTATION FOR VOICE CONVERSION WITH LIMITED DATA》</title>
    <link href="http://yoursite.com/2020/06/27/%E3%80%8AEFFECTIVE%20WAVENET%20ADAPTATION%20FOR%20VOICE%20CONVERSION%20WITH%20LIMITED%20DATA%E3%80%8B/"/>
    <id>http://yoursite.com/2020/06/27/%E3%80%8AEFFECTIVE%20WAVENET%20ADAPTATION%20FOR%20VOICE%20CONVERSION%20WITH%20LIMITED%20DATA%E3%80%8B/</id>
    <published>2020-06-27T07:16:54.000Z</published>
    <updated>2020-09-06T16:12:25.973Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627153546.png" alt="image-20200627153541122"></p><a id="more"></a><h1 id="1-模型"><a href="#1-模型" class="headerlink" title="1.模型"></a>1.模型</h1><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627153718.png" alt="image-20200627153716639"></p><hr><h1 id="2-模型特点-amp-amp-训练需求"><a href="#2-模型特点-amp-amp-训练需求" class="headerlink" title="2.模型特点 &amp;&amp; 训练需求"></a>2.模型特点 &amp;&amp; 训练需求</h1><ul><li>大数据集 + 少数据集（target）、many-to-many（没强调）</li><li>不采用“独立模型”的思路（e.g.不根据性别来分组训练），而是先用 多说话人的大量数据集，训练 <strong>Speaker Independent (SI) WaveNet model</strong></li><li>再用 少量<strong>Target Speaker</strong>数据进行微调；</li><li>（和上一篇 三星论文 思路有点像，但三星侧重转换模型（引入MultiHead Attention），他的 <strong>WaveNet</strong> 就用现成的；</li><li>本文则 侧重后端声码器 <strong>WaveNet</strong> 的优化：<strong>速度</strong> 和 <strong>质量</strong>）</li></ul><hr><h1 id="3-改进-WaveNet-的思路"><a href="#3-改进-WaveNet-的思路" class="headerlink" title="3.改进 WaveNet 的思路"></a>3.改进 WaveNet 的思路</h1><ul><li><strong>phonetic posteriorgram (PPG)</strong> （<strong>音素后验概率</strong>）和 语音波形（时域信号） 直接映射（本来呢？）</li><li><strong>🌟singular value decomposition (SVD)</strong>（<strong>奇异值分解</strong>）：减少 WaveNet 的训练参数量（<strong>重点</strong>）</li><li><strong>between PPGs and the corresponding time-domain speech signals of the same speaker.</strong>：模型的预训练，是在同一个说话人的 <strong>PPG</strong> 特征 和对应的 <strong>时域信号</strong> 之间进行训练；</li><li></li></ul><hr><h1 id="4-关于PPG-amp-amp-SVD"><a href="#4-关于PPG-amp-amp-SVD" class="headerlink" title="4.关于PPG &amp;&amp; SVD"></a>4.关于PPG &amp;&amp; SVD</h1><ul><li><del>PPG 有一个其他人自己写的 python 包，但是没有正规的开源工具包，很多论文都直接说用到了这个特征，却从没交代怎么提取，从哪来的。【<strong>请教老师</strong>】</del></li><li><strong>在Deep VC项目里面的 Train1.py 部分，出来的就是语音的 PPG</strong>，（它是想预先训一个ASR模型）</li><li>或者用Kaldi来求；</li><li>本质都是，训练一个 <strong>phonetic recognition system.</strong>，然后用这个识别网络去识别（过程和识别出MFCC特征很像）；<strong>怎么 VC 领域又给牵扯到 ASR 领域去了，四不像</strong></li></ul><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627162338.png" alt="image-20200627162336086"></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627175055.png" alt="image-20200627174816852"></p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627175051.png" alt="image-20200627175049966"></p><hr><h1 id="5-其他"><a href="#5-其他" class="headerlink" title="5.其他"></a>5.其他</h1><h2 id="这类模型：-原本的转换流程："><a href="#这类模型：-原本的转换流程：" class="headerlink" title="* 这类模型： 原本的转换流程："></a>* 这类模型： 原本的转换流程：</h2><h3 id="一句话——-gt-PPG特征不是直接合成出语音，而要经过转换；"><a href="#一句话——-gt-PPG特征不是直接合成出语音，而要经过转换；" class="headerlink" title="* 一句话——&gt;PPG特征不是直接合成出语音，而要经过转换；"></a>* 一句话——&gt;PPG特征不是直接合成出语音，而要经过转换；</h3><ol><li>从 .wav 中提取 <strong>source</strong> 的 PPG 特征（自注：需要额外训练一个声学模型，用来提取PPG）</li><li>用 “提前用大量 多说话人数据集 训练的” <strong>SI</strong> <strong>Conversion</strong> <strong>Model</strong>， 将 PPG 特征转化成 声学特征（<strong>mel ？</strong>）</li><li>再将 前一步骤的声学特征，扔进 <strong>经过（用 Target 语音）适应性调整的 WavaNet 声码器</strong>，以此合成最终转换语音；</li></ol><hr><h2 id="本文改进的转换流程："><a href="#本文改进的转换流程：" class="headerlink" title="*本文改进的转换流程："></a>*本文改进的转换流程：</h2><h3 id="特征转换模型-和-语音生成模型-是分开训练的；"><a href="#特征转换模型-和-语音生成模型-是分开训练的；" class="headerlink" title="* 特征转换模型 和 语音生成模型 是分开训练的；"></a>* 特征转换模型 和 语音生成模型 是分开训练的；</h3><h3 id="但是训练完之后，在转换步骤里，它利用（PPG）作为-本地条件-直接生成🌟时域语音信号"><a href="#但是训练完之后，在转换步骤里，它利用（PPG）作为-本地条件-直接生成🌟时域语音信号" class="headerlink" title="* 但是训练完之后，在转换步骤里，它利用（PPG）作为 本地条件 直接生成🌟时域语音信号"></a>* 但是训练完之后，在转换步骤里，它利用（PPG）作为 本地条件 <strong>直接生成🌟时域语音信号</strong></h3><p>即：输入 source 语音（提取特征后）给WaveNet 模型，然后WaveNet<strong>直接转换出来</strong> Target 语音；</p><ol><li><strong>SI</strong> WaveNet Conversion Model 的训练（用多说话人大数据量训练）(其实就是让WaveNet学会根据给定特征，<strong>重建出语音波形</strong>，模型的输入就是大量独立的语音，从而实现：让模型学会 <strong>与说话人无关的波形重建能力</strong>)</li><li>上述模型的适应性调整 <strong>adaption</strong>（基于target语料）</li><li><strong>run-time conversion</strong></li></ol><p>具体的，结合图片：</p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627194348.png" alt="image-20200627194342790"></p><h2 id="在第一步中："><a href="#在第一步中：" class="headerlink" title="在第一步中："></a>在第一步中：</h2><p>五个特征【<strong>PPG、Energy、F0、V/UV、BAP</strong>】（BAP 待查）</p><ol><li><p>为了训练说话人无关的 SI WaveNet Conversion Model，先从多说话人的数据集中，读取 <strong>PPG</strong> 特征（另外训练的声学特征提取模型），用来表征 <strong>说话内容</strong></p></li><li><p><strong>Energy</strong> 用的是 <strong>梅尔倒谱</strong> 的<strong>第一维度</strong>用来表征 能量轮廓（这和我们说的 mel-cepstral 用来代表<strong>频谱图的轮廓信息</strong> 相联系）</p><p><img src="/2020/06/27/%E3%80%8AEFFECTIVE%20WAVENET%20ADAPTATION%20FOR%20VOICE%20CONVERSION%20WITH%20LIMITED%20DATA%E3%80%8B/huangshengjie/Documents/2020/%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99hexo%E6%B5%8B%E8%AF%95/Typora%E5%8D%9A%E5%AE%A2%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9/20200627195116.png" alt="image-20200627195114177"></p></li><li><p><strong>F0</strong> 取 log 对树</p></li><li><p><strong>V/UV</strong> 用来表示 发声/不发声 的一个标志（实现的话，我想可以用 f0 来判断当前帧 有没有人声；只有发声了，f0 才大于 0 ）</p></li><li><p><strong>BAPs</strong> ：还没查，指向一篇 06 年日本的文章，说法是，这个特征对语音波形的 <strong>重建</strong> 很有帮助</p></li></ol><p>这五个特征，concate 到一起，输入 SI WaveNet Conv Model 训练；</p><hr><h2 id="步骤二"><a href="#步骤二" class="headerlink" title="步骤二"></a>步骤二</h2><ul><li>用少量的 Target 内容语音，重复上述过程；</li><li>作用就是在前述的 SI 模型中，添加一点 SD（依赖于当前的 Target Speaker）</li></ul><hr><h2 id="步骤三："><a href="#步骤三：" class="headerlink" title="步骤三："></a>步骤三：</h2><p>具体转换实现时：</p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627201313.png" alt="image-20200627201311810"></p><ul><li>转换时，输入source语音；</li><li>提取该source语音的五个特征；</li><li>对其中的 $logf0$ 做微调：其中 $\mu$ 表示均值，$\sigma$ 表示方差，$logf0_y$ 表示转换好的Target $ logf0 $</li><li>上述$logf0_y$和其他四个 source 的特征，一起送入第二步微调完的模型，做转换；</li><li>完事了；</li></ul><h2 id="以上是整体的优化方案；"><a href="#以上是整体的优化方案；" class="headerlink" title="以上是整体的优化方案；"></a>以上是整体的优化方案；</h2><h2 id="以下还有一点：对-SI-WaveNet-结构本身再做调整："><a href="#以下还有一点：对-SI-WaveNet-结构本身再做调整：" class="headerlink" title="以下还有一点：对 SI WaveNet 结构本身再做调整："></a>以下还有一点：对 SI WaveNet 结构本身再做调整：</h2><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627204424.png" alt="image-20200627204422535"></p><ul><li><p>在 2019 也是这群人，发了一篇关于 WaveNet 内部结构改造：</p><ul><li><strong>data-efﬁcient SD WaveNet vocoder</strong></li></ul></li><li><p>本文则对上面的改造再做优化： <strong>SD</strong> 改造为 <strong>SVD</strong>（singular value decomposition）<strong>奇异值分解</strong></p></li><li><p>以期降低复杂度，减少训练参数</p></li><li><p>具体实现上：</p></li><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627204710.png" alt="image-20200627204708474"></p></li><li><p><strong>：</strong>在 每一个 <strong>扩展卷积层</strong> 后面，再加一个 <strong>1 x 1</strong> 的卷积层；</p></li><li><p>说是这样就能显著减少 <strong>模型参数量</strong>；</p></li><li><p>——&gt;训练时间减少，效果还和19年的文章效果差不多；</p></li></ul><p><strong>Ps</strong>.（TF 当中倒是有一个单独的 SVD 工具，但是应该是针对更具体的计算公式的，和这里的 在WaveNet 模型内部优化方法不太一样？不确定？）</p><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627230800.png" alt="image-20200627230757346"></p><hr><h1 id="6-杂项整理"><a href="#6-杂项整理" class="headerlink" title="6.杂项整理"></a>6.杂项整理</h1><ul><li>数据集：VC的常规数据集两个：CMU-ARCTIC  &amp;&amp;  <strong>CSTR-VCTK</strong>（跑过torch版stargan了：109人，44 h，每人三百条左右语音，都是平行数据；两个大类：16K &amp; 48K；另外还配有文本，还可以用作合成数据）</li><li>本文把 VCTK 全拿来训练 SI WaveNet 了；</li><li>转换步骤，用的 ARCTIC 数据集；</li><li>其他一些实现细节：</li><li><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627212421.png" alt="image-20200627212417782"></li><li>其他需要的作为对比的 <strong>Baseline</strong> 模型的构建参数给了挺多；不展开了；<ul><li>• AMA-WORLD:</li><li>• AMA-WaveNet:</li><li>• WaveNet-adp:</li><li>•WaveNet-SVD-adp:【本文提出的】</li></ul></li></ul><h2 id="新增一个-主观评价指标"><a href="#新增一个-主观评价指标" class="headerlink" title="* 新增一个 主观评价指标"></a>* 新增一个 主观评价指标</h2><h3 id="AB-and-XAB-测试：【A-B】中选一个-【不选-A-B】-三个中选一个"><a href="#AB-and-XAB-测试：【A-B】中选一个-【不选-A-B】-三个中选一个" class="headerlink" title="AB and XAB 测试：【A/B】中选一个 / 【不选/A/B】 三个中选一个"></a>AB and XAB 测试：【A/B】中选一个 / 【不选/A/B】 三个中选一个</h3><ul><li>multiple stimuli with hidden reference and anchor (<strong>MUSHRA</strong>)</li><li>“主观评估中间声音质量的方法”</li><li>–：让听众在两者之间选择一个更优秀的结果；置信区间取 95%</li><li></li></ul><h2 id="新增一个-Objective-evaluation-客观评价指标"><a href="#新增一个-Objective-evaluation-客观评价指标" class="headerlink" title="* 新增一个 Objective evaluation 客观评价指标"></a>* 新增一个 Objective evaluation 客观评价指标</h2><ul><li><p><strong>RMSE</strong>（root mean squared error）：均方根误差；【单位（dB）】</p></li><li><p>：evaluate distortion between the target and converted speech.</p></li><li><p>原理和 MCD 差不多；MCD 评测的是经过 DTW 的语音 Mel 谱特征；</p></li><li><p>🌟<a href="https://www.w3cschool.cn/tensorflow_python/tensorflow_python-15ev2z8o.html" target="_blank" rel="noopener"><strong>Tensorflow 有对应的 API</strong></a>：</p></li><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627225534.png" alt="image-20200627225531701"></p></li><li><p>RMSE 他在这里处理的对象比较细致：</p></li><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627213700.png" alt="image-20200627213656629"></p></li><li><h2 id="疑问："><a href="#疑问：" class="headerlink" title="疑问："></a>疑问：</h2></li></ul><ol><li><strong>frequency bin</strong>：频率槽；这个参数的 <strong>频率间隔</strong> 一般设置多少？</li></ol><ul><li>是按照 <strong>1HZ</strong> 来分隔吗？？？</li><li>【WORLD特征的帧长是 5ms（5ms frame shift）】</li></ul><ol start="2"><li>这个 <strong>magnitude</strong> 值，是直接用 <strong>当前频率的 频谱图幅值</strong> 吗？</li></ol><h1 id="7-其他疑问点："><a href="#7-其他疑问点：" class="headerlink" title="7.其他疑问点："></a>7.其他疑问点：</h1><ul><li><ol><li><strong>The speech is encoded by 8 bits µ -law.</strong> ： 8 bits µ -law 是什么规范；</li></ol></li><li><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627213249.png" alt="image-20200627213248518"></p></li><li><ol start="2"><li><strong>PPG</strong> 的 <strong>具体构建网络</strong> 应该是怎么样的，有统一的代码模型吗。有点凌乱；</li></ol><p><img src="https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627214950.png" alt="image-20200627214947808"></p></li><li></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://blog-1301959139.cos.ap-beijing.myqcloud.com/picGo/20200627153546.png&quot; alt=&quot;image-20200627153541122&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="test" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/test/"/>
    
    
      <category term="VC" scheme="http://yoursite.com/tags/VC/"/>
    
      <category term="论文阅读笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>139. 单词拆分</title>
    <link href="http://yoursite.com/2020/06/26/139.%20%E5%8D%95%E8%AF%8D%E6%8B%86%E5%88%86/"/>
    <id>http://yoursite.com/2020/06/26/139.%20%E5%8D%95%E8%AF%8D%E6%8B%86%E5%88%86/</id>
    <published>2020-06-25T16:15:41.464Z</published>
    <updated>2020-06-26T05:46:49.666Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-解题思路："><a href="#1-解题思路：" class="headerlink" title="1.解题思路："></a>1.解题思路：</h1><ul><li>动态规划听上去非常高大上，但是其实都是源自于一个很自然的想法，就拿这道题来说，假如需要判断”onetwothreefour”这一个字符串能不能满足条件，我们很自然的想法就是：</li><li>如果”onetwothree”这一段可以拆分，再加上four如果也可以，那不就行了；</li><li>或者</li><li>如果”onetwothre”这一段可以拆分，再加上efour如果也可以，那不就行了；</li><li>这其实已经抓住了动态规划的最核心的东西了，换成式子来表达，就是</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dp[<span class="string">"onetwothreefour"</span>] = dp[<span class="string">"onetwothree"</span>这一段] &amp;&amp; 判断一下<span class="string">"four"</span></span><br><span class="line">dp[<span class="string">"onetwothreefour"</span>] = dp[<span class="string">"onetwothre"</span>这一段] &amp;&amp; 判断一下<span class="string">"efour"</span></span><br></pre></td></tr></table></figure><h1 id="2-代码："><a href="#2-代码：" class="headerlink" title="2.代码："></a>2.代码：</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> 单词拆分<span class="title">_139</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> Map&lt;String, Boolean&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">wordBreak</span><span class="params">(String s, List&lt;String&gt; wordDict)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span>[] dp = <span class="keyword">new</span> <span class="keyword">boolean</span>[s.length() + <span class="number">1</span>];</span><br><span class="line">        <span class="comment">//Java boolean数组默认值为False</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//将List中单词放进 HashMap</span></span><br><span class="line">        <span class="keyword">for</span>(String word:wordDict)&#123;</span><br><span class="line">            map.put(word, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//初始化</span></span><br><span class="line">        dp[<span class="number">0</span>] = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//遍历</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        * public String substring(int beginIndex, int endIndex)</span></span><br><span class="line"><span class="comment">        * beginIndex -- 起始索引（包括）, 索引从 0 开始</span></span><br><span class="line"><span class="comment">        * endIndex -- 结束索引（不包括）</span></span><br><span class="line"><span class="comment">        * 索引：读取数组时的下标</span></span><br><span class="line"><span class="comment">        * */</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;s.length();i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=i-<span class="number">1</span>;j&gt;=<span class="number">0</span>;j--)&#123;</span><br><span class="line">                dp[i] = dp[j] &amp;&amp; check(s.substring(j, i));</span><br><span class="line">                <span class="keyword">if</span>(dp[i]) <span class="keyword">break</span>;<span class="comment">//这句也很精髓</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[s.length()];</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">check</span><span class="params">(String s)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> map.getOrDefault(s, <span class="keyword">false</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="3-Refrence"><a href="#3-Refrence" class="headerlink" title="3.Refrence"></a>3.<a href="https://leetcode-cn.com/problems/word-break/solution/dan-ci-chai-fen-ju-jue-zhuang-xcong-jian-dan-de-xi/" target="_blank" rel="noopener">Refrence</a></h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="-[Java, 算法] -[LeetCode]" scheme="http://yoursite.com/categories/Java-%E7%AE%97%E6%B3%95-LeetCode/"/>
    
    
      <category term="-[LeetCode] -[动态规划]" scheme="http://yoursite.com/tags/LeetCode-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
</feed>
